## Lecture 13 Peceptron_0

### 13.1 ANN Learning Rules

* The McCullock-Pitts neuron made a base for a machine (network of units) capable of
* The McCullock-Pitts neuronä¸ºæœºå™¨ï¼ˆå•å…ƒç½‘ç»œï¼‰å¥ å®šäº†åŸºç¡€ã€‚
  * **<font color=red>storing information and</font>**
  * **<font color=red>å­˜å‚¨ä¿¡æ¯</font>**
  * **<font color=red>producing logical and arithmetical operations on it</font>**
  * **<font color=red>å¯¹å…¶è¿›è¡Œé€»è¾‘å’Œç®—æœ¯è¿ç®—</font>**

* è¿™é‡Œå…ˆç®€åŒ–ä¸€ä¸‹è¿™ä¸ªç¥ç»å…ƒæ˜¯å¹²å˜›ç”¨çš„
* ![  ](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126000614230.png)

* The next step
  * must be to realise another important function of the brain, which is
    * to acquire new knowledge through experience, i.e. learning.
    * ä¸‹ä¸€æ­¥ï¼Œæ˜¯å®ç°å¤§è„‘çš„é‡è¦åŠŸèƒ½ä¹‹ä¸€ï¼Œå­¦ä¹ ã€‚

![image-20231126000823094](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126000823094.png)

* ANN learning rule is the rule **how to adjust the weights of connections to get desirable output.**
* ANN å­¦ä¹ è§„åˆ™æ˜¯å¦‚ä½•**è°ƒæ•´è¿æ¥æƒé‡ä»¥è·å¾—ç†æƒ³è¾“å‡ºçš„è§„åˆ™**ã€‚



### 13.2 MP Neuron vs. Hebbâ€™s Rule

![image-20231126001045301](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126001045301.png)



* Given a fixed MP neuron, that is, **all weights of connections** and neuron threshold are set up **in advance.**
* ç»™å®šä¸€ä¸ªå›ºå®šçš„ MP ç¥ç»å…ƒï¼Œå³äº‹**å…ˆè®¾ç½®å¥½æ‰€æœ‰è¿æ¥æƒé‡**å’Œç¥ç»å…ƒé˜ˆå€¼ã€‚

![image-20231126001512586](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126001512586.png)

![image-20231126001604575](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126001604575.png)

![image-20231126003249849](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126003249849.png)



### 13.3 Unsupervised Learning æ— ç›‘ç£å­¦ä¹ 

* Unsupervised learning is a type of machine learning in which **<font color=red size=5>the algorithm is not provided with any pre-assigned labels or scores for the training data.</font>** As a result, unsupervised learning algorithms must first **self-discover any naturally occurring patterns** in that training data set.
* æ— ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§ç±»å‹ï¼Œåœ¨è¿™ç§ç±»å‹ä¸­ï¼Œ**ç®—æ³•æ²¡æœ‰ä¸ºè®­ç»ƒæ•°æ®æä¾›ä»»ä½•é¢„å…ˆåˆ†é…çš„æ ‡ç­¾æˆ–åˆ†æ•°ã€‚**å› æ­¤ï¼Œæ— ç›‘ç£å­¦ä¹ ç®—æ³•å¿…é¡»é¦–å…ˆ**è‡ªæˆ‘å‘ç°è®­ç»ƒæ•°æ®é›†ä¸­ä»»ä½•è‡ªç„¶å‡ºç°çš„æ¨¡å¼**ã€‚

![image-20231126004138235](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126004138235.png)

* è¿™ä¸ªé—®é¢˜æ˜¯åœ¨ 20 ä¸–çºª 40 å¹´ä»£é¦–æ¬¡æå‡ºçš„ã€åœ¨ç¥ç»ç§‘å­¦å®éªŒè¿˜å¾ˆæœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™äº›å­¦ä¹ è§„åˆ™çš„ç»å…¸å®šä¹‰
  è¿™äº›å­¦ä¹ è§„åˆ™çš„ç»å…¸å®šä¹‰å¹¶éæ¥è‡ªç”Ÿç‰©å­¦ï¼Œè€Œæ˜¯å¿ƒç†å­¦



### 13.4 Perceptron æ„ŸçŸ¥å™¨

* Rosenblatt (1958) explicitly considered the problem of pattern recognition, where a â€œteacherâ€ is essential. (Consider the difference with unsupervised learning)
* ç½—æ£®å¸ƒæ‹‰ç‰¹ï¼ˆRosenblattï¼Œ1958 å¹´ï¼‰æ˜ç¡®è€ƒè™‘äº†æ¨¡å¼è¯†åˆ«é—®é¢˜ï¼Œåœ¨è¿™ä¸ªé—®é¢˜ä¸Šï¼Œ"æ•™å¸ˆ "æ˜¯å¿…ä¸å¯å°‘çš„ã€‚(è€ƒè™‘ä¸æ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ï¼‰
* A Perceptron is a neural network that changes with â€œexperienceâ€ using an **error-correction rule.**
* æ„ŸçŸ¥å™¨æ˜¯ä¸€ç§åˆ©ç”¨çº é”™è§„åˆ™éš "ç»éªŒ "å˜åŒ–çš„ç¥ç»ç½‘ç»œã€‚
* According to the rule, **weight of a neuron changes when it makes error response to the input presented to the network.**
* æ ¹æ®è¿™ä¸€è§„åˆ™ï¼Œ**å½“ç¥ç»å…ƒå¯¹ç½‘ç»œè¾“å…¥åšå‡ºé”™è¯¯å“åº”æ—¶ï¼Œå…¶æƒé‡å°±ä¼šå‘ç”Ÿå˜åŒ–ã€‚**

![image-20231126005252762](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126005252762.png)

![image-20231126004749106](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126004749106.png)

* The simplest architecture of perceptron comprises **one layer of idealised â€œneuronsâ€**, which we also sometimes call â€œunitsâ€ of the network.
* æœ€ç®€å•çš„æ„ŸçŸ¥å™¨ç»“æ„åŒ…æ‹¬**ä¸€å±‚ç†æƒ³åŒ–çš„ "ç¥ç»å…ƒ"**ï¼Œæˆ‘ä»¬æœ‰æ—¶ä¹Ÿç§°å…¶ä¸ºç½‘ç»œçš„ **"å•å…ƒ"**ã€‚

 ![image-20231126004837149](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126004837149.png)

![image-20231126012104933](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126012104933.png)

* **è¾“å…¥å±‚ï¼ˆInput Layerï¼‰**ï¼šç”±ä¸€ç³»åˆ—è¾“å…¥å•å…ƒ*a*1,*a*2,...,*a**n* ç»„æˆï¼Œå…¶ä¸­ a0=1*a*0=1 é€šå¸¸è¡¨ç¤ºåç½®å•å…ƒï¼Œå®ƒçš„ä½œç”¨æ˜¯å…è®¸è¾“å‡ºé˜ˆå€¼å¯è°ƒæ•´ã€‚
* **è¾“å‡ºå±‚ï¼ˆOutput Layerï¼‰**ï¼šåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å‡ºç¥ç»å…ƒ *x* *j*ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåªæ˜¾ç¤ºäº†ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒã€‚

![image-20231126012752329](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126012752329.png)

* The two layers are fully interconnected, i.e. every input neuron is connected to every output neuron.
* ä¸¤å±‚éƒ½æ˜¯å……åˆ†äº’è”çš„

![image-20231126012952359](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126012952359.png)

* Semantically, a perceptron can be considered as a vector-valued function that maps
* ä»è¯­ä¹‰ä¸Šè®²ï¼Œæ„ŸçŸ¥å™¨å¯è¢«è§†ä¸º å¯è§†ä¸ºä¸€ä¸ªå‘é‡å€¼å‡½æ•°

**![Uploaded image](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\av4RUJF5eUbei8c3WkAVEwhUKvfVOO0JyqW3GDY%3D.png)**

* å°½ç®¡æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒéƒ½æ¥æ”¶ç›¸åŒçš„è¾“å…¥ï¼Œä½†å®ƒä»¬å¯¹åº”çš„è¿æ¥æƒé‡ *w**ji* æ˜¯ç‹¬ç«‹çš„ã€‚è¿™æ„å‘³ç€å³ä½¿å¯¹äºç›¸åŒçš„è¾“å…¥ï¼Œä¸åŒçš„è¾“å‡ºç¥ç»å…ƒä¹Ÿå¯èƒ½æœ‰ä¸åŒçš„ååº”ï¼Œå› ä¸ºå®ƒä»¬é€šè¿‡ç‹¬ç«‹çš„æƒé‡æ¥åŠ æƒè¾“å…¥ä¿¡å·ã€‚
* å¹»ç¯ç‰‡ä¸­æåˆ°çš„â€œindividual connectionsâ€è¡¨æ˜æ¯ä¸ªè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„è¿æ¥æ˜¯å”¯ä¸€çš„ï¼Œæ¯ä¸ªè¿æ¥éƒ½æœ‰è‡ªå·±çš„æƒé‡ *w**ji*ã€‚ç”±äºè¿™äº›æƒé‡æ˜¯ç‹¬ç«‹çš„ï¼Œæ¯ä¸ªè¾“å‡ºç¥ç»å…ƒå¯ä»¥ç‹¬ç«‹è°ƒæ•´ï¼Œä»¥ä¾¿åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è¿›è¡Œä¼˜åŒ–ã€‚
* ä»¥ *j*-th è¾“å‡ºç¥ç»å…ƒä¸ºä¾‹ï¼Œå®ƒçš„è¾“å‡º *X**j* ä¼šç”±è¾“å…¥çš„åŠ æƒå’Œé€šè¿‡æ¿€æ´»å‡½æ•° *f* ç¡®å®šã€‚æƒé‡ 0*w**j*0ï¼ˆç­‰äº âˆ’âˆ’*Î¸**j*ï¼‰æ˜¯åç½®æƒé‡ï¼Œç”¨äºè°ƒèŠ‚æ¿€æ´»é˜ˆå€¼ã€‚è¿™ç§ç»“æ„å…è®¸æ„ŸçŸ¥æœºæ¨¡å‹å¯¹æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒè¿›è¡Œä¸ªæ€§åŒ–çš„å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«è¾“å…¥æ¨¡å¼å¹¶æ®æ­¤åšå‡ºç‹¬ç«‹çš„å†³ç­–ã€‚

![image-20231126014952246](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126014952246.png)

![image-20231126015104405](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126015104405.png)

* a0=1æ˜¯ä¸€ä¸ªåç½®å•å…ƒï¼Œå¾€å¾€ç”¨äºè°ƒæ•´æ¿€æ´»å‡½æ•°çš„é˜™å€¼ã€‚è¿™ä¹ˆæƒ³ï¼Œå½“a0å¤§,wj0ä¸ºæ­£æ•°ï¼Œé‚£ä¹ˆå°±ä»¥æ¿€æ´»ï¼Œåä¹‹ã€‚

![image-20231126015324246](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126015324246.png)

* With this trick, we have no need to set a threshold manually. Now we can train the threshold like weights.
* æœ‰äº†è¿™ä¸ªæŠ€å·§ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦ æ‰‹åŠ¨è®¾ç½®é˜ˆå€¼ã€‚ç°åœ¨æˆ‘ä»¬ å¯ä»¥åƒè®­ç»ƒæƒé‡ä¸€æ ·è®­ç»ƒé˜ˆå€¼ã€‚

![image-20231126015422997](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126015422997.png)

#### 13.4.1 MP Neuron vs. Perceptron (Syntax and Semantics)

![image-20231126020800637](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126020800637.png)

![image-20231126020817894](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126020817894.png)

![image-20231126020846812](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126020846812.png)

![image-20231126020904287](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126020904287.png)

![image-20231126021021811](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126021021811.png)

* - ä¸Hebbæ³•åˆ™ç±»ä¼¼ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´ä¸¤å±‚ä¹‹é—´çš„æƒé‡ï¼ˆè®­ç»ƒï¼‰æ¥å­¦ä¹  çŸ¥è¯†ã€‚
  - å¦‚æœæ•°æ®é›†æ˜¯æ— æ ‡è®°çš„ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒæ„ŸçŸ¥å™¨ç½‘ç»œå°†è¾“å…¥åˆ†ç»„åˆ°ä¸åŒçš„ç»„ä¸­ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ã€‚

![image-20231126021141199](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126021141199.png)

* å¦‚æœæ•°æ®é›†æ˜¯æœ‰æ ‡ç­¾çš„ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®­ç»ƒæ„ŸçŸ¥å™¨ç½‘ç»œï¼Œä½¿å…¶é’ˆå¯¹ç‰¹å®šè¾“å…¥äº§ç”Ÿæ‰€éœ€çš„è¾“å‡ºï¼ˆç›‘ç£å­¦ä¹ ï¼‰ã€‚

## Lecture 14 Peceptron_1

![image-20231126021505580](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126021505580.png)

* Rosenblatt (1958) explicitly considered the problem of pattern recognition, where a â€œteacherâ€ is essential.
* A Perceptron is a neural network that changes with â€œexperienceâ€ using an **<font color=red>error-correction rule</font>**. (Supervised learning!)
* According to the rule, **<font color=red>weight of a response unit changes when it makes error response to the input presented to the network.</font>**
* **æ ¹æ®è¯¥è§„åˆ™ï¼Œå½“å“åº”å•å…ƒå¯¹å‘ˆç°ç»™ç½‘ç»œçš„è¾“å…¥åšå‡ºé”™è¯¯å“åº”æ—¶ï¼Œå“åº”å•å…ƒçš„æƒé‡ä¼šå‘ç”Ÿå˜åŒ–ã€‚**



### 14.1.1 Perceptron Training

![image-20231126021748237](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126021748237.png)

* **<font color=red>Weights ğ‘¤ji</font>** of connections between two layers are changed according to **<font color=red>perceptron learning rule</font>**, so the network is more likely to produce the desired output in response to certain inputs.
* ä¸¤å±‚ä¹‹é—´è¿æ¥çš„**æƒé‡ğ‘¤ji** ä¼šæ ¹æ®**æ„ŸçŸ¥å™¨å­¦ä¹ è§„åˆ™**å‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤ç½‘ç»œæ›´æœ‰å¯èƒ½å¯¹ç‰¹å®šè¾“å…¥äº§ç”Ÿæ‰€éœ€çš„è¾“å‡ºã€‚

![image-20231126023302958](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126023302958.png)

* The perceptron is trained by using a training set with target outputs (labels).
* æ„ŸçŸ¥å™¨é€šè¿‡ä½¿ç”¨å¸¦æœ‰ç›®æ ‡è¾“å‡ºï¼ˆæ ‡ç­¾ï¼‰çš„è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒã€‚
  * **Training set**: a set of input patterns repeatedly presented to the network during training;
  * **è®­ç»ƒé›†**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åå¤å‘ˆç°ç»™ç½‘ç»œçš„ä¸€ç»„è¾“å…¥æ¨¡å¼ï¼›
  * **Target/desired output (label)**: the pre-defined correct output of an input pattern in the training set.
  * **ç›®æ ‡/é¢„æœŸè¾“å‡ºï¼ˆæ ‡ç­¾ï¼‰**ï¼šè®­ç»ƒé›†ä¸­è¾“å…¥æ¨¡å¼çš„é¢„å®šä¹‰æ­£ç¡®è¾“å‡ºã€‚

![image-20231126024714824](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126024714824.png)

* Every input pattern is used multiple times for training
* æ¯ä¸ªè¾“å…¥æ¨¡å¼éƒ½ä¼šè¢«å¤šæ¬¡ç”¨äºè®­ç»ƒ

![image-20231126024800195](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126024800195.png)

* Input pattern in the set is ğ’-dimensional! Note ğ‘ ! =1
* é›†åˆä¸­çš„è¾“å…¥æ¨¡å¼ä¸º ğ’ ç»´ï¼æ³¨æ„ ğ‘ 0=1

![image-20231126024903078](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126024903078.png)

* Note that in unsupervised learning, no label is needed.
* è¯·æ³¨æ„ï¼Œåœ¨æ— ç›‘ç£ å­¦ä¹ ä¸­ä¸éœ€è¦æ ‡ç­¾ã€‚

![image-20231126025003795](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126025003795.png)

* The network outputs ğ‘¿ ğ’‹ are then **compared to the desired outputs** specified in the training set and obtain the error.
* ç„¶åå°†ç½‘ç»œè¾“å‡º ğ‘¿ ğ’‹ ä¸è®­ç»ƒé›†ä¸­**æŒ‡å®šçš„æœŸæœ›è¾“å‡ºè¿›è¡Œæ¯”è¾ƒ**ï¼Œå¾—å‡ºè¯¯å·®ã€‚
* è¿™æ˜¯ä¸€ä¸ªæœ‰ç›‘ç£å­¦ä¹ ï¼Œæ¯”è¾ƒè¾“å‡ºç»“æœå’Œæ ‡å‡†ç»“æœå¾—å‡ºè¯¯å·®æ˜¯ä¸€ä¸ªå¾ˆå…¸å‹çš„ç›‘ç£å­¦ä¹ ç‰¹å¾ã€‚

![image-20231126025454546](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126025454546.png)

* The **error of an output neuron** is the difference between the target output and the instant one.
* **ç¥ç»å…ƒçš„é”™è¯¯**è¾“å‡ºæ˜¯ç›®æ ‡è¾“å‡ºä¸å³æ—¶è¾“å‡ºçš„å·®å€¼ã€‚

![image-20231126025717617](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126025717617.png)

* The **errors** are then used to readjust the values of the weights of the connections.
* ç„¶ååˆ©ç”¨**è¯¯å·®**é‡æ–°è°ƒæ•´è¿æ¥çš„æƒé‡å€¼ã€‚

![image-20231126025840216](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126025840216.png)

* The weights readjustment is done in such a way that the network is â€“ on the whole â€“ **more likely to give the desired response next time.**
* æƒé‡é‡æ–°è°ƒæ•´çš„æ–¹å¼æ˜¯ä½¿ç½‘ç»œæ•´ä½“ä¸Š**æ›´æœ‰å¯èƒ½åœ¨ä¸‹ä¸€æ¬¡åšå‡ºé¢„æœŸçš„å“åº”**ã€‚

* The **goal of the training** is to arrive at a single set of weights that allow each input in the training set to be mapped to the correct output by the network.
* è®­ç»ƒçš„**ç›®çš„æ˜¯è·å¾—ä¸€ç»„æƒé‡**ï¼Œä½¿ç½‘ç»œèƒ½å°†è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªè¾“å…¥æ˜ å°„åˆ°æ­£ç¡®çš„è¾“å‡ºã€‚



### 14.1.2 Weight Update

![image-20231126030835562](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231126030835562.png)

![image-20231128004732106](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128004732106.png)

#### 14.1.2.1 Difference with Hebb`s Rule

![image-20231128005158158](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128005158158.png)



![image-20231128005417818](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128005417818.png)

* The value of the â€œlearning rateâ€ C is  
  * usually set below 1 and 
  * ç»å¸¸è®¾ç½®åœ¨1ä»¥ä¸‹
  * determines the amount of correction made in a single iteration.
  * å†³å®šäº†å•è¯è¿­ä»£çš„ä¿®æ­£é‡

![image-20231128005349786](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128005349786.png)

* The overall learning time of the network is affected by the value of the â€œlearning rateâ€ C: slower for small values and faster for larger.
* è¶Šå°çš„å­¦ä¹ ç‡æ„å‘³ç€è¶Šæ…¢çš„å­¦ä¹ æ—¶é—´ï¼Œåä¹‹ã€‚

![image-20231128005955579](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128005955579.png)

* The network performance during training session is measured by a **root-mean-square (RMS)** error value. 
* è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç½‘ç»œæ€§èƒ½ä»¥**å‡æ–¹æ ¹è¯¯å·®å€¼**æ¥è¡¡é‡ã€‚

![image-20231128010154312](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128010154312.png)

* å½“RMS=0çš„æ—¶å€™ï¼Œåœæ­¢è®¡ç®—ã€‚

![image-20231128010451887](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128010451887.png)

* The first sum is taken over all patterns in the training set, and the second sum is taken over all output neurons. 
* ç¬¬ä¸€ä¸ªæ€»å’Œæ˜¯è®­ç»ƒé›†ä¸­æ‰€æœ‰æ¨¡å¼çš„æ€»å’Œï¼Œç¬¬äºŒä¸ªæ€»å’Œæ˜¯æ‰€æœ‰è¾“å‡ºç¥ç»å…ƒçš„æ€»å’Œã€‚
* 1. **å¯¹æ‰€æœ‰æ¨¡å¼è¿›è¡Œæ±‚å’Œ**ï¼šè¿™ä¸ªæ­¥éª¤ç´¯åŠ äº†æ•´ä¸ªæ•°æ®é›†çš„è¯¯å·®è´¡çŒ®ã€‚
* 2. **å¯¹æ¯ä¸ªæ¨¡å¼çš„æ‰€æœ‰è¾“å‡ºç¥ç»å…ƒçš„è¯¯å·®å¹³æ–¹è¿›è¡Œæ±‚å’Œ**ï¼šè¿™ä¸ªæ­¥éª¤è€ƒè™‘äº†å•ä¸ªæ•°æ®ç‚¹å¯¹æ‰€æœ‰è¾“å‡ºçš„è´¡çŒ®ã€‚

![image-20231128011608524](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128011608524.png)

* ç”±äºtï¼Œr,m,éƒ½æ˜¯å›ºå®šçš„å€¼ï¼Œæ‰€ä»¥å°†RMSç†è§£æˆä¸€ä¸ªå‡½æ•°ï¼Œå”¯ä¸€å—å½±å“çš„å°±æ˜¯Xã€‚

![image-20231128011806010](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128011806010.png)

* è‹¥æ˜¯å°†Xä¹Ÿè§†ä½œä¸€ä¸ªå‡½æ•°ï¼Œå½±å“Xçš„å€¼å°±æ˜¯å„ä¸ªæƒé‡wï¼Œå› ä¸ºaæ˜¯å›ºå®šçš„ï¼Œæ˜¯ä¸€ä¸ªæ’å®šå€¼

![image-20231128012350584](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128012350584.png)

* æœ€ç»ˆå¯ä»¥è·å¾—è¿™ä¸ªå‡½æ•°

![image-20231128012437939](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128012437939.png)

* **<font color=red>The best performance of the network</font>** over a given data set D corresponds to **<font color=red>the minimum of the RMS error</font>**, and we adjust the weight of connections w in order to reach the minimum.
* åœ¨ç»™å®šçš„æ•°æ®é›† D ä¸Šï¼Œç½‘ç»œçš„æœ€ä½³æ€§èƒ½ä¸å‡æ–¹æ ¹è¯¯å·®çš„æœ€å°å€¼ç›¸å¯¹åº”ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´è¿æ¥æƒé‡ w æ¥è¾¾åˆ°æœ€å°å€¼ã€‚

![image-20231128013437613](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128013437613.png)

![image-20231128013458705](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128013458705.png)

![image-20231128013754599](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128013754599.png)

* æœ€ç»ˆï¼Œå‡½æ•°æ”¶æ•›convergedã€‚

## Lecture15_Peceptron_2

### 15.1 **A Running Example**

![image-20231128060256517](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128060256517.png)



![image-20231128061555933](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128061555933.png)



![image-20231128061607332](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128061607332.png)

![image-20231128061701216](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128061701216.png)

![image-20231128061710883](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128061710883.png)

## Lecture 16 Peceptron_3

### 16.1 Convergence of Perceptron Learning Algorithm

* **<font color=red size=5>The perceptron learning algorithm can converge for the data set that is linearly separable</font>**
* å¯¹äºçº¿æ€§å¯åˆ†ç¦»çš„æ•°æ®é›†ï¼Œæ„ŸçŸ¥å™¨å­¦ä¹ ç®—æ³•å¯ä»¥æ”¶æ•›

![image-20231128064347408](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128064347408.png)

* Without loss of generality, we consider a simplified case. 
* åœ¨ä¸å¤±ä¸€èˆ¬æ€§çš„å‰æä¸‹ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸€ç§ç®€åŒ–çš„æƒ…å†µã€‚
  * There is only one output in the network
  * æˆ‘ä»¬ä¸€èˆ¬è€ƒè™‘åœ¨ç¥ç»ç½‘ç»œåªè¾“å‡ºä¸€ä¸ªç»“æœ
  * The learning rate C is set as 1.
  * å­¦ä¹ ç‡è¢«è®¾ç½®ä¸º1

![image-20231128064533510](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128064533510.png)

* Recall the following informal definition we mentioned for MP neuron. 
* å›é¡¾ä¸€ä¸‹æˆ‘ä»¬æåˆ°çš„ MP ç¥ç»å…ƒçš„éæ­£å¼å®šä¹‰ã€‚
* **<font size=5>Linear separability</font>** (for Boolean functions): There exists a line (plane) such that all inputs which produce a 1 for the function lie on one side of the line (plane) and all inputs which produce a 0 lie on other side of the line (plane).
* çº¿æ€§å¯åˆ†æ€§ï¼ˆå¸ƒå°”å‡½æ•°ï¼‰ï¼š å­˜åœ¨ä¸€æ¡çº¿ï¼ˆä¸€ä¸ªå¹³é¢ï¼‰ï¼Œä½¿å¾—å‡½æ•°äº§ç”Ÿ 1 çš„æ‰€æœ‰è¾“å…¥éƒ½ä½äºè¯¥çº¿ï¼ˆå¹³é¢ï¼‰çš„ä¸€è¾¹ï¼Œè€Œäº§ç”Ÿ 0 çš„æ‰€æœ‰è¾“å…¥éƒ½ä½äºè¯¥çº¿ï¼ˆå¹³é¢ï¼‰çš„å¦ä¸€è¾¹ã€‚

* ![image-20231128065104038](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128065104038.png)

![image-20231128065236972](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128065236972.png)

![image-20231128065814800](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128065814800.png)

#### 16.1.1 Perceptron Learning Algorithm (One output)

![image-20231128070039656](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128070039656.png)

* Rewriting the general perceptron learning algorithm. 
  * Since we only consider one output, the result becomes a weight vector w=(w0, w1, w2......, wn)  rather than the weight matrix. Here wi represents for the weight of the connection between the i-th input and the output.
  * ç”±äºæˆ‘ä»¬åªè€ƒè™‘ä¸€ä¸ªè¾“å‡ºï¼Œç»“æœå°±å˜æˆäº†æƒé‡å‘é‡ w=ï¼ˆw0, w1, w2......, wnï¼‰ï¼Œè€Œä¸æ˜¯æƒé‡çŸ©é˜µã€‚è¿™é‡Œ wi ä»£è¡¨ç¬¬ i ä¸ªè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„è¿æ¥æƒé‡ã€‚

![image-20231128071912265](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128071912265.png)

* ç”±äºçº¿æ€§å¯åˆ†ï¼Œæˆ‘ä»¬å°†D`åˆ†ä¸ºPå’ŒNä¸¤ä¸ªæ•°ç»„ï¼Œé€šè¿‡è¿™ä¸ªæ¥å¯¹è¿›è¡Œåˆ’åˆ†ã€‚

![image-20231128072120081](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128072120081.png)

* **The convergence of the learning rule means we successfully find a feasible wâˆ—!** 
* **å½“å­¦ä¹ ç‡é€’å½’çš„æ—¶å€™æ„å‘³ç€å¯»æ‰¾ç‰¹å¾wæˆåŠŸ**

![image-20231128072347291](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128072347291.png)

![image-20231128072516169](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128072516169.png)

![image-20231128072601258](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128072601258.png)



#### 16.1.2 Geometric Interpretation å‡ ä½•è§£è¯»

![image-20231128072736562](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128072736562.png)

* Red arrows denote the points in P, 
* çº¢è‰²ç®­å¤´è¡¨ç¤ºåœ¨é›†åˆPçš„ç‚¹
* Blue arrows denote the points in N, 
* è“è‰²çš„ç®­å¤´è¡¨ç¤ºåœ¨é›†åˆNçš„ç‚¹
* Thick black line denotes the barrier of w*T a = 0

![image-20231128073740105](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128073740105.png)

![image-20231128073728493](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128073728493.png)

*  **<font size=5>Orthogonal æ¨ªå‘</font>**

![image-20231128074227748](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128074227748.png)



![image-20231128074738388](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128074738388.png)



![image-20231128074956470](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128074956470.png)

![image-20231128075011764](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128075011764.png)

![image-20231128075114948](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128075114948.png)

![image-20231128080434134](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128080434134.png)

![image-20231128080817764](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128080817764.png)

* è¿™é‡Œçš„æƒé‡wkè¢«åˆ†é…ä¸º1

![image-20231128084625011](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128084625011.png)

![image-20231128084732241](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128084732241.png)

![image-20231128084802452](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128084802452.png)

* æˆ‘ä»¬å¾—åˆ°äº†åˆ†æ¯çš„ä¸Šé™

![image-20231128085329791](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128085329791.png)

![image-20231128085531575](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128085531575.png)

* å¾—åˆ°ç»“æœ

![image-20231128085612816](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128085612816.png)

![image-20231128085640957](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128085640957.png)

![image-20231128085650532](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128085650532.png)

![image-20231128090309275](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128090309275.png)



## Lecture 17  Multi-Layer Perceptron_0

![image-20231128222745929](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128222745929.png)

![image-20231128223654328](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128223654328.png)

* å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…³å¿ƒçš„æ˜¯Sè¿™ä¸ªå…¬å¼çš„ç¬¦å·ã€‚
* wçš„æ–¹å‘éå¸¸é‡è¦ï¼Œè€Œä¸æ˜¯é•¿åº¦ã€‚æ¯”å¦‚|w|ä¸ä¸­ç”¨
* å°†å­¦ä¹ è§„åˆ™è§†ä¸ºåœ¨ä¸åŒæƒ…å†µä¸‹æ”¹å˜ w æ–¹å‘çš„æ–¹æ³•ã€‚
* Consider the learning rule as the ways to change the direction of w under different situations.
* é‚£ä¹ˆï¼Œæ„ŸçŸ¥å™¨å­¦ä¹ è§„åˆ™çš„æ”¶æ•›æ€§å¯ä»¥è®¤ä¸ºæ˜¯ï¼Œw æœ€ç»ˆä¸å­˜åœ¨ä½†æœªçŸ¥çš„æœ€ä¼˜ w* å…·æœ‰ç›¸ä¼¼çš„æ–¹å‘ã€‚
* Then the convergence of perceptron learning rule can be considered as that w finally has the similar direction with the optimal w* that exists but is unknown.
* åœ¨è¯æ˜è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯è¯æ˜ w å’Œ wâˆ— ä¹‹é—´çš„å¤¹è§’ä¼šéšç€wâˆ—æ•°ç›®çš„å¢åŠ è€Œå˜å°ï¼ˆä¸ä¸€å®šæ˜¯å•è°ƒçš„ï¼‰ã€‚å’Œ wâˆ— ä¹‹é—´çš„å¤¹è§’ä¼šéšç€è¯¯åˆ†ç±»æ¬¡æ•°çš„å¢åŠ è€Œå˜å°ï¼ˆä¸ä¸€å®šæ˜¯å•è°ƒçš„ çš„è§’åº¦ä¼šè¶Šæ¥è¶Šå°ï¼ˆä¸ä¸€å®šæ˜¯å•è°ƒçš„ï¼‰ï¼Œå¹¶ä¸”ä¸ä¼šå°äº 0ã€‚

![image-20231128224321379](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128224321379.png)

### 17.1 Beyond Linear Separability

![image-20231128224521993](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128224521993.png)

* ç°åœ¨æˆ‘ä»¬çŸ¥é“ï¼Œå¯¹äºçº¿æ€§å¯åˆ†ç¦»çš„æ•°æ®é›†ï¼Œæ„ŸçŸ¥å™¨å­¦ä¹ ç®—æ³•æœ€ç»ˆå¯ä»¥æ”¶æ•›ã€‚æ”¶æ•›ã€‚
* **é‚£ä¹ˆï¼Œä¸å¯çº¿æ€§åˆ†ç¦»çš„æ•°æ®é›†å‘¢ï¼Ÿ**
* **æœ€è‘—åçš„ä¾‹å­å°±æ˜¯ "XOR"ï¼ˆæ’ä»– "OR"ï¼‰é—¨ã€ ç§°ä¸º XOR é—®é¢˜ã€‚**



#### 17.1.1 The XOR Problem ï¼ˆä¸€ä¸ªå…¸å‹çš„éçº¿æ€§é—®é¢˜ï¼‰

![image-20231128225013099](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128225013099.png)

![image-20231128230120370](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128230120370.png)

* ç”±ä¸Šé¢è¿™ä¸ªå¼å­å¯ä»¥è¯æ˜XORä¸ç¬¦åˆçº¿æ€§å¯åˆ†



![image-20231128231233712](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128231233712.png)

* Minsky and Papert showed that in the case of any non-linearly separable problem, such as XOR, in the network architecture there must be **<font color=red size=5>â€œhidden neuronsâ€</font>**, i.e. the neurons with output not available to the outside world, in order to help turn the problem into a linearly separable one for the outputs.
* æ˜æ–¯åŸºå’Œå¸•å¸•ç‰¹æŒ‡å‡ºï¼Œåœ¨ä»»ä½•éçº¿æ€§å¯åˆ†é—®é¢˜ï¼ˆå¦‚ XORï¼‰çš„æƒ…å†µä¸‹ï¼Œç½‘ç»œç»“æ„ä¸­å¿…é¡»æœ‰ "éšè—ç¥ç»å…ƒ"ï¼Œå³å¯¹å¤–éƒ¨ä¸–ç•Œæ²¡æœ‰è¾“å‡ºçš„ç¥ç»å…ƒï¼Œä»¥å¸®åŠ©å°†é—®é¢˜è½¬åŒ–ä¸ºè¾“å‡ºçš„çº¿æ€§å¯åˆ†é—®é¢˜ã€‚

#### 17.1.2 Hidden Neurons

![image-20231128233328557](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128233328557.png)

* Minsky and Papert showed that in the case of any non-linearly separable problem, such as XOR, in the network architecture there must be â€œhidden neuronsâ€, i.e. the neurons with output not available to the outside world, in order to help turn the problem into a linearly separable one for the outputs.

* æ˜æ–¯åŸºå’Œå¸•å¸•ç‰¹æŒ‡å‡ºï¼Œåœ¨ä»»ä½•éçº¿æ€§å¯åˆ†é—®é¢˜ï¼ˆå¦‚ XORï¼‰çš„æƒ…å†µä¸‹ï¼Œç½‘ç»œç»“æ„ä¸­å¿…é¡»æœ‰ "éšè—ç¥ç»å…ƒ"ï¼Œå³å¯¹å¤–éƒ¨ä¸–ç•Œæ²¡æœ‰è¾“å‡ºçš„ç¥ç»å…ƒï¼Œä»¥å¸®åŠ©å°†é—®é¢˜è½¬åŒ–ä¸ºè¾“å‡ºçš„çº¿æ€§å¯åˆ†é—®é¢˜ã€‚

![image-20231128234001899](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128234001899.png)

* The three-layer perceptron below is capable to represent XOR. 
* ä¸‹é¢çš„ä¸‰å±‚æ„ŸçŸ¥å™¨èƒ½å¤Ÿè¡¨ç¤º XORã€‚
* Each hidden neuron separates the input space into a **closed positive** and **open negative half-space**. 
* æ¯ä¸ªéšè—ç¥ç»å…ƒå°†è¾“å…¥ç©ºé—´åˆ†ä¸ºå°é—­çš„**æ­£åŠç©ºé—´å’Œå¼€æ”¾çš„è´ŸåŠç©ºé—´**ã€‚
* The left bottom figure shows the linear separations defined by each hidden unit, while the positive half-spaces are shaded. 
* å·¦ä¸‹å›¾æ˜¾ç¤ºäº†æ¯ä¸ªéšè—å•å…ƒå®šä¹‰çš„çº¿æ€§åˆ†éš”ï¼Œè€Œæ­£åŠç©ºåˆ™ç”¨é˜´å½±è¡¨ç¤ºã€‚



* è¿™é‡Œè§£é‡Šä¸€ä¸‹è¿™é‡Œçš„hidden neuron, **æ ‡æ³¨ä¸º âˆ’0.5âˆ’0.5 çš„å€¼æ˜¯ä¸¤ä¸ªéšè—å±‚ç¥ç»å…ƒçš„åç½®é¡¹ã€‚**åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒå¯ä»¥æœ‰ä¸€ä¸ªåç½®é¡¹ï¼Œè¿™ä¸ªåç½®é¡¹æ˜¯åœ¨è®¡ç®—ç¥ç»å…ƒçš„æ€»è¾“å…¥æ—¶åŠ ä¸Šçš„ä¸€ä¸ªå¸¸æ•°ã€‚åç½®é¡¹çš„ä½œç”¨æ˜¯ä½¿å¾—ç¥ç»å…ƒæ¿€æ´»å‡½æ•°çš„é˜ˆå€¼å¯ä»¥è°ƒæ•´ï¼Œè¿™æ ·å³ä½¿æ‰€æœ‰è¾“å…¥éƒ½æ˜¯é›¶ï¼Œç¥ç»å…ƒä¹Ÿå¯ä»¥è¾“å‡ºä¸€ä¸ªéé›¶å€¼ã€‚
* ![image-20231128234429518](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128234429518.png)



![image-20231128234448598](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128234448598.png)



#### 17.1.3 Multilayer Perceptron å¤šé‡æ„ŸçŸ¥å™¨

![image-20231128234556845](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128234556845.png)

* A multilayer perceptron (MLP) is a layered architecture of neurons, where 
* å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ˜¯ä¸€ç§ç¥ç»å…ƒåˆ†å±‚ç»“æ„ï¼Œå…¶ä¸­
  * all the neurons are divided into - subsets, each set is called a layer; 
  * æ‰€æœ‰ç¥ç»å…ƒè¢«åˆ†ä¸º - ä¸ªå­é›†ï¼Œæ¯ä¸ªå­é›†ç§°ä¸ºä¸€å±‚ï¼›
  * There are only connections between two adjacent layers. Usually, the neurons within a layer are not connected to each other, though some neural models make use of this kind of architecture. 
  * ç›¸é‚»ä¸¤å±‚ä¹‹é—´åªæœ‰è¿æ¥ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå±‚å†…çš„ç¥ç»å…ƒä¹‹é—´æ²¡æœ‰è¿æ¥ï¼Œä½†æœ‰äº›ç¥ç»æ¨¡å‹ä¼šä½¿ç”¨è¿™ç§ç»“æ„ã€‚

![image-20231128234918046](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128234918046.png)

* The first layer is **the input layer** (We donâ€™t usually count it); 
* ç¬¬ä¸€å±‚æ˜¯è¾“å…¥å±‚ï¼ˆæˆ‘ä»¬é€šå¸¸ä¸è®¡ç®—å®ƒï¼‰ï¼›
* The last layer is **the output layer**. 
* æœ€åä¸€å±‚æ˜¯è¾“å‡ºå±‚ã€‚
* All other layers with no direct connections from or to the outside are called **hidden layers**. 
* æ‰€æœ‰ä¸å¤–éƒ¨æ²¡æœ‰ç›´æ¥è”ç³»çš„å…¶ä»–å±‚éƒ½ç§°ä¸ºéšè—å±‚ã€‚



![image-20231128235232325](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128235232325.png)

* We consider the fully-connected architectures in this module, that is, **every neuron from one layer is connected to all neurons in the following layer.** 
* æˆ‘ä»¬åœ¨æœ¬æ¨¡å—ä¸­è€ƒè™‘äº†å…¨è¿æ¥æ¶æ„ï¼Œ**å³ä¸€å±‚çš„æ¯ä¸ª éƒ½ä¸ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿æ¥**
* Each connection is associated with **a real weight and a real bias**. 
* æ¯ä¸ªè¿æ¥éƒ½ä¸**å®é™…æƒé‡å’Œå®é™…åå·®**ç›¸å…³è”ã€‚
* Inputs are real. Outputs are real.
* è¾“å…¥æ˜¯çœŸå®çš„ã€‚è¾“å‡ºæ˜¯çœŸå®çš„ã€‚



![image-20231128235451085](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128235451085.png)

* The input is processed and propagated from one layer to the next, until the final result is computed. 
* è¾“å…¥ç»è¿‡å¤„ç†åä»ä¸€å±‚ä¼ æ’­åˆ°ä¸‹ä¸€å±‚ï¼Œç›´åˆ°è®¡ç®—å‡ºæœ€ç»ˆç»“æœã€‚
* This process represents **<font color=red size=5>the forward propagation</font>**. 
* è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯**å‰å‘ä¼ æ’­**ã€‚
* For simplicity, from now we assume **<font color=red>the biases in the network are all zero.</font>**
* ä¸ºç®€å•èµ·è§ï¼Œä»ç°åœ¨èµ·æˆ‘ä»¬å‡è®¾**ç½‘ç»œä¸­çš„åå·®å‡ä¸ºé›¶ã€‚**



![image-20231128235815096](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231128235815096.png)

* The difference of the multilayer perceptron, compared to the single layer one, is that the output value X' of the first layer is not the output value of the multilayer perceptron any more. 
* å¤šå±‚æ„ŸçŸ¥å™¨ä¸å•å±‚æ„ŸçŸ¥å™¨çš„åŒºåˆ«åœ¨äºï¼Œç¬¬ä¸€å±‚çš„è¾“å‡ºå€¼ X'ä¸å†æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨çš„è¾“å‡ºå€¼ã€‚
* The output value X' of the first layer is the input to the next layer.
* ç¬¬ä¸€å±‚çš„è¾“å‡ºå€¼ X' æ˜¯ä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚

![image-20231129000534879](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129000534879.png)

![image-20231129000651132](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129000651132.png)

* First of all, we introduce the computation for a single neuron. For instance, consider the first neuron in the first hidden layer.
* é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»å•ä¸ªç¥ç»å…ƒçš„è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ç¬¬ä¸€ä¸ªéšè—å±‚çš„ç¬¬ä¸€ä¸ªç¥ç»å…ƒã€‚

![image-20231129001549392](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129001549392.png)

* We start from the first hidden layer. The output of the 3-th neuron in the first layer is
* æˆ‘ä»¬ä»ç¬¬ä¸€éšè—å±‚å¼€å§‹ã€‚ç¬¬ä¸€å±‚ç¬¬ 3 ä¸ªç¥ç»å…ƒçš„è¾“å‡ºä¸º

![image-20231129002048335](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129002048335.png)

![image-20231129002141982](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129002141982.png)

* The process of such layer-by-layer calculation to obtain the output of a multilayer perceptron is thus called **forward propagation.**
* å› æ­¤ï¼Œè¿™ç§é€å±‚è®¡ç®—ä»¥è·å¾—å¤šå±‚æ„ŸçŸ¥å™¨è¾“å‡ºçš„è¿‡ç¨‹è¢«ç§°ä¸º**å‰å‘ä¼ æ’­ã€‚**

##### 17.1.3.1 A Running Example

![image-20231129003344268](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129003344268.png)

![image-20231129003405620](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129003405620.png)

* è¿™é‡Œéœ€è¦è§£é‡Šä¸€ä¸‹IDå‡½æ•°å’Œsigmoidå‡½æ•°
* åœ¨ç¬¬ä¸€å±‚éšè—å±‚ä¸­ï¼Œå› ä¸ºæ‰€ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯**æ’ç­‰å‡½æ•°ï¼ˆidentity functionï¼Œé€šå¸¸è¡¨ç¤ºä¸º *id*ï¼‰**
* åœ¨ç¬¬äºŒå±‚éšè—å±‚ä¸­ï¼Œä½¿ç”¨çš„æ˜¯ **Sigmoid æ¿€æ´»å‡½æ•°**ï¼Œè¿™æ˜¯ä¸€ç§å¸¸ç”¨çš„éçº¿æ€§æ¿€æ´»å‡½æ•°

![image-20231129003414411](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129003414411.png)

![image-20231129003627641](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129003627641.png)

* é€šè¿‡ä»¥ä¸Šè®¡ç®—å¯ä»¥å¾—åˆ°å„ä¸ªèŠ‚ç‚¹çš„å€¼ï¼Œä½†æ˜¯æ³¨æ„ï¼Œç¬¬äºŒå±‚çš„èŠ‚ç‚¹çš„å€¼éœ€è¦é€šè¿‡sigmoidè®¡ç®—å¾—åˆ°X

![image-20231129003821310](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129003821310.png)

* **Issue: The hidden neurons cannot be trained by making their outputs become closer to the desired values given by the training set.** 
* **é—®é¢˜ï¼š æ— æ³•é€šè¿‡è®©éšç¥ç»å…ƒçš„è¾“å‡ºæ›´æ¥è¿‘è®­ç»ƒé›†ç»™å‡ºçš„æœŸæœ›å€¼æ¥è®­ç»ƒéšç¥ç»å…ƒã€‚**



## Lecture 18_Multilayer_Peceptron_1

### 18.1 Gradient Decent and Activation Function Design æ¢¯åº¦ä¿®æ­£å’Œæ¿€æ´»å‡½æ•°è®¾è®¡

![image-20231129005952810](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129005952810.png)

* The output error of a multilayer perceptron for the k-th input pattern is a half of the squared error:
* å¤šå±‚æ„ŸçŸ¥å™¨å¯¹ç¬¬ k ä¸ªè¾“å…¥æ¨¡å¼çš„è¾“å‡ºè¯¯å·®æ˜¯å¹³æ–¹è¯¯å·®çš„ä¸€åŠ

![image-20231129010057432](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129010057432.png)

* Now we define the performance of multilayer perceptron over a Data set D as a half of the total squared error:

* ç°åœ¨ï¼Œæˆ‘ä»¬å°†å¤šå±‚æ„ŸçŸ¥å™¨åœ¨æ•°æ®é›† D ä¸Šçš„æ€§èƒ½å®šä¹‰ä¸ºæ€»å¹³æ–¹è¯¯å·®çš„ä¸€åŠï¼š

![image-20231129011113331](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129011113331.png)

* Now we define the performance of multilayer perceptron over a Data set D as a half of the total squared error:
* ç°åœ¨ï¼Œæˆ‘ä»¬å°†å¤šå±‚æ„ŸçŸ¥å™¨åœ¨æ•°æ®é›† D ä¸Šçš„æ€§èƒ½å®šä¹‰ä¸ºæ€»å¹³æ–¹è¯¯å·®çš„ä¸€åŠï¼š

![image-20231129011331797](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129011331797.png)

![image-20231129011646959](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129011646959.png)

* During the training stage, the input value a to the network are the input vectors of the training set, which are constant parameters for the process. 
* åœ¨è®­ç»ƒé˜¶æ®µï¼Œç½‘ç»œçš„è¾“å…¥å€¼ a æ˜¯è®­ç»ƒé›†çš„è¾“å…¥å‘é‡ï¼Œå®ƒä»¬æ˜¯è¿‡ç¨‹çš„æ’å®šå‚æ•°ã€‚
* Therefore, the MLP error function E is a function of the weights of connections only:
* ![image-20231129011710473](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129011710473.png)
* å› æ­¤ï¼ŒMLP è¯¯å·®å‡½æ•° E åªæ˜¯è¿æ¥æƒé‡çš„å‡½æ•°ï¼š
* The better the MLP performs, the smaller the MLP error function E is.
* MLP çš„è¡¨ç°è¶Šå¥½ï¼ŒMLP è¯¯å·®å‡½æ•° E å°±è¶Šå°ã€‚
* Thus MLP learning can be considered as the optimization problem: min wE(w)
* å› æ­¤ï¼ŒMLP å­¦ä¹ å¯è§†ä¸ºä¼˜åŒ–é—®é¢˜ï¼š min wE(w)



### 18.2 Gradient Decent æ¢¯åº¦ä¸‹é™

![image-20231129012016382](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129012016382.png)

* Gradient descent is based on the observation that if the multi-variable function F(x) is defined and differentiable in a neighborhood of a point a, then F(x) decreases fastest if one goes from a in the direction of the negative gradient of F at a, âˆ’âˆ‡F(a) . It follows that, if
* æ¢¯åº¦ä¸‹é™æ³•çš„åŸºç¡€æ˜¯ï¼Œå¦‚æœå¤šå˜é‡å‡½æ•° F(x) åœ¨ç‚¹ a çš„é‚»åŸŸå†…æ˜¯å¯å®šä¹‰å’Œå¯å¾®åˆ†çš„ï¼Œé‚£ä¹ˆå¦‚æœä»ç‚¹ a æ²¿ F åœ¨ç‚¹ a çš„è´Ÿæ¢¯åº¦æ–¹å‘ï¼ˆ-âˆ‡F(a)ï¼‰ä¸‹é™ï¼ŒF(x) ä¸‹é™å¾—æœ€å¿«ã€‚ç”±æ­¤å¯è§ï¼Œå¦‚æœ

![image-20231129014559375](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129014559375.png)

* **<font color=red size>æ„Ÿè§‰éå¸¸åƒä¹‹å‰é«˜ä¸­çš„æ±‚å¯¼ï¼Œç›®çš„æ˜¯å¯»æ‰¾æœ€å¤§çš„å€¼</font>**



![image-20231129015054752](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129015054752.png)



![image-20231129015020017](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129015020017.png)

![image-20231129015220388](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129015220388.png)

* Gradient descent method addresses the issue of how to update weights.
* æ¢¯åº¦ä¸‹é™ç®—æ³•è§£å†³äº†å¦‚ä½•æ›´æ–°æƒé‡çš„é—®é¢˜ã€‚

![image-20231129015324874](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129015324874.png)

* One of the most popular techniques is called 
  * **error backpropagation,  ï¼ˆåå‘ä¼ æ’­ï¼‰**
* where the error of output neurons is propagated back to derive the weight adjustment of a given hidden neuron, based on how much the neuron contributes to the output error.
* å³æ ¹æ®è¾“å‡ºç¥ç»å…ƒå¯¹è¾“å‡ºè¯¯å·®çš„è´¡çŒ®ç¨‹åº¦ï¼Œå°†è¾“å‡ºç¥ç»å…ƒçš„è¯¯å·®åå‘ä¼ æ’­ï¼Œä»è€Œå¾—å‡ºç»™å®šéšè—ç¥ç»å…ƒçš„æƒé‡è°ƒæ•´ã€‚
* The backpropagation algorithm updates the weights of connections w computationally efficiently based on the method of gradient descent.
* åå‘ä¼ æ’­ç®—æ³•åŸºäºæ¢¯åº¦ä¸‹é™æ³•ï¼Œä»¥é«˜æ•ˆçš„è®¡ç®—æ–¹å¼æ›´æ–°è¿æ¥æƒé‡ wã€‚
* **è¿™é‡Œæ‰€è¯´çš„å°±æ˜¯é€šè¿‡åå‘ä¼ æ’­æ¥æ›´æ–°hidden layerä¸­çš„æƒé‡ã€‚**

![image-20231129020338420](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129020338420.png)

* Gradient descent method addresses the issue of how to update weights. 
* æ¢¯åº¦ä¸‹é™æ³•è§£å†³äº†å¦‚ä½•æ›´æ–°æƒé‡çš„é—®é¢˜ã€‚
* The backpropagation algorithm makes the weight updating efficient.
* åå‘ä¼ æ’­ç®—æ³•ä½¿æƒé‡æ›´æ–°å˜å¾—é«˜æ•ˆã€‚

![image-20231129020551102](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129020551102.png)



## Lecture21_ Genetic Algorithms

 ### 21.1 Biological Motivation

![image-20231129043542589](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129043542589.png)

* Species adapt to the environment via **<font color=red>natural selection</font>**.
* ç‰©ç§é€šè¿‡**è‡ªç„¶é€‰æ‹©**æ¥é€‚åº”ç¯å¢ƒã€‚
* The selection favours those species for survival and further evolution that are best adapted to the environmental condition â€“ **<font color=red>â€œsurvival of the fittestâ€</font>**.
* é€‰æ‹©æœ‰åˆ©äºé‚£äº›æœ€èƒ½é€‚åº”ç¯å¢ƒæ¡ä»¶çš„ç‰©ç§ç”Ÿå­˜å’Œè¿›ä¸€æ­¥è¿›åŒ–--**"é€‚è€…ç”Ÿå­˜"**ã€‚
* <font color=red>**Phenotype**</font> is the manner of response and physical embodiment of an individual. There occur small, apparently random and undirected variations between the phenotypes of parents and their offspring.
* **<font color=red>è¡¨å‹</font>**æ˜¯ä¸ªä½“çš„ååº”æ–¹å¼å’Œèº«ä½“ä½“ç°ã€‚äº²ä»£å’Œå­ä»£çš„è¡¨å‹ä¹‹é—´å­˜åœ¨ç€å¾®å°çš„ã€æ˜æ˜¾éšæœºçš„å’Œéå®šå‘çš„å˜å¼‚ã€‚
* These **mutations** prevail through selection, if they prove their worth in the current environment; otherwise they perish.
* å¦‚æœè¿™äº›**å˜å¼‚**åœ¨å½“å‰ç¯å¢ƒä¸­è¯æ˜äº†å®ƒä»¬çš„ä»·å€¼ï¼Œå®ƒä»¬å°±ä¼šé€šè¿‡é€‰æ‹©è€Œè·èƒœï¼›å¦åˆ™ï¼Œå®ƒä»¬å°±ä¼šç­äº¡ã€‚



![image-20231129045600746](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129045600746.png)

* **<font color=red>Production of offspring</font>** is the basic driving force for selection. 
* **ç”Ÿäº§åä»£**æ˜¯é€‰æ‹©çš„åŸºæœ¬åŠ¨åŠ›ã€‚
* In a favourable environment, population grows exponentially. This growth is generally limited by **finite resources.** 
* åœ¨æœ‰åˆ©çš„ç¯å¢ƒä¸­ï¼Œäººå£å‘ˆæŒ‡æ•°å¢é•¿ã€‚è¿™ç§å¢é•¿é€šå¸¸å—åˆ°**æœ‰é™èµ„æºçš„é™åˆ¶**ã€‚
* When resources are no longer sufficient to support all individuals in a population, **only the fittest, i.e. those most effectively utilizing the resources, survive and produce offspring.** 
* å½“èµ„æºä¸å†è¶³ä»¥æ”¯æŒç§ç¾¤ä¸­çš„æ‰€æœ‰ä¸ªä½“æ—¶ï¼Œ**åªæœ‰é‚£äº›æœ€é€‚åˆçš„ä¸ªä½“ï¼Œå³é‚£äº›æœ€æœ‰æ•ˆåœ°åˆ©ç”¨èµ„æºçš„ä¸ªä½“ï¼Œæ‰èƒ½ç”Ÿå­˜ä¸‹æ¥å¹¶ç¹è¡åä»£ã€‚**
  * **In an unkind environment, organisms in a population are at a selective advantage to utilize resources most effectively.** 
  * **åœ¨ä¸å‹å–„çš„ç¯å¢ƒä¸­ï¼Œç§ç¾¤ä¸­çš„ç”Ÿç‰©å…·æœ‰é€‰æ‹©æ€§ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ€æœ‰æ•ˆåœ°åˆ©ç”¨èµ„æºã€‚**

### 21.2 Neo-Darwinism Theory of Evolution (Microscope)

![image-20231129050046861](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129050046861.png)

* All living organisms consist of **cells**. 
* æ‰€æœ‰ç”Ÿç‰©ä½“éƒ½ç”±**ç»†èƒ**ç»„æˆã€‚
* Each cell in an organism contains the same set of one or more **chromosomes** 
* ç”Ÿç‰©ä½“ä¸­çš„æ¯ä¸ªç»†èƒéƒ½å«æœ‰ç›¸åŒçš„ä¸€ç»„æˆ–å¤šç»„**æŸ“è‰²ä½“**ã€‚
* Chromosomes are strings of **DNA** that serve as a **blueprint** for the organism. 
* æŸ“è‰²ä½“æ˜¯ä¸€ä¸² DNAï¼Œæ˜¯ç”Ÿç‰©ä½“çš„è“å›¾ã€‚

![image-20231129050510162](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129050510162.png)

![image-20231129050529652](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129050529652.png)

#### 21.2.1 DNA Structure DNAç»“æ„

![image-20231129050620438](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129050620438.png)

* A chromosome can be conceptually divided into genes --- **Gene is a functional block of DNA coding a particular protein.** 
* ä»æ¦‚å¿µä¸Šè®²ï¼ŒæŸ“è‰²ä½“å¯åˆ†ä¸ºåŸºå› --åŸºå› æ˜¯ DNA çš„ä¸€ä¸ªåŠŸèƒ½å—ï¼Œ**ç¼–ç ä¸€ç§ç‰¹å®šçš„è›‹ç™½è´¨**

#### 21.2.2 DNA Code DNAç¼–ç 

![image-20231129050813943](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129050813943.png)

* DNA molecule consists of 
  * two ribbons of phosphate-sugar chains, and 
  * ä¸¤æ¡ç£·é…¸-ç³–é“¾ï¼Œä»¥åŠ 
  * the horizontal rods of the pairs of nitrogenous bases holding the chains together 
  * å°†ç³–é“¾å›ºå®šåœ¨ä¸€èµ·çš„ä¸€å¯¹å«æ°®ç¢±åŸºçš„æ°´å¹³æ†
* **There exist only four different nitrogenous bases in DNA: <font color=red size=5>adenine, guanine, thymine, cytosine.</font>** 
* **DNA ä¸­åªæœ‰å››ç§ä¸åŒçš„å«æ°®ç¢±åŸºï¼šè…ºå˜Œå‘¤ã€é¸Ÿå˜Œå‘¤ã€èƒ¸è…ºå˜§å•¶å’Œèƒå˜§å•¶ã€‚**

![image-20231129051410751](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129051410751.png)

* The two chains held together by **hydrogen bonds(æ°¢é”®)** formed between **pairs of bases ç¢±åŸºå¯¹ä¹‹é—´çš„åºåˆ—** . 
* é€šè¿‡ç¢±åŸºå¯¹ä¹‹é—´å½¢æˆçš„æ°¢é”®å°†ä¸¤æ¡é“¾å›ºå®šåœ¨ä¸€èµ·ã€‚
* Pairing is highly specific. It is always that 
* é…å¯¹å…·æœ‰é«˜åº¦ç‰¹å¼‚æ€§ã€‚æ€»æ˜¯è¿™æ · 
  * Adenine pairs with Thymine, A = T; 
  * è…ºå˜Œå‘¤ä¸èƒ¸è…ºå˜§å•¶é…å¯¹ï¼ŒA = Tï¼› 
  * Guanine pairs with Cytosine, G = C. 
  * é¸Ÿå˜Œå‘¤ä¸èƒå˜§å•¶é…å¯¹ï¼ŒG = Cã€‚
* **<font color=red>The precise sequence of bases carries the genetic information.</font>** 
* **<font color=red>ç¢±åŸºçš„ç²¾ç¡®åºåˆ—æºå¸¦ç€é—ä¼ ä¿¡æ¯ã€‚</font>**

![image-20231129052106058](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129052106058.png)

* The precise sequence of bases carries the genetic information. 
* ç²¾ç¡®çš„ç¢±åŸºåºåˆ—æºå¸¦é—ä¼ ä¿¡æ¯ã€‚
* Gene is a functional block of DNA coding a particular protein. 
* åŸºå› æ˜¯ DNA çš„ä¸€ä¸ªåŠŸèƒ½å—ï¼Œç¼–ç ä¸€ç§ç‰¹å®šçš„è›‹ç™½è´¨ã€‚
* **Problem: there are just 4 letters in the DNA alphabet to code, but 20 amino acidsæ°¨åŸºé…¸ found in proteins.**
* **é—®é¢˜ï¼šDNA å­—æ¯è¡¨ä¸­åªæœ‰ 4 ä¸ªå­—æ¯å¯ä»¥ç¼–ç ï¼Œä½†è›‹ç™½è´¨ä¸­å´æœ‰ 20 ç§æ°¨åŸºé…¸ã€‚**

![image-20231129052540119](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129052540119.png)

* é¦–å…ˆï¼Œè‚¯å®šä¸å¯ä»¥ä¸€ä¸€å¯¹åº”ï¼Œå› ä¸ºæ•°é‡ä¸Šä¸å¤Ÿ

![image-20231129052628237](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129052628237.png)

* There can not be a two nitrogenous bases to one amino acid match either, as it gives just 4^2 = 16 different pairs of nucleotides < 20
* ä¹Ÿä¸å¯èƒ½å­˜åœ¨ä¸¤ä¸ªå«æ°®ç¢±åŸºå¯¹ä¸€ä¸ªæ°¨åŸºé…¸çš„åŒ¹é…ï¼Œå› ä¸ºè¿™æ ·å°±åªæœ‰ 4^2 = 16 å¯¹ä¸åŒçš„æ ¸è‹·é…¸ < 20

![image-20231129053836846](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129053836846.png)

* **Nitrogenous bases(å«æ°®ç¢±åŸºå¯¹)** in combination as a triplet are required to code for each amino acid, as this gives 4^3 = 64 possible combinations.
* æ¯ä¸ªæ°¨åŸºé…¸éƒ½éœ€è¦å«æ°®ç¢±åŸºä»¥ä¸‰è”ä½“çš„å½¢å¼ç»„åˆç¼–ç ï¼Œè¿™æ ·å°±æœ‰ 4^3 = 64 ç§å¯èƒ½çš„ç»„åˆã€‚

![image-20231129054611269](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129054611269.png)

* The genetic code is **<font color=red>universal</font>**, as the codons that code for amino acids are the same in bacteria, plants and animals.
* é—ä¼ å¯†ç æ˜¯**é€šç”¨**çš„ï¼Œå› ä¸ºç¼–ç æ°¨åŸºé…¸çš„å¯†ç å­åœ¨ç»†èŒã€æ¤ç‰©å’ŒåŠ¨ç‰©ä¸­éƒ½æ˜¯ç›¸åŒçš„ã€‚



##### 21.2.2.1 DNA Code Crossing-Over

![image-20231129054905132](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129054905132.png)

* **Organisms with unpaired sets of chromosomes are called haploid.** 
* **æŸ“è‰²ä½“ä¸æˆå¯¹çš„ç”Ÿç‰©ç§°ä¸ºå•å€ä½“ã€‚**
* **Organisms, whose chromosomes are arranged in pairs are called diploid.** Most sexually reproducing organisms are diploid.
* **æŸ“è‰²ä½“æˆå¯¹æ’åˆ—çš„ç”Ÿç‰©ç§°ä¸ºäºŒå€ä½“ã€‚**å¤§å¤šæ•°æœ‰æ€§ç”Ÿæ®–çš„ç”Ÿç‰©éƒ½æ˜¯äºŒå€ä½“ã€‚ 
* During sexual reproduction, **<font color=red>crossing-over</font>** or **<font color=red>recombination</font>** of genes in parent chromosomes occurs. In each parent cell, a pair of chromosomes first doubles, then the chromosomes exchange genes, and finally produce four gametes, single chromosomes, ready to couple with the other parent gametes to form a new diploid cell.
* åœ¨æœ‰æ€§ç”Ÿæ®–è¿‡ç¨‹ä¸­ï¼Œäº²æœ¬æŸ“è‰²ä½“ä¸Šçš„åŸºå› ä¼šå‘ç”Ÿ**äº¤å‰æˆ–é‡ç»„**ã€‚åœ¨æ¯ä¸ªäº²æœ¬ç»†èƒä¸­ï¼Œä¸€å¯¹æŸ“è‰²ä½“é¦–å…ˆåŠ å€ï¼Œç„¶åæŸ“è‰²ä½“äº¤æ¢åŸºå› ï¼Œæœ€åäº§ç”Ÿå››ä¸ªé…å­ï¼ˆå•æ¡æŸ“è‰²ä½“ï¼‰ï¼Œå‡†å¤‡ä¸å…¶ä»–äº²æœ¬é…å­ç»“åˆå½¢æˆæ–°çš„äºŒå€ä½“ç»†èƒã€‚



##### 21.2.2.2 DNA Code Mutations

![image-20231129060500575](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129060500575.png)

* **<font color=red>Mutation is a random change of a â€œletterâ€, single nucleotide in a chromosome.</font>** 
* **<font color=red>çªå˜æ˜¯æŸ“è‰²ä½“ä¸­ä¸€ä¸ª "å­—æ¯"ï¼ˆå•ä¸ªæ ¸è‹·é…¸ï¼‰çš„éšæœºå˜åŒ–ã€‚</font>**
* Mutations usually result from copying errors in parent chromosomes and then are reproduced in offspring.
* çªå˜é€šå¸¸æ˜¯ç”±äº²ä»£æŸ“è‰²ä½“çš„å¤åˆ¶é”™è¯¯å¼•èµ·çš„ï¼Œç„¶ååœ¨å­ä»£ä¸­å¤åˆ¶ã€‚

##### 21.2.2.3 Neo-Darwinism Theory of Evolution

![image-20231129060659764](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129060659764.png)

* **Gene is a functional block of DNA coding a particular protein**, see â€œgene encodes a trait, e.g. eye colourâ€. 
* **åŸºå› æ˜¯ç¼–ç ç‰¹å®šè›‹ç™½è´¨çš„ DNA åŠŸèƒ½å—**ï¼Œè§ "åŸºå› ç¼–ç æ€§çŠ¶ï¼Œå¦‚çœ¼ç›é¢œè‰²"ã€‚
* **Different possible settings for a trait**, e.g. blue, brown, black eye colour, are called alleles. 
* **ä¸€ç§æ€§çŠ¶çš„ä¸åŒå¯èƒ½è®¾ç½®**ï¼Œå¦‚è“è‰²ã€æ£•è‰²ã€é»‘è‰²çš„çœ¼ç›é¢œè‰²ï¼Œç§°ä¸ºç­‰ä½åŸºå› ã€‚
* Each gene is located at a particular **locus (position)** on the chromosome. 
* æ¯ä¸ªåŸºå› éƒ½ä½äº**æŸ“è‰²ä½“ä¸Šçš„ä¸€ä¸ªç‰¹å®šä½ç‚¹**ï¼ˆä½ç½®ï¼‰ã€‚

![image-20231129061400113](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129061400113.png)

* **Gene is a functional block of DNA coding a particular protein**, see â€œgene encodes a trait, e.g. eye colourâ€. 
* **åŸºå› æ˜¯ç¼–ç ç‰¹å®šè›‹ç™½è´¨çš„ DNA åŠŸèƒ½å—**ï¼Œè§ "åŸºå› ç¼–ç æ€§çŠ¶ï¼Œå¦‚çœ¼ç›é¢œè‰²"ã€‚
* **<font color=red>The complete collection of all genetic material,</font>** that is all chromosomes taken together, is called the organismâ€™s **<font color=red>genome or genotype</font>**.
* **æ‰€æœ‰é—ä¼ ç‰©è´¨çš„å®Œæ•´é›†åˆ**ï¼Œå³æ‰€æœ‰æŸ“è‰²ä½“åŠ åœ¨ä¸€èµ·ï¼Œç§°ä¸ºç”Ÿç‰©ä½“çš„**åŸºå› ç»„æˆ–åŸºå› å‹ã€‚**

![image-20231129061648109](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129061648109.png)

* **<font color=red>Genes are transfer units of heredity.</font>** Genes are occasionally changed by mutations. 
* **åŸºå› æ˜¯é—ä¼ çš„ä¼ é€’å•ä½ã€‚**åŸºå› å¶å°”ä¼šå› çªå˜è€Œæ”¹å˜ã€‚
* **Phenotype** expresses complex interaction within the genotype as well as its interaction with environment. 
* **è¡¨å‹**è¡¨è¾¾äº†åŸºå› å‹å†…éƒ¨å¤æ‚çš„ç›¸äº’ä½œç”¨ä»¥åŠä¸ç¯å¢ƒçš„ç›¸äº’ä½œç”¨ã€‚

* Modern biochemistry and genetics explain microscopic mechanisms of heredity. 
* ç°ä»£ç”Ÿç‰©åŒ–å­¦å’Œé—ä¼ å­¦è§£é‡Šäº†é—ä¼ çš„å¾®è§‚æœºåˆ¶ã€‚

![image-20231129062317397](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129062317397.png)

* **ç§ç¾¤æ˜¯ä¸æ–­è¿›åŒ–çš„å•ä½ã€‚**ç§ç¾¤ä½œä¸ºä¸€ä¸ªå•ä½ï¼Œé™å®šäº†ä¸€ä¸ªå…±åŒçš„åŸºå› åº“ åŒ…æ‹¬æ‰€æœ‰ä¸ªä½“çš„åŸºå› å‹ã€‚

![image-20231129062422939](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129062422939.png)

* **<font color=red size=5>Individual fitness</font>** is measured indirectly as the individual growth rate in comparison to others. 
* **ä¸ªä½“é€‚å®œæ€§**æ˜¯é€šè¿‡ä¸å…¶ä»–äººç›¸æ¯”çš„ä¸ªä½“å¢é•¿ç‡æ¥é—´æ¥è¡¡é‡çš„ã€‚
* **<font color=red>The fitness is the individual propensity to survive and reproduce in a particular environment. </font>**
* é€‚åº”æ€§æ˜¯ä¸ªä½“åœ¨ç‰¹å®šç¯å¢ƒä¸­ç”Ÿå­˜å’Œç¹æ®–çš„å€¾å‘ã€‚
* Thus, **Natural Selection** according to the individual fitness 
* å› æ­¤ï¼Œæ ¹æ®ä¸ªä½“é€‚å®œæ€§è¿›è¡Œçš„**è‡ªç„¶é€‰æ‹©** 
  * is NOT an active driving force, 
  * ä¸æ˜¯ä¸€ç§ä¸»åŠ¨çš„é©±åŠ¨åŠ›
  * is differential survival and reproduction within a population.
  * æ˜¯ç§ç¾¤å†…éƒ¨ç”Ÿå­˜å’Œç¹æ®–çš„å·®å¼‚
* 

![image-20231129070302988](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129070302988.png)

## Lecture22_Comp_appeal

* ç´§æ¥ç€Lecture21

![image-20231129070448112](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129070448112.png)

#### 22.1 Natural Evolution. Computational Appeal? **Natural Evolution. Computational Appeal?**

![image-20231129070549895](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129070549895.png)

* åŸºå› å¦‚ä½•æ˜ å°„åˆ°è¡¨å‹ï¼Œç„¶åå†ååº”åˆ°é€‚åº”æ€§ä¸Š



* map a given genotype into the corresponding phenotype and 
* å°†ç»™å®šçš„åŸºå› å‹æ˜ å°„åˆ°ç›¸åº”çš„è¡¨å‹ä¸­ï¼Œä»¥åŠ 
* map the phenotype into the individual fitness to survive and reproduce ? 
* å°†è¡¨å‹æ˜ å°„åˆ°ä¸ªä½“çš„ç”Ÿå­˜å’Œç¹æ®–èƒ½åŠ›ï¼Ÿ

![image-20231129071349474](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129071349474.png)

![image-20231129071533387](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129071533387.png)

![image-20231129071823271](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129071823271.png)

![image-20231129071836868](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129071836868.png)





##### 22.1.1 Parallelism å¹¶è¡Œæ€§

![image-20231129071636484](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129071636484.png)

* **<font color=red size=5>Parallelism å¹¶è¡Œæ€§</font>**
  * **Every individual in the population is tested independently in parallel with the others, and that speeds up evolution of the population.**
  * **ç§ç¾¤ä¸­çš„æ¯ä¸ªä¸ªä½“éƒ½è¦ä¸å…¶ä»–ä¸ªä½“åŒæ—¶æ¥å—ç‹¬ç«‹æµ‹è¯•ï¼Œè¿™å°±åŠ å¿«äº†ç§ç¾¤çš„è¿›åŒ–é€Ÿåº¦ã€‚**



##### 22.1.2 Adaptation to a changing environment. é€‚åº”å˜åŒ–çš„ç¯å¢ƒ

![image-20231129072121287](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129072121287.png)

* **<font color=red size=5>Adaptation to a changing environment.  é€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒ</font>**
  * **Due to the natural selection, only those individuals best fitted for the current environment do survive. The selection results in the population as a whole best adapted to the environment.**
  * **ç”±äºè‡ªç„¶é€‰æ‹©çš„ä½œç”¨ï¼Œåªæœ‰é‚£äº›æœ€é€‚åˆå½“å‰ç¯å¢ƒçš„ä¸ªä½“æ‰èƒ½ç”Ÿå­˜ä¸‹æ¥ã€‚é€‰æ‹©çš„ç»“æœæ˜¯æ•´ä¸ªç§ç¾¤æœ€é€‚åº”ç¯å¢ƒã€‚**



##### 22.1.3 Optimization ä¼˜åŒ–

![image-20231129072350203](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129072350203.png)

* **<font color=red size=5>Optimization ä¼˜åŒ–</font>**
* Due to the natural selection, only individuals best fitted or â€œoptimalâ€ for the current environment do survive and reproduce. Thus, the selection also performs optimization on an individual level. 
* åœ¨è‡ªç„¶é€‰æ‹©çš„ä½œç”¨ä¸‹ï¼Œåªæœ‰æœ€é€‚åˆæˆ– "æœ€ä½³ "å½“å‰ç¯å¢ƒçš„ä¸ªä½“æ‰èƒ½ç”Ÿå­˜å’Œç¹è¡ã€‚å› æ­¤ï¼Œé€‰æ‹©ä¹Ÿæ˜¯åœ¨ä¸ªä½“å±‚é¢ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚





![image-20231129072533344](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129072533344.png)

* The listed features of the natural selection are very attractive for many problems in computer science.
* å¯¹äºè®¡ç®—æœºç§‘å­¦ä¸­çš„è®¸å¤šé—®é¢˜æ¥è¯´ï¼Œè‡ªç„¶é€‰æ‹©çš„ä¸Šè¿°ç‰¹ç‚¹éå¸¸å…·æœ‰å¸å¼•åŠ›ã€‚
* As these parallelism, adaptation, and optimisation are achieved in nature by very simple manipulation of individual DNA sequences, it is tempting to use the algorithm for non-bio problem solving in engineering, computer science, etc.
* ç”±äºè¿™äº›å¹¶è¡Œæ€§ã€é€‚åº”æ€§å’Œä¼˜åŒ–æ€§åœ¨è‡ªç„¶ç•Œä¸­æ˜¯é€šè¿‡å¯¹å•ä¸ª DNA åºåˆ—è¿›è¡Œéå¸¸ç®€å•çš„æ“ä½œæ¥å®ç°çš„ï¼Œå› æ­¤å°†è¯¥ç®—æ³•ç”¨äºè§£å†³å·¥ç¨‹ã€è®¡ç®—æœºç§‘å­¦ç­‰é¢†åŸŸçš„éç”Ÿç‰©é—®é¢˜æ˜¯å¾ˆæœ‰å¸å¼•åŠ›çš„ã€‚



### 22.2 Genetic Algorithms. Historical Introduction é—ä¼ ç®—æ³•çš„å†å²ç®€ä»‹

* **By 1950s, it became clear that** 
  * **if a solution of a computational problem might be formulated, i.e., coded, in a form of string of â€œcharactersâ€,** 
  * **å¦‚æœä¸€ä¸ªè®¡ç®—é—®é¢˜çš„è§£å†³æ–¹æ¡ˆå¯ä»¥ç”¨ "å­—ç¬¦ "ä¸²çš„å½¢å¼æ¥è¡¨è¿°ï¼Œå³ç¼–ç **
  * **then the optimal solution to the problem might be searched for using the natural selection technique among a â€œpopulationâ€, i.e., set of candidate solutions.**
  * **é‚£ä¹ˆå°±å¯ä»¥åˆ©ç”¨è‡ªç„¶é€‰æ‹©æŠ€æœ¯ï¼Œåœ¨ "ç§ç¾¤"ï¼ˆå³ä¸€ç»„å€™é€‰è§£å†³æ–¹æ¡ˆï¼‰ä¸­å¯»æ‰¾é—®é¢˜çš„æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚**

![image-20231129073449846](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129073449846.png)

* 1950s-1960s -- several computer scientists independently studied evolutionary systems with the idea to use the algorithm to solve optimisation problems in engineering.
* 20 ä¸–çºª 50 å¹´ä»£è‡³ 60 å¹´ä»£ -- å‡ ä½è®¡ç®—æœºç§‘å­¦å®¶ç‹¬ç«‹ç ”ç©¶äº†è¿›åŒ–ç³»ç»Ÿï¼Œå¹¶èŒç”Ÿäº†ä½¿ç”¨è¯¥ç®—æ³•è§£å†³å·¥ç¨‹ä¼˜åŒ–é—®é¢˜çš„æƒ³æ³•ã€‚



![image-20231129073433085](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129073433085.png)

![image-20231129073635901](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129073635901.png)

![image-20231129073717202](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129073717202.png)

![image-20231129073736544](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231129073736544.png)

* In contrast with Nature, 
  * **there is no universal code in GAs,** 
  * **å…¨çƒå®šä½ç³»ç»Ÿä¸­æ²¡æœ‰é€šç”¨çš„ç¼–ç **
  * **every coding is problem dependent.** 
  * **æ¯ä¸ªç¼–ç éƒ½å–å†³äºé—®é¢˜**
* The art of coding, though left beyond the scope, is very important in Genetic Algorithms, as from the very beginning the approach depends on whether the problem can be coded as a string of characters at all. 
* ç¼–ç è‰ºæœ¯è™½ç„¶ä¸åœ¨æœ¬æ–‡è®¨è®ºèŒƒå›´ä¹‹å†…ï¼Œä½†åœ¨é—ä¼ ç®—æ³•ä¸­å´éå¸¸é‡è¦ï¼Œå› ä¸ºä»ä¸€å¼€å§‹ï¼Œæ–¹æ³•å°±å–å†³äºé—®é¢˜æ˜¯å¦èƒ½è¢«ç¼–ç æˆä¸€ä¸²å­—ç¬¦ã€‚
* The above implies serious restrictions on the class of problems capable of solving by GAs. 
* ä¸Šè¿°æƒ…å†µä¸¥é‡é™åˆ¶äº†å…¨çƒå®šä½ç³»ç»Ÿæ‰€èƒ½è§£å†³çš„é—®é¢˜ç±»åˆ«

## Lecture_23_Terminology

* ç´§æ¥ç€ä¸Šä¸€è¯¾çš„å†…å®¹

### 23.1 GAs by John Holland: Chromosomes çº¦ç¿°-éœå…°çš„åŸºå› ç®—æ³•: æŸ“è‰²ä½“

* **Definition: Genetic Algorithmâ€™s Chromosome is <font color=red>a string of characters</font>**
* **å®šä¹‰ é—ä¼ ç®—æ³•çš„æŸ“è‰²ä½“æ˜¯ä¸€ä¸²å­—ç¬¦**
  * Often the GA chromosome is defined as a
  * é€šå¸¸ï¼ŒGA æŸ“è‰²ä½“è¢«å®šä¹‰ä¸º 
    * Fixed length binary string,
    * **<font color=red>å›ºå®šé•¿åº¦çš„äºŒè¿›åˆ¶å­—ç¬¦ä¸²</font>**



* è§£é‡Šä¸€ä¸‹å…³äºåŸºå› ç®—æ³•
* ![image-20231130053906363](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130053906363.png)

![image-20231130051518727](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130051518727.png)

* The same as nature blindly operates on real chromosomes and does not know or care what information is actually coded in the chromosomes, Holland had left behind the question of what particular information and in what way was coded in the strings of the Genetic Algorithms chromosomes
* å°±åƒå¤§è‡ªç„¶ç›²ç›®åœ°å¯¹çœŸæ­£çš„æŸ“è‰²ä½“è¿›è¡Œæ“ä½œï¼Œä¸çŸ¥é“ä¹Ÿä¸å…³å¿ƒæŸ“è‰²ä½“ä¸­åˆ°åº•ç¼–ç äº†ä»€ä¹ˆä¿¡æ¯ä¸€æ ·ï¼Œéœå…°ä¹ŸæŠ›å¼€äº†é—ä¼ ç®—æ³•æŸ“è‰²ä½“ä¸²ä¸­åˆ°åº•ç¼–ç äº†ä»€ä¹ˆç‰¹å®šä¿¡æ¯ä»¥åŠä»¥ä½•ç§æ–¹å¼ç¼–ç çš„é—®é¢˜ã€‚

![image-20231130054040436](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130054040436.png)

* Haploid organisms, i.e., the organisms with on-paired chromosomes are considered in the Genetic Algorithms for some operators, e.g., crossover.
* é—ä¼ ç®—æ³•ä¸­çš„æŸäº›è¿ç®—ç¬¦ï¼ˆå¦‚äº¤å‰ï¼‰ä¼šè€ƒè™‘å•å€ä½“ç”Ÿç‰©ï¼Œå³æŸ“è‰²ä½“æˆå¯¹çš„ç”Ÿç‰©ã€‚

![image-20231130054048916](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130054048916.png)

* é—ä¼ å› å­æŸ“è‰²ä½“ä¸åŸºå› å‹é‡åˆï¼Œå³åŸºå› å‹ç”±å•æ¡æŸ“è‰²ä½“ç»„æˆã€‚

![image-20231130054309592](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130054309592.png)

* For simplicity, there is no phenotype concept in Genetic Algorithms either, and therefore we do not distinguish the following concepts:
  GAs chromosome = GAs genotype = GAs organism
* ä¸ºç®€å•èµ·è§ï¼Œé—ä¼ ç®—æ³•ä¸­ä¹Ÿæ²¡æœ‰è¡¨å‹æ¦‚å¿µã€‚å› æ­¤æˆ‘ä»¬ä¸åŒºåˆ†ä»¥ä¸‹æ¦‚å¿µï¼š
  **é—ä¼ ç®—æ³•æŸ“è‰²ä½“ = é—ä¼ ç®—æ³•åŸºå› å‹ = é—ä¼ ç®—æ³•ç”Ÿç‰©ä½“**

![image-20231130054448487](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130054448487.png)

* It is also called population of candidate solutions.
* å®ƒä¹Ÿè¢«ç§°ä¸ºå€™é€‰æ–¹æ¡ˆç¾¤ã€‚

![image-20231130060319500](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130060319500.png)

* **In a GAs chromosome, a character represents a gene.**
* **åœ¨åŸºå› ç»„æŸ“è‰²ä½“ä¸­ï¼Œä¸€ä¸ªå­—ç¬¦ä»£è¡¨ä¸€ä¸ªåŸºå› ã€‚**
* **Possible settings for a gene are called <font color=red>alleles</font>**, e.g. in the example above the alleles are 0s and 1s, and if a gene codes a trait then an allele is the trait instance. For binary chromosomes, the alleles â€œalphabetâ€ consists of just two characters, 0 and 1. There might be bigger â€œalphabetsâ€ to represent bigger number of alleles.
* **åŸºå› çš„å¯èƒ½è®¾ç½®ç§°ä¸ºç­‰ä½åŸºå› **ï¼Œä¾‹å¦‚ï¼Œåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç­‰ä½åŸºå› æ˜¯ 0 å’Œ 1ï¼Œå¦‚æœä¸€ä¸ªåŸºå› ç¼–ç ä¸€ä¸ªæ€§çŠ¶ï¼Œé‚£ä¹ˆç­‰ä½åŸºå› å°±æ˜¯è¯¥æ€§çŠ¶çš„å®ä¾‹ã€‚å¯¹äºäºŒè¿›åˆ¶æŸ“è‰²ä½“ï¼Œç­‰ä½åŸºå› çš„ "å­—æ¯è¡¨ "åªæœ‰ 0 å’Œ 1 ä¸¤ä¸ªå­—ç¬¦ã€‚
* **Position of a gene in the string is called <font color=red>locus of the gene</font>.**
* **åŸºå› åœ¨å­—ç¬¦ä¸²ä¸­çš„ä½ç½®ç§°ä¸ºåŸºå› åº§ã€‚**



### 23.2 GAs by John Holland: Crossover äº¤å‰

![image-20231130060846778](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130060846778.png)

* In nature, crossover occurs when two parents exchange parts of their corresponding chromosomes
* åœ¨è‡ªç„¶ç•Œä¸­ï¼Œå½“ä¸¤ä¸ªäº²æœ¬äº¤æ¢å…¶ç›¸åº”æŸ“è‰²ä½“çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œå°±ä¼šå‘ç”Ÿäº¤å‰ç°è±¡
* In a genetic algorithm, crossover recombines parts of two parent chromosomes to make two â€œchildrenâ€.
* åœ¨é—ä¼ ç®—æ³•ä¸­ï¼Œäº¤å‰å°†ä¸¤ä¸ªäº²ä»£æŸ“è‰²ä½“çš„ä¸€éƒ¨åˆ†é‡ç»„ä¸ºä¸¤ä¸ª "å­ä»£"ã€‚

![image-20231130070032927](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130070032927.png)

* **The crossover cutting point is chosen randomly.** 
* **äº¤å‰åˆ‡ç‚¹æ˜¯éšæœºé€‰æ‹©çš„ã€‚**
* **<font color=red size=5>One-point crossover is considered most often.</font>**
* **æœ€å¸¸è€ƒè™‘å•ç‚¹äº¤å‰ã€‚**

### 23.3 GAs by John Holland: Mutation

![image-20231130070248487](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130070248487.png)

* **<font color=red size=5>Mutation is random of allele of gene at randomly chosen location</font>**
* **çªå˜æ˜¯éšæœºé€‰æ‹©åŸºå› ç­‰ä½åŸºå› çš„ä½ç½®**

![image-20231130070448967](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130070448967.png)

* **å¯¹äºäºŒè¿›åˆ¶æŸ“è‰²ä½“æ¥è¯´ï¼Œè¿™æ„å‘³ç€åœ¨éšæœºé€‰æ‹©çš„ä½ç‚¹ä¸Šç¿»è½¬ä¸€ä¸ªæ¯”ç‰¹ï¼Œä¾‹å¦‚**
* **For binary chromosomes that means a flip of a bit at random chosen locus, e.g.,**

![image-20231130071124277](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130071124277.png)

* **For chromosomes with larger alphabet, mutation means replacement of a character at randomly chosen location with randomly chosen new character.** 
* **å¯¹äºå­—æ¯è¾ƒå¤§çš„æŸ“è‰²ä½“ï¼Œçªå˜æ„å‘³ç€ç”¨éšæœºé€‰æ‹©çš„æ–°å­—ç¬¦æ›¿æ¢éšæœºé€‰æ‹©ä½ç½®ä¸Šçš„å­—ç¬¦ã€‚**



### 23.4 GAs by John Holland: Inversion and Translocation åè½¬å’Œç§»ä½

![image-20231130071325249](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130071325249.png)

* **<font size=5>Inversion</font>** is a mutation when part of a chromosome is cut out, rotates by 180Â° and then fitted back into the same position
* **<font size=5>å€’ä½</font>**æ˜¯ä¸€ç§çªå˜ï¼Œå³æŸ“è‰²ä½“çš„ä¸€éƒ¨åˆ†è¢«åˆ‡æ‰ï¼Œæ—‹è½¬ 180Â°ï¼Œç„¶åè£…å›åŸæ¥çš„ä½ç½®ã€‚
* ![image-20231130071354734](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130071354734.png)
* **<font size=5>Translocation</font>** is a mutation when part of a chromosome is cut out and moved to a different location in the chromosome:
* **<font size=5>ç§»ä½</font>**æ˜¯æŒ‡æŸ“è‰²ä½“çš„ä¸€éƒ¨åˆ†è¢«åˆ‡æ‰å¹¶ç§»åˆ°æŸ“è‰²ä½“çš„å¦ä¸€ä¸ªä½ç½®çš„çªå˜ï¼š
* ![image-20231130071414187](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130071414187.png)



### 23.5 GAs by John Holland: Fitness Function é€‚åº”åº¦å‡½æ•°

![image-20231130071751005](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130071751005.png)

* **To evaluate a chromosome fitness**, that is how good the candidate solution solves the problem, **a Genetic Algorithm uses fitness function.** 
* **ä¸ºäº†è¯„ä¼°æŸ“è‰²ä½“çš„é€‚åº”åº¦**ï¼Œå³å€™é€‰æ–¹æ¡ˆè§£å†³é—®é¢˜çš„å¥½åï¼Œ**é—ä¼ ç®—æ³•ä½¿ç”¨äº†é€‚åº”åº¦å‡½æ•°**ã€‚
* The fitness function
  *  Takes a chromosome as an input; 
  *  å°†æŸ“è‰²ä½“ä½œä¸ºè¾“å…¥
  *  Produces its quantitative fitness evaluation as an output.
  *  å°†æŸ“è‰²ä½“çš„å®šé‡é€‚é…æ€§è¯„ä»·ä½œä¸ºè¾“å‡º

![image-20231130072141842](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130072141842.png)

![image-20231130072811565](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130072811565.png)

* The same as that a particular code to be used for coding a problem solution in a form of GAs chromosomes depends on the problem itself, a particular form of the fitness function also depends on the problem to be solved. 
* å°±åƒåœ¨é—ä¼ ç®—æ³•æŸ“è‰²ä½“å½¢å¼ä¸­ç¼–ç é—®é¢˜è§£å†³æ–¹æ¡ˆæ‰€ä½¿ç”¨çš„ç‰¹å®šä»£ç å–å†³äºé—®é¢˜æœ¬èº«ä¸€æ ·ï¼Œç‰¹å®šå½¢å¼çš„é€‚åº”åº¦å‡½æ•°ä¹Ÿå–å†³äºè¦è§£å†³çš„é—®é¢˜ã€‚

![image-20231130072741847](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130072741847.png)



![image-20231130073014785](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130073014785.png)

* Two important requirements: 
  * Should **correlate closely with the designer's goal** 
  * åº”ä¸**è®¾è®¡è€…çš„ç›®æ ‡å¯†åˆ‡ç›¸å…³**
  * Should be **computationally efficient**
  * åº”å…·**æœ‰è®¡ç®—æ•ˆç‡**

* Precise fitness function but computation is time consuming
* é€‚åˆåº¦å‡½æ•°ç²¾ç¡®ï¼Œä½†è®¡ç®—è€—æ—¶
* Approximate fitness function but very efficient 
* **è¿‘ä¼¼æ‹Ÿåˆå‡½æ•°ï¼Œä½†éå¸¸é«˜æ•ˆï¼ˆWe Chose this wayï¼‰**

![image-20231130073225398](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130073225398.png)

* Specifically, we may use approximate fitness function when:
* å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨è¿‘ä¼¼æ‹Ÿåˆå‡½æ•°
  * Precise fitness function is extremely time consuming;
  * ç²¾ç¡®æ‹Ÿåˆå‡½æ•°éå¸¸è€—æ—¶ï¼›
  * Precise fitness function is missing or hardly obtained;
  * ç²¾ç¡®æ‹Ÿåˆå‡½æ•°ç¼ºå¤±æˆ–éš¾ä»¥è·å¾—ï¼›
  * Precise fitness function model itself contains uncertainties.
  * ç²¾ç¡®æ‹Ÿåˆå‡½æ•°æ¨¡å‹æœ¬èº«åŒ…å«ä¸ç¡®å®šæ€§ã€‚

### 23.6 GAs by John Holland: Evolving unit æ¼”åŒ–å•ä½

![image-20231130073709107](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130073709107.png)

* The same as in nature, in **GAs Population** of chromosomes is **the evolving unit**. 
* è‡ªç„¶ç•Œç›¸åŒï¼Œåœ¨é—ä¼ ä¼˜åŒ–ä¸­ï¼ŒæŸ“è‰²ä½“ç¾¤æ˜¯è¿›åŒ–å•ä½ã€‚
* The GAs population evolves from **one generation** of chromosomes to the next generation and so on.
* é—ä¼ ç®—æ³•çš„ç¾¤ä½“ä»ä¸€ä»£æŸ“è‰²ä½“è¿›åŒ–åˆ°ä¸‹ä¸€ä»£æŸ“è‰²ä½“ï¼Œä»¥æ­¤ç±»æ¨ã€‚

### 23.6 GAs by John Holland: Selection Operator é€‰æ‹©æ“ä½œ

![image-20231130075110302](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130075110302.png)

*  To simulate natural evolution of population of chromosomes, there must be **a rule how to choose which chromosome is more likely to produce offspring for the next generation.** 
*  è¦æ¨¡æ‹ŸæŸ“è‰²ä½“ç¾¤ä½“çš„è‡ªç„¶è¿›åŒ–ï¼Œ**å¿…é¡»æœ‰ä¸€ä¸ªè§„åˆ™æ¥é€‰æ‹©å“ªæ¡æŸ“è‰²ä½“æ›´æœ‰å¯èƒ½ä¸ºä¸‹ä¸€ä»£äº§ç”Ÿåä»£ã€‚**
*  We shall call that rule **<font color=red>a selection operator.</font>** 
*  æˆ‘ä»¬ç§°è¿™ç§è§„åˆ™ä¸º**é€‰æ‹©ç®—å­ã€‚**
*  Usually, a selection operator is defined in a way that better fitted chromosome is selected for reproduction more often and therefore will produce more offspring.
*  é€šå¸¸ï¼Œé€‰æ‹©ç®—å­çš„å®šä¹‰æ˜¯ï¼Œæ›´é€‚åˆçš„æŸ“è‰²ä½“è¢«æ›´é¢‘ç¹åœ°é€‰æ‹©ç”¨äºç¹æ®–ï¼Œå› æ­¤ä¼šäº§ç”Ÿæ›´å¤šçš„åä»£ã€‚



### 23.7 GAs by John Holland: Search Space. æœç´¢ç©ºé—´

![image-20231130075400661](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130075400661.png)

* ç”±äºé—ä¼ ç®—æ³•çš„ç›®çš„æ˜¯å¯»æ‰¾é—®é¢˜çš„æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œå› æ­¤è‡ªç„¶è¦ä¸ºç®—æ³•å¼•å…¥ "æœç´¢ç©ºé—´"ï¼ˆSearch Spaceï¼‰çš„æ¦‚å¿µã€‚
* **å®šä¹‰ï¼š**
  **ä¸€ä¸ªé—®é¢˜æ‰€æœ‰å¯èƒ½è§£å†³æ–¹æ¡ˆçš„é›†åˆç§°ä¸ºé—ä¼ ç®—æ³•æœç´¢ç©ºé—´ã€‚**
* å› æ­¤ï¼Œæˆ‘ä»¬è¦åœ¨å¯èƒ½è§£å†³æ–¹æ¡ˆçš„æœç´¢ç©ºé—´ä¸­å¯»æ‰¾é—®é¢˜çš„æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚é—®é¢˜çš„æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚

### 23.8 GAs by John Holland: Fitness Landscape å’Œé€‚åº¦å›¾

![image-20231130075455752](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130075455752.png)

* A fitness landscape is a representation of all possible solutions along with their fitness. 
* é€‚åˆåº¦æ™¯è§‚æ˜¯æ‰€æœ‰å¯èƒ½è§£å†³æ–¹æ¡ˆåŠå…¶é€‚åˆåº¦çš„è¡¨ç¤ºã€‚
* The candidate solutions are represented by points in the coordinate plane, i.e. the search space, and corresponding fitness is measured along an additional dimension.
* å€™é€‰æ–¹æ¡ˆç”±åæ ‡å¹³é¢ï¼ˆå³æœç´¢ç©ºé—´ï¼‰ä¸Šçš„ç‚¹è¡¨ç¤ºï¼Œç›¸åº”çš„é€‚é…åº¦åˆ™æ²¿ç€é¢å¤–çš„ç»´åº¦è¿›è¡Œæµ‹é‡ã€‚

![image-20231130075655203](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130075655203.png)

- é€‚åˆåº¦æ™¯è§‚æ˜¯æ‰€æœ‰å¯èƒ½è§£å†³æ–¹æ¡ˆåŠå…¶é€‚åˆåº¦çš„è¡¨ç¤ºã€‚
- è¿™é‡Œä¼šæœ‰ "å±±å³°"ã€"ä¸˜é™µ "å’Œ "å±±è°·"ï¼Œå°±åƒè‡ªç„¶æ™¯è§‚ä¸€æ ·ã€‚æ™¯è§‚ã€‚
- **<font size=5>è¿›åŒ–ä½¿ç§ç¾¤ å‘é€‚åˆåº¦çš„å³°é¡¶ç§»åŠ¨ æ™¯è§‚ã€‚</font>**

## Lecture24_Basic_GAs

### 24.1 Basic Genetic Algorithms

![image-20231130214203640](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130214203640.png)

* a **â€œpopulationâ€** of binary strings which he called **â€œchromosomesâ€**.
* ä»–ç§°ä¹‹ä¸º "æŸ“è‰²ä½“ "çš„äºŒè¿›åˆ¶å­—ç¬¦ä¸² "ç§ç¾¤"ã€‚
* The **â€œpopulationâ€** evolves using kind of **â€œnatural selectionâ€** together with the genetics- inspired operators of **crossover, mutation, and inversion**.
* ç§ç¾¤ "é€šè¿‡ "**è‡ªç„¶é€‰æ‹©** "ä¸é—ä¼ å­¦å¯å‘çš„**äº¤å‰ã€çªå˜å’Œåè½¬**è¿ç®—ç¬¦ä¸€èµ·è¿›åŒ–ã€‚
* **Bits in a â€œchromosomeâ€ represent genes, and each â€œgeneâ€ is an instance of a particular â€œalleleâ€, 0 or 1.**
* æŸ“è‰²ä½“ "ä¸­çš„æ¯”ç‰¹ä»£è¡¨åŸºå› ï¼Œæ¯ä¸ª "åŸºå›  "éƒ½æ˜¯ç‰¹å®š "ç­‰ä½åŸºå› "ï¼ˆ0 æˆ– 1ï¼‰çš„å®ä¾‹ã€‚
* The **selection operator** chooses those chromosomes in the population that will be allowed to reproduce, and on average the fitter chromosomes produce more offspring than the less fit ones.
* **é€‰æ‹©ç®—å­**é€‰æ‹©ç§ç¾¤ä¸­å¯ä»¥ç¹æ®–çš„æŸ“è‰²ä½“ï¼Œå¹³å‡è€Œè¨€ï¼Œé€‚åˆçš„æŸ“è‰²ä½“æ¯”ä¸é€‚åˆçš„æŸ“è‰²ä½“äº§ç”Ÿæ›´å¤šçš„åä»£ã€‚
* **Crossover** exchange subparts of two chromosomes
* **äº¤å‰äº¤æ¢**ä¸¤ä¸ªæŸ“è‰²ä½“çš„å­éƒ¨åˆ†
* **Mutation** randomly changes the allele values at some locations in the chromosome.
* **çªå˜**éšæœºæ”¹å˜æŸ“è‰²ä½“ä¸ŠæŸäº›ä½ç½®çš„ç­‰ä½åŸºå› å€¼ã€‚
* **Inversion** reverses the order of a contiguous section of the chromosome rearranging the genes order.
* **å€’ç½®**å°†æŸ“è‰²ä½“ä¸Šè¿ç»­éƒ¨åˆ†çš„é¡ºåºé¢ å€’è¿‡æ¥ï¼Œé‡æ–°æ’åˆ—åŸºå› é¡ºåºã€‚



#### 24.1.1 Basic Structure of a Genetic Algorithm

![image-20231130214911289](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130214911289.png)

1. Randomly generate initial population of n bit strings (â€œchromosomesâ€). 

2. éšæœºç”Ÿæˆç”± n ä¸ªæ¯”ç‰¹ä¸²ï¼ˆ"æŸ“è‰²ä½“"ï¼‰ç»„æˆçš„åˆå§‹ç¾¤ä½“ã€‚

3. **<font color=red size=5>Evaluate</font>** the fitness of each string in the population. 

4. è¯„ä¼°ç§ç¾¤ä¸­æ¯ä¸ªå­—ç¬¦ä¸²çš„é€‚åˆåº¦ã€‚

5. Repeat the following steps until next generation of n individual produced. 

   a. **<font color=red size=5>Select</font>** pair of parent chromosomes from current population according to their fitness, i.e. chromosomes with higher fitness are selected more often. 

   æ ¹æ®é€‚é…åº¦ä»å½“å‰ç§ç¾¤ä¸­é€‰æ‹©ä¸€å¯¹çˆ¶æŸ“è‰²ä½“ï¼Œå³é€‚é…åº¦è¶Šé«˜çš„æŸ“è‰²ä½“è¢«é€‰æ‹©çš„æ¬¡æ•°è¶Šå¤šã€‚

   b. Apply **<font color=red size=5>crossover</font>** (with probability). 

    åº”ç”¨äº¤å‰ï¼ˆæ¦‚ç‡ï¼‰ã€‚

   c. Apply **<font color=red size=5>mutation</font>** (with probability of occurrence) . 

   åº”ç”¨çªå˜æ¦‚ç‡

6. a. æ ¹æ®é€‚é…åº¦ä»å½“å‰ç§ç¾¤ä¸­é€‰æ‹©ä¸€å¯¹çˆ¶æŸ“è‰²ä½“ï¼Œå³é€‚é…åº¦è¶Šé«˜çš„æŸ“è‰²ä½“è¢«é€‰æ‹©çš„æ¬¡æ•°è¶Šå¤šã€‚ b. åº”ç”¨äº¤å‰ï¼ˆæ¦‚ç‡ï¼‰ã€‚

7. Apply generational **<font color=red size=5>replacement</font>**. 

8. åº”ç”¨ä¸–ä»£**æ›¿æ¢** 

9. Go to 2 or terminate if termination condition is met.

10. å¦‚æœæ»¡è¶³ç»ˆæ­¢æ¡ä»¶ï¼Œåˆ™è½¬åˆ° 2 æˆ–ç»ˆæ­¢ã€‚

##### 24.1.1.1 Example Implementation of a GA

![image-20231130215651741](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130215651741.png)

* Let length of each chromosome (binary string) l = 8. And the number of chromosomes in the population (population size) n = 4. 
* å‡è®¾æ¯æ¡æŸ“è‰²ä½“ï¼ˆäºŒè¿›åˆ¶å­—ç¬¦ä¸²ï¼‰çš„é•¿åº¦ä¸º l = 8ï¼Œç§ç¾¤ä¸­æŸ“è‰²ä½“çš„æ•°é‡ï¼ˆç§ç¾¤å¤§å°ï¼‰ä¸º n = 4ã€‚
* Fitness function f(x) is equal to **the number of ones in the bit string x.** 
* é€‚åˆåº¦å‡½æ•° f(x) **ç­‰äºæ¯”ç‰¹ä¸² x ä¸­çš„ 1 çš„ä¸ªæ•°ã€‚**
* Selection operator. Fitness-proportionate selection, that is, the number of times an individual expected to be selected is equal to its fitness fi divided by the average fitness of the population Ë‰*f*Ë‰(å¹³å‡å€¼)(also known as **Roulette Wheel Selection**):
* é€‰æ‹©è¿ç®—ç¬¦ã€‚é€‚åˆåº¦æ¯”ä¾‹é€‰æ‹©ï¼Œå³ä¸ªä½“è¢«é€‰ä¸­çš„æ¬¡æ•°ç­‰äºå…¶é€‚åˆåº¦ fi é™¤ä»¥ç§ç¾¤çš„å¹³å‡é€‚åˆåº¦ f(å¹³å‡å€¼)ï¼ˆåˆç§°**è½®ç›˜é€‰æ‹©**ï¼‰ï¼š

![image-20231130220457813](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130220457813.png)

###### 24.1.1.1.1 The First Generation

![image-20231130220615737](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130220615737.png)

![image-20231130220630421](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130220630421.png)

![image-20231130220716036](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130220716036.png)

![image-20231130221337390](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130221337390.png)

* chromosome number 1 shall be selected one time, 
* 1 å·æŸ“è‰²ä½“åº”é€‰æ‹©ä¸€æ¬¡
* chromosome number 2 shall be selected two times, 
* 2å·æŸ“è‰²ä½“åº”é€‰æ‹©ä¸¤æ¬¡
* chromosome number 3 shall not be selected at all, 
* ç¬¬ 3 å·æŸ“è‰²ä½“æ ¹æœ¬ä¸ä¼šè¢«é€‰ä¸­
* chromosome number 4 shall be selected one time. 
* ç¬¬ 4 å·æŸ“è‰²ä½“åº”é€‰æ‹©ä¸€æ¬¡

* ![image-20231130221359023](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130221359023.png)

![image-20231130221500236](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130221500236.png)

* **<font color=red>é€‰æ‹©Næœ€é«˜çš„ä½œä¸ºçˆ¶æœ¬è¿›è¡Œäº¤å‰</font>**

![image-20231130222115384](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231130222115384.png)

* **Use a generator producing random numbers in the range between 0 and 1 to get a random number r for each pair of parental chromosomes.** 
* **ä½¿ç”¨éšæœºæ•°å‘ç”Ÿå™¨ï¼Œåœ¨ 0 å’Œ 1 ä¹‹é—´äº§ç”Ÿéšæœºæ•°ï¼Œä¸ºæ¯å¯¹äº²æœ¬æŸ“è‰²ä½“è·å–éšæœºæ•° rã€‚**
* **If the random number r produced by the generator for the pair of â€œparentsâ€ is less or equal to the crossover probability pc, then apply crossover to the â€œparentsâ€ at randomly chosen locus, otherwise the parents do not crossover.** 
*  **å¦‚æœç”Ÿæˆå™¨ä¸ºä¸€å¯¹ "äº²æœ¬ "äº§ç”Ÿçš„éšæœºæ•° r å°äºæˆ–ç­‰äºäº¤å‰æ¦‚ç‡ pcï¼Œåˆ™åœ¨éšæœºé€‰æ‹©çš„ä½ç‚¹ä¸Šå¯¹ "äº²æœ¬ "è¿›è¡Œäº¤å‰ï¼Œå¦åˆ™äº²æœ¬ä¸è¿›è¡Œäº¤å‰ã€‚**
* **If a pair of parents do not undergo crossover, their offspring are their identical copies**
* **å¦‚æœä¸€å¯¹ "äº²æœ¬ "ä¸è¿›è¡Œäº¤å‰ï¼Œé‚£ä¹ˆå®ƒä»¬çš„åä»£å°±æ˜¯å®Œå…¨ç›¸åŒçš„æ‹·è´**



![image-20231201000630789](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201000630789.png)

![image-20231201000723629](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201000723629.png)

* Use a generator producing random numbers in the range between 0 and 1 to get a random number r for each new chromosome. 
* ä½¿ç”¨éšæœºæ•°å‘ç”Ÿå™¨ï¼Œåœ¨ 0 å’Œ 1 ä¹‹é—´äº§ç”Ÿéšæœºæ•°ï¼Œä¸ºæ¯æ¡æ–°æŸ“è‰²ä½“è·å–ä¸€ä¸ªéšæœºæ•° rã€‚
* If the random number r is less or equal to the mutation probability pm(, apply mutation to the chromosome, i.e., flip a bit at randomly chosen locus.
* å¦‚æœéšæœºæ•° r å°äºæˆ–ç­‰äºçªå˜æ¦‚ç‡ pm(ï¼Œåˆ™å¯¹æŸ“è‰²ä½“è¿›è¡Œçªå˜ï¼Œå³åœ¨éšæœºé€‰æ‹©çš„ä½ç‚¹ä¸Šç¿»è½¬ä¸€ä½ã€‚

![image-20231201001053466](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201001053466.png)

![image-20231201001208119](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201001208119.png)

![image-20231201001718094](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201001718094.png)



###### 24.1.1.1.2 The Second Generation

**<font size=5>ç¬¬äºŒä¸ªä¾‹å­</font>**

![image-20231201002511741](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002511741.png)

![image-20231201002521589](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002521589.png)

![image-20231201002650559](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002650559.png)

![image-20231201002840659](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002840659.png)



![image-20231201002851329](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002851329.png)

![image-20231201002859929](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002859929.png)

![image-20231201002911086](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002911086.png)

![image-20231201002959902](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201002959902.png)

![image-20231201003013929](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201003013929.png)

![image-20231201003032901](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201003032901.png)

###### 24.1.1.1.3 The Third Generation

<font size=5>ç¬¬ä¸‰ä¸ªä¾‹å­</font>

![image-20231201004326283](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004326283.png)

![image-20231201004410954](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004410954.png)

![image-20231201004427316](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004427316.png)

![image-20231201004451421](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004451421.png)

![image-20231201004502347](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004502347.png)

![image-20231201004549032](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004549032.png)

![image-20231201004612330](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004612330.png)

![image-20231201004912704](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201004912704.png)

###### 24.1.1.1.4 Evaluate The Fitness

![image-20231201005007720](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201005007720.png)

![image-20231201005015953](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201005015953.png)

* **<font color=red>The average fitness in the population of possible solutions increases with every generation. </font>**
* **å¯èƒ½çš„è§£å†³æ–¹æ¡ˆç¾¤ä½“çš„å¹³å‡é€‚åˆåº¦æ¯ä¸€ä»£éƒ½åœ¨å¢åŠ **ã€‚

![image-20231201005023448](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201005023448.png)

## Lecture25_Schema_preliminary

![image-20231201012202852](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201012202852.png)

![image-20231201012303466](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201012303466.png)

![image-20231201012338912](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201012338912.png)



![image-20231201012352461](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201012352461.png)

![image-20231201012403620](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201012403620.png)

### 25.1 Why do GAs work?

![image-20231201012530542](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201012530542.png)

* Observation 1: **similar â€œlookingâ€ chromosomes do have similar fitness values.**
* è§‚å¯Ÿç»“æœ 1ï¼š**"çœ‹èµ·æ¥ "ç›¸ä¼¼çš„æŸ“è‰²ä½“ç¡®å®å…·æœ‰ç›¸ä¼¼çš„é€‚åº”åº¦å€¼ã€‚**
* Thus, we can obtain an almost optimal chromosome by searching for a chromosome that â€œlooksâ€ similar to the optimal solution;
* å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æœç´¢ "çœ‹èµ·æ¥ "ä¸æœ€ä¼˜è§£ç›¸ä¼¼çš„æŸ“è‰²ä½“ï¼Œå¾—åˆ°ä¸€ä¸ªå‡ ä¹æœ€ä¼˜çš„æŸ“è‰²ä½“ï¼›
* Observation 2: A chromosome can be described by a set of â€œsubstringsâ€. Thus, **<font color=red>Similar â€œlookingâ€ chromosomes</font> âŸº <font color=red>Their â€œsubstringsâ€ are similar</font>**
* è§‚å¯Ÿç»“æœ 2ï¼šæŸ“è‰²ä½“å¯ä»¥ç”¨ä¸€ç»„ "å­ä¸² "æ¥æè¿°ã€‚å› æ­¤ï¼Œ**"çœ‹èµ·æ¥ "ç›¸ä¼¼çš„æŸ“è‰²ä½“ âŸº å®ƒä»¬çš„ "å­ä¸² "ç›¸ä¼¼**

#### 25.1.1 Observation1

![image-20231201013005249](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201013005249.png)

![image-20231201013120846](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201013120846.png)

*  similar â€œlookingâ€ chromosomes do have similar fitness values, and therefore are taken for reproduction equally often. 
* çœ‹èµ·æ¥ "ç›¸ä¼¼ "çš„æŸ“è‰²ä½“ç¡®å®å…·æœ‰ç›¸ä¼¼çš„é€‚åº”åº¦å€¼ï¼Œå› æ­¤è¢«ç”¨äºç¹æ®–çš„æ¬¡æ•°ä¹Ÿç›¸åŒã€‚

![image-20231201013205403](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201013205403.png)

* Observation 1 leads to a fact: 
* è§‚å¯Ÿç»“æœ 1 æ­ç¤ºäº†ä¸€ä¸ªäº‹å®
  * **<font color=red>Similarities among highly fit strings guide the search. </font>**
  * **<font color=red>é«˜åº¦åŒ¹é…çš„å­—ç¬¦ä¸²ä¹‹é—´çš„ç›¸ä¼¼æ€§ä¼šå¼•å¯¼æœç´¢ã€‚</font>**
* Combining with Observation 2 (**Similar chromosomes âŸº Similar Substrings**), we focus on the similar â€œsubstringsâ€ of chromosomes.
* ç»“åˆè§‚å¯Ÿç»“æœ 2ï¼ˆç›¸ä¼¼çš„æŸ“è‰²ä½“ âŸº ç›¸ä¼¼çš„å­ä¸²ï¼‰ï¼Œæˆ‘ä»¬å°†é‡ç‚¹æ”¾åœ¨æŸ“è‰²ä½“çš„ç›¸ä¼¼ "å­ä¸² "ä¸Šã€‚

#### 25.1.2 Similarity Template = Schema ç›¸ä¼¼æ€§æ¨¡æ¿ = æ¨¡å¼

![image-20231201014152993](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201014152993.png)

* Holland introduced a 
* è·å…°å¼•å…¥äº†ä¸€ä¸ª 
  * **similarity template = schema = building block** 
  * **ç›¸ä¼¼æ€§æ¨¡æ¿ = æ¨¡å¼ = æ„å»ºæ¨¡å—** 
* **to describe subset of strings with similarities at certain positions.** 
* **æ¥æè¿°åœ¨æŸäº›ä½ç½®ä¸Šå…·æœ‰ç›¸ä¼¼æ€§çš„å­—ç¬¦ä¸²å­é›†ã€‚**

![image-20231201014451035](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201014451035.png)

* **A schema is a similarity template describing **
* **æ¨¡å¼æ˜¯ä¸€ç§ç›¸ä¼¼æ€§æ¨¡æ¿**
* **<font color=red size=5>a subset of strings with similarities at certain string positions.</font>**
* **ç”¨äºæè¿°åœ¨ç‰¹å®šå­—ç¬¦ä¸²ä½ç½®ä¸Šå…·æœ‰ç›¸ä¼¼æ€§çš„å­—ç¬¦ä¸²å­é›†ã€‚**
*  **Other name for a schema is a <font color=red>building block</font> of a chromosome.**
* **æ¨¡å¼çš„å¦ä¸€ä¸ªåç§°æ˜¯æŸ“è‰²ä½“çš„æ„ä»¶ã€‚**



![image-20231201015322323](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201015322323.png)

* To describe building blocks of binary chromosomes, the following three letter alphabet is used.
* ä¸ºäº†æè¿°äºŒè¿›åˆ¶æŸ“è‰²ä½“çš„æ„å»ºæ¨¡å—ï¼Œä½¿ç”¨äº†ä»¥ä¸‹ä¸‰ä¸ªå­—æ¯çš„å­—æ¯è¡¨ã€‚
* ![image-20231201015525809](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201015525809.png)
* Here âˆ— is a wildcard symbol, i.e., is a kind of **placeholder**, which can be interpreted as a number of literal characters.
* è¿™é‡Œçš„ âˆ— æ˜¯ä¸€ä¸ªé€šé…ç¬¦å·ï¼Œå³ä¸€ç§**å ä½ç¬¦**ï¼Œå¯ä»¥è§£é‡Šä¸ºå¤šä¸ªæ–‡å­—å­—ç¬¦ã€‚
* ![image-20231201015534162](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201015534162.png)
* Example: 111**0 is a schema of the chromosome 111100
* ä¾‹å¦‚ï¼š111**0 æ˜¯æŸ“è‰²ä½“ 111100 çš„æ¨¡å¼

![image-20231201015730102](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201015730102.png)

* Example: 111**0 is a schema of the chromosome 111100 
* ä¾‹å¦‚ï¼š111**0 æ˜¯æŸ“è‰²ä½“ 111100 çš„æ¨¡å¼ 
* The meaning of the schema is that it works a **pattern matching device:** 
* æ¨¡å¼çš„æ„ä¹‰åœ¨äºå®ƒæ˜¯ä¸€ç§**æ¨¡å¼åŒ¹é…è£…ç½®**ï¼š 
  * **a schema matches a particular string âŸº at every location in the schema a 1 matches 1 in the string, a 0 matches a 0, or a * matches either.** 
  * **æ¨¡å¼åŒ¹é…ç‰¹å®šçš„å­—ç¬¦ä¸² âŸº åœ¨æ¨¡å¼ä¸­çš„æ¯ä¸ªä½ç½®ï¼Œ1 ä¸å­—ç¬¦ä¸²ä¸­çš„ 1 åŒ¹é…ï¼Œ0 ä¸å­—ç¬¦ä¸²ä¸­çš„ 0 åŒ¹é…ï¼Œæˆ–è€… * ä¸å­—ç¬¦ä¸²ä¸­çš„ä»»ä¸€ä¸ªåŒ¹é…ã€‚**

![image-20231201023238433](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201023238433.png)

![image-20231201023328054](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201023328054.png)

* To match a particular chromosome of a fixed length the schema must 
* è¦ä¸å›ºå®šé•¿åº¦çš„ç‰¹å®šæŸ“è‰²ä½“åŒ¹é…ï¼Œæ¨¡å¼å¿…é¡» 
  * a) be of the same length, 
  * é•¿åº¦ç›¸åŒ
  * b) have either 
    * the same defining symbol as the chromosome does or 
    * ä¸æŸ“è‰²ä½“çš„å®šä¹‰ç¬¦å·ç›¸åŒï¼Œæˆ–è€…
    * an â€œ*â€œ â€“ placeholder symbol - at any particular locus
    * åœ¨ä»»ä½•ç‰¹å®šåŸºå› åº§ä¸Šæœ‰ä¸€ä¸ª "*"--å ä½ç¬¦

![image-20231201023356007](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201023356007.png)

* Thus, for each of the k placeholder positions, the chromosome that can be matched by the schema can have a 1 or a 0. So there will be 2^k chromosomes.
* å› æ­¤ï¼Œå¯¹äº k ä¸ªå ä½ç¬¦ä½ç½®ä¸­çš„æ¯ä¸ªä½ç½®ï¼Œæ¨¡å¼å¯åŒ¹é…çš„æŸ“è‰²ä½“å¯ä»¥æ˜¯ 1 æˆ– 0ã€‚



![image-20231201031349842](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201031349842.png)

![image-20231201031601104](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201031601104.png)

![image-20231201031629617](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201031629617.png)

* **How many building blocks are there in a particular chromosome of a fixed length l?** 
* **å›ºå®šé•¿åº¦ä¸º l çš„ç‰¹å®šæŸ“è‰²ä½“ä¸­æœ‰å¤šå°‘æ„ä»¶ï¼Ÿ**
* To match a particular chromosome of a fixed length the schema must 
* è¦ä¸å›ºå®šé•¿åº¦çš„ç‰¹å®šæŸ“è‰²ä½“ç›¸åŒ¹é…ï¼Œæ¨¡å¼å¿…é¡» 
  * a) be of the same length, 
  * é•¿åº¦ç›¸åŒ
  * b) have either 
    * the same symbol as the chromosome does or 
    * ä¸æŸ“è‰²ä½“ç¬¦å·ç›¸åŒï¼Œæˆ–è€… 
    * an â€œ*â€œ â€“ placeholder symbol - at any particular locus
    * åœ¨ä»»ä½•ç‰¹å®šåŸºå› åº§ä¸Šæœ‰ä¸€ä¸ª "*"--**å ä½ç¬¦**



* Thus, in a binary chromosome of a length l there are 2^l building blocks, as a symbol at a particular locus in the chromosome must correspond to the same or the placeholder symbol in the corresponding locus in the schema. 
* å› æ­¤ï¼Œåœ¨é•¿åº¦ä¸º l çš„äºŒè¿›åˆ¶æŸ“è‰²ä½“ä¸­ï¼Œæœ‰ 2^l ä¸ªæ„å»ºæ¨¡å—ï¼Œå› ä¸ºæŸ“è‰²ä½“ä¸­ç‰¹å®šä½ç‚¹ä¸Šçš„ç¬¦å·å¿…é¡»ä¸æ¨¡å¼ä¸­ç›¸åº”ä½ç‚¹ä¸Šçš„ç›¸åŒæˆ–å ä½ç¬¦å·ç›¸å¯¹åº”ã€‚

##### 25.1.2.1 Order

![image-20231201032007881](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201032007881.png)

* **<font color=red>The Order of a schema H is represented as O(H) , denoting the number of defining symbols (non-* symbols) it contains.</font>**
* **æ¨¡å¼ H çš„é¡ºåºç”¨ O(H)è¡¨ç¤ºï¼Œè¡¨ç¤ºå®ƒåŒ…å«çš„å®šä¹‰ç¬¦å·ï¼ˆé*ç¬¦å·ï¼‰çš„æ•°é‡ã€‚**



##### 25.1.2.2 Defining Length

![image-20231201033150312](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201033150312.png)

![image-20231201033356208](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201033356208.png)

**å®šä¹‰**ï¼šæ¨¡å¼çš„å®šä¹‰é•¿åº¦ *Î´*(*H*) æŒ‡çš„æ˜¯åœ¨æ¨¡å¼ä¸­ï¼Œä¸¤ä¸ªå®šä¹‰ç¬¦å·ï¼ˆå³éâ€œ*â€ç¬¦å·ï¼‰ä¹‹é—´çš„æœ€å¤§è·ç¦»ã€‚



![image-20231201033420026](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201033420026.png)





## Lecture26_Schema_Theorem

![image-20231201033632594](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201033632594.png)

### 26.1 Schema Theorem

![image-20231201035310343](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201035310343.png)

![image-20231201035824236](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201035824236.png)

#### 26.1.1 Schema Theorem: Proof

##### 26.1.1.1 crossover

![image-20231201040528086](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201040528086.png)

![image-20231201040544213](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201040544213.png)

![image-20231201042214192](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201042214192.png)

![image-20231201042200795](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201042200795.png)



##### 26.1.1.2 mutation

![image-20231201042315681](D:\ç¬”è®°\ç¬”è®°å›¾ç‰‡\image-20231201042315681.png)





















