# COMP219

##  Week3 Lecture5 Safety and Reliability Issues

![image-20221012110935059](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221012110935059.png)

### 1. Safely Concerns

#### 1.1 Generalisation Error

![image-20221012111725284](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221012111725284.png)

## Lecture8 Decision - Tree - Learning 

ä¸»è¦è®²çš„å°±æ˜¯è‡ªä¸Šè€Œä¸‹çš„å†³ç­–æ ‘ç®—æ³•



### 1. The history of Decision Tree Algorithms

* ID3, or Iterative Dichotomizer(ID3 æˆ–äºŒåˆ†è¿­ä»£å™¨
  * åœ¨æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šæµ‹è¯•ä¸€ä¸ªå±æ€§
  * æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Š
* CART, or Classification and Regression Trees()
  * äºŒå‰æ ‘
  * é€šè¿‡äºŒåˆ†æ ‡å‡†é€‰æ‹©æ‹†åˆ†
* C4.5,æ”¹è¿›çš„ID3ç‰ˆæœ¬

â€‹	

### 2. Top-down Decision Tree Learning



### 3. Occamâ€™s razo



## Lab3 Decision-Tree



## Lecture10 Overfitting and K-Nearest Neighbour



### 1. Overfitting

#### 1.1 Overfitting in Decision Tree



##### 1.1.1 ç”±äºæ•°æ®å™ªå£°å¯¼è‡´çš„è¿‡æ‹Ÿåˆ

##### 1.1.2 overfitting with noise-free data 

ä¸çŸ¥é“å™ªå£°æ˜¯ä»€ä¹ˆçš„æƒ…å†µ

ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿçœ‹å½•æ’­







### 2. K-NN



# ------------------------------------------------------

## Lecture 3 

![image-20230125170549116](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230125170549116.png)

![image-20230125170613301](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230125170613301.png)



## Lecture4  Probability  Foundation for Machine Learning

## 1. Probaility Foundations æ¦‚ç‡è®ºåŸºç¡€

### 1.1 Random Variables

* We have a population of students   æˆ‘ä»¬æœ‰ä¸€å®šæ•°é‡çš„å­¦ç”Ÿ

  * We want to reason about their grades æˆ‘ä»¬æƒ³è¦æ¨ç†ä»–ä»¬çš„æˆç»©
  * Random variable: Grade éšæœºå˜é‡ æˆç»©
  * P(Grade) associates a probability with each outcome Val(Grade)={ A, B, C } P(ç­‰çº§)å…³è”æ¯ä¸ªç»“æœçš„æ¦‚ç‡Val(ç­‰çº§)={a, B, C}

* ![image-20221127170428116](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127170428116.png)

* ![image-20221127170358374](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127170358374.png)

  * Distribution is referred to as a **multinomial** 
  * åˆ†å¸ƒå¼è¢«ç§°ä¸º**å¤šé¡¹**
  * If **Val{X}={false,true}** then it is a **Bernoullidistribution** 
  * é‚£ä¹ˆå®ƒå°±æ˜¯**ä¼¯åŠªåˆ©åˆ†å¸ƒ**
  * P(X) is known as the **marginal distribution** of X       P(X)è¢«ç§°ä¸ºXçš„**è¾¹ç•Œåˆ†å¸ƒ**

  

### 1.2 Joint and Conditional Distributions  è”åˆå’Œæ¡ä»¶åˆ†å¸ƒ

#### 1.2.1 Recap: Marginal, Joint, Conditional Probality

* **Marginal Probability: è¾¹ç¼˜æ¦‚ç‡**  the probability of an event occurring P(A), it may be thought of as an unconditional probability. It is not conditioned on another event.
* å‘ç”ŸP(A)äº‹ä»¶çš„æ¦‚ç‡,å¯èƒ½è¢«è®¤ä¸ºæ˜¯**æ— æ¡ä»¶çš„æ¦‚ç‡**ã€‚å®ƒä¸å—å¦ä¸€ä¸ªäº‹ä»¶çš„çº¦æŸã€‚
* ä¸‹é¢çš„ä¾‹å­ä»¥æ‰‘å…‹ç‰Œä¸¾ä¾‹ï¼š

![image-20221127171139068](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127171139068.png)

* **Joint probability: è”åˆæ¦‚ç‡**   P(A and B). The probability of event A and event B occurring. It is the probability of the intersection of two or more events. The probability of the intersection of A and B may be written P(A âˆ© B).  
* äº‹ä»¶Aå’Œäº‹ä»¶Bå‘ç”Ÿçš„æ¦‚ç‡ã€‚è¿™æ˜¯ä¸¤ä¸ªæˆ–å¤šä¸ªäº‹ä»¶äº¤å‰çš„æ¦‚ç‡ã€‚Aå’ŒBç›¸äº¤çš„æ¦‚ç‡å¯ä»¥å†™æˆP(Aâˆ©B)ã€‚ ä¸¤ä»¶äº‹åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚
* ä¸‹é¢çš„ä¾‹å­ä»¥æ‰‘å…‹ç‰Œä¸¾ä¾‹ï¼š

![image-20221127171339883](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127171339883.png)

* **Conditional probabilityï¼šæ¡ä»¶æ¦‚ç‡** P(A|B) is the probability of event A occurring, given that event B occurs.  P(A|B)æ˜¯äº‹ä»¶Aå‘ç”Ÿçš„æ¦‚ç‡ï¼Œå‰ææ˜¯äº‹ä»¶Bå‘ç”Ÿã€‚
* ä¸‹é¢çš„ä¾‹å­ä»¥æ‰‘å…‹ç‰Œä¸¾ä¾‹ï¼š

![image-20221127171520967](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127171520967.png)



#### 1.2.2 Joint Distribution è”åˆåˆ†å¸ƒ

**è”åˆåˆ†å¸ƒ(joint distribution)æè¿°äº†å¤šä¸ªéšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ˜¯å¯¹å•ä¸€éšæœºå˜é‡çš„è‡ªç„¶æ‹“å±•ã€‚è”åˆåˆ†å¸ƒçš„å¤šä¸ªéšæœºå˜é‡éƒ½å®šä¹‰åœ¨åŒä¸€ä¸ªæ ·æœ¬ç©ºé—´ä¸­ã€‚**

* We are interested in questions involving several random variables   æˆ‘ä»¬å¯¹æ¶‰åŠå‡ ä¸ªéšæœºå˜é‡çš„é—®é¢˜å¾ˆæ„Ÿå…´è¶£

  ![image-20221127175251733](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127175251733.png)

* åœ¨æˆç»©Aï¼Œæ™ºåŠ›é«˜çš„æƒ…å†µä¸‹ï¼Œæ¦‚ç‡ä¸º0.18

* **ä¸Šå›¾æ˜¯ä¸¤ä¸ªéšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¾—åˆ°çš„ç»“æœä¹‹å’Œä¸º1**

* åŒæ—¶ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ª**è¾¹ç¼˜åˆ†å¸ƒï¼ˆMarginal Distributionï¼‰**çš„ä¾‹å­

#### 1.2.3 Conditional Probability æ¡ä»¶æ¦‚ç‡

* P(Intelligence|Grade=A) describes the distribution over events describable by Intelligence given the knowledge that studentâ€™s grade 
  is A. P(æ™ºåŠ›|å¹´çº§=A)æè¿°äº†åœ¨å·²çŸ¥å­¦ç”Ÿæˆç»©ä¸ºAçš„æƒ…å†µä¸‹ï¼Œæ™ºåŠ›æ‰€æè¿°çš„äº‹ä»¶çš„åˆ†å¸ƒæƒ…å†µ

![image-20221127180217738](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127180217738.png)



### 1.3 Independence and Conditional Independence ç‹¬ç«‹å’Œæ¡ä»¶ç‹¬ç«‹



#### 1.3.1 Recap: Chain Rules

**Chain rule** (also called **the general product rule**) permits the calculation of 
any member of the joint distribution of a set of random variables using 
only conditional probabilities.  

**é“¾å¼æ³•åˆ™(ä¹Ÿç§°ä¸ºä¸€èˆ¬ç§¯æ³•åˆ™)å…è®¸è®¡ç®—ä¸€ç»„éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒçš„ä»»ä½•æˆå‘˜ï¼Œåªä½¿ç”¨æ¡ä»¶æ¦‚ç‡ã€‚**

![image-20221127180623996](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127180623996.png)

ä½¿ç”¨æ¡ä»¶æ¦‚ç‡è®¡ç®—ä¸€ç»„éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒçš„ä»»ä½•æˆå‘˜ã€‚

#### 1.3.2 Independent Random Variables

* We expect P(Î±|Î²) to be different from P(Î±)  æˆ‘ä»¬æœŸæœ›P(Î±|Î²)ä¸P(Î±)ä¸åŒ

  * i.e., Î² is true changes our probability over Î±    Î² æ”¹å˜äº†Î±çš„æ¦‚ç‡

  

* Sometimes equality can occur, i.e, P(Î±|Î²)=P(Î±)  æœ‰æ—¶å¹³ç­‰ä¹Ÿä¼šå‘ç”Ÿ,ä¾‹å¦‚, P(Î±|Î²)=P(Î±)

  * i.e., learning that Î² occurs did not change our probability of Î± å¾—åˆ°Î²å‘ç”Ÿå¹¶æ²¡æœ‰æ”¹å˜Î±çš„æ¦‚ç‡
  * We say event Î± is independent of event Î², denoted  æˆ‘ä»¬è¯´äº‹ä»¶Î±ç‹¬ç«‹äºäº‹ä»¶Î²ï¼Œè®°ä¸º
  * ![image-20221127181309864](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127181309864.png)
  * A distribution P satisfies (Î±âŠ¥Î²) if and only if P(Î±âˆ§Î²)=P(Î±)P(Î²) 
  * åˆ†å¸ƒPæ»¡è¶³(Î±âˆ§Î²)å½“ä¸”ä»…å½“P(Î±âˆ§Î²)=P(Î±)P(Î²)

  

#### 1.3.3 Conditional Independence

* While independence is a useful property, we donâ€™t often encounter two independent events  è™½ç„¶ç‹¬ç«‹æ€§æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å±æ€§ï¼Œä½†æˆ‘ä»¬å¹¶ä¸ç»å¸¸é‡åˆ°ä¸¤ä¸ªç‹¬ç«‹äº‹ä»¶
* æ›´å¸¸è§çš„æƒ…å†µæ˜¯,å½“ä¸¤ä¸ªäº‹ä»¶ç‹¬ç«‹äºå¦ä¸€ä¸ªäº‹ä»¶æ—¶
  * Reason about student accepted at Stanford or MIT 
  * ![image-20221127181548072](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221127181548072.png)

![image-20221220133615115](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220133615115.png)

### 1.4 Querying Joint Probability Distributions æŸ¥è¯¢è”åˆæ¦‚ç‡åˆ†å¸ƒ

#### 1.4.1 Query Type æŸ¥è¯¢ç±»å‹

* Probability Queries æ¦‚ç‡æŸ¥è¯¢
  * Given evidence (the values of a subset of random variables),
  * ç»™å®šè¯æ®(éšæœºå˜é‡å­é›†çš„å€¼)ï¼Œ
  * compute distribution of another subset of random variables
  * è®¡ç®—éšæœºå˜é‡çš„å¦ä¸€ä¸ªå­é›†çš„åˆ†å¸ƒ
* MAP Queries
  * Maximum a posteriori probability 
  * â€¢æœ€å¤§åéªŒæ¦‚ç‡
  * Also called MPE (Most Probable Explanation) 
  * â€¢ä¹Ÿç§°ä¸ºMPE(æœ€å¯èƒ½è§£é‡Š)
  * What is the most likely setting of a subset of random variables
  * â€¢éšæœºå˜é‡å­é›†çš„æœ€å¯èƒ½è®¾ç½®æ˜¯ä»€ä¹ˆ
  * Marginal MAP Queries
  * â€¢è¾¹ç¼˜MAPæŸ¥è¯¢
  * When some variables are known
  * â€¢å½“ä¸€äº›å˜é‡æ˜¯å·²çŸ¥çš„



#### 1.4.2 Probability Queries æ¦‚ç‡æŸ¥è¯¢

* Most common type of query is a probability query 
* Query has two parts 
  * Evidence: a subset E of variables and their instantiation e 
  * Query Variables: a subset Y of random variables
* Inference Task: P(Y|E=e) 
  * Posterior probability distribution over values y of Y 
  * Conditioned on the fact E=e 
  * Can be viewed as Marginal over Y in distribution we obtain by conditioning on e 

æœ€å¸¸è§çš„æŸ¥è¯¢ç±»å‹æ˜¯æ¦‚ç‡æŸ¥è¯¢

â€¢æŸ¥è¯¢åˆ†ä¸ºä¸¤éƒ¨åˆ†

â€¢è¯æ®:å˜é‡çš„å­é›†EåŠå…¶å®ä¾‹åŒ–E

â€¢æŸ¥è¯¢å˜é‡:éšæœºå˜é‡çš„ä¸€ä¸ªå­é›†Y

â€¢æ¨ç†ä»»åŠ¡:P(Y|E= E)

â€¢yå€¼çš„åéªŒæ¦‚ç‡åˆ†å¸ƒ

â€¢ä»¥E= Eä¸ºæ¡ä»¶

â€¢å¯ä»¥çœ‹ä½œæ˜¯æˆ‘ä»¬é€šè¿‡æ¡ä»¶eå¾—åˆ°çš„åˆ†å¸ƒä¸­çš„è¾¹é™…é™¤ä»¥Y

![image-20221220134412179](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220134412179.png)



#### 1.4.3 Recapï¼š Max and Argmax

![image-20221220134602171](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220134602171.png)

maxæ˜¯æœ€å¤§å€¼

argmaxæ˜¯maxå¯¹åº”çš„ç´¢å¼•å€¼

#### 1.4.4 MAP Queries ï¼ˆMost Probable Explanationï¼‰

https://www.jianshu.com/p/9c153d82ba2d  å…³äºMAP

ä¸ºäº†è§£é‡Šè¿™ä¸ªï¼Œæˆ‘å…ˆè¡¥å……ä¸€äº›å…¶ä»–çŸ¥è¯†ï¼Œæ¯”å¦‚è´å¶æ–¯å…¬å¼

![image-20221220135824256](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220135824256.png)

![image-20221220142339940](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220142339940.png)

ç„¶åçœ‹ä¸€ä¸‹å…³äºMAPçš„æ›´è¯¦å°½çš„è§£é‡Š

![image-20221220142456347](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220142456347.png)

<font color='red'>ä¸è¿‡æˆ‘è®¤ä¸ºè¿™ä¸ªä¹Ÿå°±æ˜¯ä¸ªå®šä¹‰ï¼Œç›¸å¯¹æ¥è¯´å®é™…ä½¿ç”¨æ›´é‡è¦ä¸€ç‚¹</font>

* Finding a high probability assignment to some subset of variables 

* å¯»æ‰¾å˜é‡å­é›†çš„é«˜æ¦‚ç‡èµ‹å€¼

* Most likely assignment to all non-evidence variables W = V â€“ E 

* æœ€æœ‰å¯èƒ½èµ‹å€¼ç»™æ‰€æœ‰éè¯æ®å˜é‡W = V - E

* <font color='red'>è®°ä½å…¬å¼</font>

  ![image-20221220135007463](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220135007463.png)

* i.e., value of w for which P(w,e) is maximum 

* Difference from probability query 

* ä¸æ¦‚ç‡æŸ¥è¯¢çš„åŒºåˆ«

  * Instead of a probability we get the most likely value for all remaining variables 
  * æˆ‘ä»¬å¾—åˆ°çš„ä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯æ‰€æœ‰å‰©ä½™å˜é‡çš„æœ€å¯èƒ½å€¼

![image-20221220142759809](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220142759809.png)

![image-20221220143109428](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220143109428.png)

![image-20221220143119166](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220143119166.png)

è®°å¾—ï¼Œè¿™éƒ¨åˆ†ç”¨Chain Rules

![image-20221220143159479](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220143159479.png)

![image-20221220143250181](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220143250181.png)

Q3å¾ˆéº»çƒ¦ï¼Œæ³¨æ„ç‚¹

![image-20221220143624348](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220143624348.png)

![image-20221220143707101](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220143707101.png)

![image-20221220144042625](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220144042625.png)

Exercise1

![image-20221220144852142](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220144852142.png)

Exercise2

![image-20221220144905237](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220144905237.png)



## Lecture5 Safety Problems and Linear-Algebra

### 1. Safety and Reliability Issues

#### 1.1 Safety Concerns

* Despite the success of machine learning in many areas, serious 
  concerns have been raised in applying machine learning to real-world 
  safety-critical systems such as self-driving cars, automatic medical 
  diagnosis, etc.
* å°½ç®¡æœºå™¨å­¦ä¹ åœ¨è®¸å¤šé¢†åŸŸéƒ½å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨å°†æœºå™¨å­¦ä¹ åº”ç”¨äºç°å®ä¸–ç•Œçš„å®‰å…¨å…³é”®ç³»ç»Ÿ(å¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€è‡ªåŠ¨åŒ»ç–—è¯Šæ–­ç­‰)æ—¶ï¼Œäººä»¬æå‡ºäº†ä¸¥é‡çš„æ‹…å¿§ã€‚
* Simply speaking, a learned model    is to approximate a target 
  function   . Therefore, the erroneous behaviour of exists when it is 
  inconsistent with . 
* ç®€å•åœ°è¯´ï¼Œå­¦ä¹ æ¨¡å‹å°±æ˜¯è¿‘ä¼¼ä¸€ä¸ªç›®æ ‡å‡½æ•°ã€‚å› æ­¤ï¼Œé”™è¯¯çš„è¡Œä¸ºå­˜åœ¨äºä¸ä¹‹ä¸ä¸€è‡´çš„æ—¶å€™ã€‚

![image-20221220150310957](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220150310957.png)

##### 1.1.1 Generalisation Error ä¸€èˆ¬ä¼šå‡ºç°çš„é”™è¯¯

* Empirical loss:ç»éªŒæŸå¤±

![image-20221220150408941](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220150408941.png)

* Expected loss:é¢„æœŸæŸå¤±

  ![image-20221220150520410](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220150520410.png)

* Generalisation error: ä¸€èˆ¬åŒ–é”™è¯¯ æ³›åŒ–è¯¯å·®

  ![image-20221220150553597](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220150553597.png)

* Generalisation error is closely related to the overfitting problem of 
  machine learning algorithms.
* æ³›åŒ–è¯¯å·®ä¸æœºå™¨å­¦ä¹ ç®—æ³•çš„è¿‡æ‹Ÿåˆé—®é¢˜å¯†åˆ‡ç›¸å…³ã€‚
* A machine learning model is overfitted if it performs well on training data samples but badly on test data samples
* å¦‚æœä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æ ·æœ¬ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®æ ·æœ¬ä¸Šè¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯è¿‡æ‹Ÿåˆçš„

##### 1.1.2 Adversarial Examples å¯¹æŠ—æ ·æœ¬

* Adversarial examples represent another class of erroneous behaviours that also introduce safety implications. 
* å¯¹æŠ—æ€§çš„ä¾‹å­ä»£è¡¨äº†å¦ä¸€ç±»é”™è¯¯è¡Œä¸ºï¼Œä¹Ÿä¼šå¸¦æ¥å®‰å…¨éšæ‚£ã€‚
* It represents a mis-match of the decisions made by a human and by a neural network, and does not necessarily involve an adversary.
* å®ƒä»£è¡¨äº†äººç±»å’Œç¥ç»ç½‘ç»œåšå‡ºçš„å†³ç­–çš„ä¸åŒ¹é…ï¼Œå¹¶ä¸ä¸€å®šæ¶‰åŠå¯¹ç«‹ã€‚

![image-20221220153331659](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220153331659.png)

![image-20221220153401032](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220153401032.png)

###### 1.1.2.1 Measurement Of Adversarial Examples

![image-20221220153533301](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220153533301.png)

![image-20221220153618690](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220153618690.png)

<font color='red'>è¿™é‡Œæ¶‰åŠåˆ°åé¢çš„å¯¹æŠ—æ”»å‡»ï¼Œæ˜¯ä¸ªå°çš„å¼•å­</font>

##### 1.1.3 Data Poisoning

* Poisoning attack occurs when the adversary injects malicious data 
  into training process, and hence get a machine learning algorithm to 
  learn something it should not. There are two types of poisoning 
  attacks, one for data poisoning attacks and the other for backdoor 
  attacks. 
* å½“å¯¹æ‰‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨å…¥æ¶æ„æ•°æ®ï¼Œä»è€Œä½¿æœºå™¨å­¦ä¹ ç®—æ³•å­¦ä¹ åˆ°å®ƒä¸åº”è¯¥å­¦ä¹ çš„ä¸œè¥¿æ—¶ï¼Œå°±ä¼šå‘ç”ŸæŠ•æ¯’æ”»å‡»ã€‚æŠ•æ¯’æ”»å‡»æœ‰ä¸¤ç§ç±»å‹ï¼Œä¸€ç§æ˜¯æ•°æ®æŠ•æ¯’æ”»å‡»ï¼Œå¦ä¸€ç§æ˜¯åé—¨æ”»å‡»ã€‚

##### 1.1.4 Backdoor Attack

åé—¨è¿™ä¸ªè¯åœ¨ä¼ ç»Ÿè½¯ä»¶å®‰å…¨ä¸­å‡ºç°çš„æ¯”è¾ƒå¤šï¼Œå…¶ä¸º**ç»•è¿‡è½¯ä»¶çš„å®‰å…¨æ€§æ§åˆ¶ï¼Œä»æ¯”è¾ƒéšç§˜çš„é€šé“è·å–å¯¹ç¨‹åºæˆ–ç³»ç»Ÿè®¿é—®æƒçš„é»‘å®¢æ–¹æ³•**ã€‚åœ¨è½¯ä»¶å¼€å‘æ—¶ï¼Œè®¾ç½®åé—¨å¯ä»¥æ–¹ä¾¿ä¿®æ”¹å’Œæµ‹è¯•ç¨‹åºä¸­çš„ç¼ºé™·ï¼Œä½†å¦‚æœåé—¨è¢«å…¶ä»–äººçŸ¥é“ï¼ˆå¯ä»¥æ˜¯æ³„å¯†æˆ–è€…è¢«æ¢æµ‹åˆ°åé—¨ï¼‰ï¼Œæˆ–æ˜¯åœ¨å‘å¸ƒè½¯ä»¶ä¹‹å‰æ²¡æœ‰å»é™¤åé—¨ï¼Œé‚£ä¹ˆå®ƒå°±å¯¹è½¯ä»¶å®‰å…¨é€ æˆäº†å¨èƒã€‚

![image-20221220154106117](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220154106117.png)

![image-20221220154124826](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220154124826.png)

### 2. Linear Algebra For Machine Learning  çº¿æ€§ä»£æ•°

#### 2.1 Scalar æ ‡é‡

* Single number ä¸€ä¸ªç‹¬ç«‹çš„æ•°å­—
* Represented in lower-case italic *x*  ç”¨å°å†™æ–œä½“xè¡¨ç¤º

![image-20221220160115314](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220160115314.png)

#### 2.2 Vector å‘é‡

* An array of numbers 
* ä¸€ç»„æ•°å­—
* Arranged in order 
* æŒ‰æ¬¡åºæ’åˆ—
* Each no. identified by an index 
* æ¯ä¸€ä¸ªéƒ½ç”±ç´¢å¼•æ ‡è¯†
* Vectors are shown in lower-case bold 
* å‘é‡ç”¨å°å†™ç²—ä½“è¡¨ç¤º
* If each element is in R then x is in Rn
* We think of vectors as points in space 
* æˆ‘ä»¬è®¤ä¸ºå‘é‡æ˜¯ç©ºé—´ä¸­çš„ç‚¹
  * Each element gives coordinate along an axis 
  * æ¯ä¸ªå…ƒç´ ç»™å‡ºæ²¿è½´çš„åæ ‡

![image-20221220160452039](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220160452039.png)

#### 2.3 Matrix çŸ©é˜µ

* 2-D array of numbers äºŒç»´æ•°ç»„
* Each element identified by two indices  æ¯ä¸ªå…ƒç´ ç”±ä¸¤ä¸ªç´¢å¼•æ ‡è¯†
* Denoted by bold typeface A   ç”¨ç²—ä½“Aè¡¨ç¤º
* Elements indicated as Am,n
  â€¢E.g.
* ![image-20221220160710214](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220160710214.png)
* A[i:] is ith row of A, A[:j] is jth column of A 
* If A has shape of height m and width n with real-values then![image-20221220160902896](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220160902896.png)
* å¦‚æœAæœ‰ä¸€ä¸ªé«˜må®½nçš„å®å€¼å½¢çŠ¶ï¼Œé‚£ä¹ˆ![image-20221220160905408](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220160905408.png)

![image-20221220160912642](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220160912642.png)

#### 2.4 Tensor å¼ é‡

éå¸¸åƒä¸€ç§å¤šç»´æ•°ç»„ï¼Œä½†æ˜¯åœ¨Pythoné‡Œï¼ŒTensorç±»æä¾›GPUè®¡ç®—å’Œè‡ªåŠ¨æ±‚æ¢¯åº¦ç­‰æ›´å¤šåŠŸèƒ½ã€‚

![image-20221220162737026](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220162737026.png)

![image-20221220162816493](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220162816493.png)

#### 2.5 Transpose of Matrix çŸ©é˜µçš„è½¬è‡³

* Mirror image across principal diagonal  å¯¹è§’çº¿ä¸Šçš„è¿›è¡Œé•œåƒ

![image-20221220163039076](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163039076.png)

* Vectors are matrices with a single column  å‘é‡æ˜¯åªæœ‰ä¸€åˆ—çš„çŸ©é˜µ
  * Often written in-line using transpose  é€šå¸¸ç”¨è½¬ç½®å†™åœ¨è¡Œå†…
  * ![image-20221220163130935](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163130935.png)

* Since a scalar is a matrix with one element  ![image-20221220163209494](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163209494.png)
* å› ä¸ºæ ‡é‡æ˜¯ä¸€ä¸ªåªæœ‰ä¸€ä¸ªå…ƒç´ çš„çŸ©é˜µ![image-20221220163212086](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163212086.png)

![image-20221220163256660](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163256660.png)

#### 2.6 Linear Transformation çº¿æ€§è½¬æ¢

![image-20221220163353856](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163353856.png)

![image-20221220163439798](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163439798.png)

#### 2.7 Identity and Inverse Matrices æ’ç­‰ä¸é€†çŸ©é˜µ

* Matrix inversion is a powerful tool to analytically solve Ax=b  
* é€†çŸ©é˜µæ˜¯è§£å†³Ax=bçš„æœ‰æ•ˆå·¥å…·
* Needs concept of Identity matrix 
* éœ€è¦å•ä½çŸ©é˜µçš„æ¦‚å¿µ
* Identity matrix does not change value of vector 
* å•ä½çŸ©é˜µä¸æ”¹å˜å‘é‡çš„å€¼
* when we multiply the vector by identity matrix 
* å½“æˆ‘ä»¬ç”¨å•ä½çŸ©é˜µä¹˜ä»¥è¿™ä¸ªå‘é‡

![image-20221220163925226](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220163925226.png)

##### 2.7.1 Matrix Inverse é€†è½¬çŸ©é˜µ

![image-20221220164037705](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221220164037705.png)

#### 2.8 Norms æ¨¡

* Used for measuring the size of a vector 
* ä½¿ç”¨æµ‹é‡å‘é‡çš„å¤§å°
* Norms map vectors to non-negative values 
* æ¨¡å°†å‘é‡æ˜ å°„åˆ°éè´Ÿå€¼
* Norm of vector x is distance from origin to x 
* å‘é‡xçš„æ¨¡æ˜¯ä»åŸç‚¹åˆ°xçš„è·ç¦»
  * It is any function f that satisfies:

![image-20221221122203515](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122203515.png)



#### 2.9 LPNormï¼ˆPæ˜¯ä¸Šæ ‡ï¼‰

![image-20221221122325166](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122325166.png)

![image-20221221122602752](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122602752.png)

![image-20221221122615592](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122615592.png)

![image-20221221122652789](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122652789.png)

![image-20221221122726162](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122726162.png)



#### 2.10 Size of Matrix çŸ©é˜µçš„å¤§å°

![image-20221221122912030](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122912030.png)

![image-20221221122920866](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221122920866.png)



### 3. Introduction to Scientific Python 

**<font color=red>ä¸»è¦æ˜¯å…³äºPythonçš„ç›¸å…³æ–¹æ³•,ç›´æ¥çœ‹PPTå§ï¼Œä¸»è¦å†…å®¹ä»Lecture5åˆ°Lecture6çš„å‰åŠæ®µ</font>**



#### 3.1 Numpy

* Fundamental package for scientific computing with Python 
* ä½¿ç”¨Pythonè¿›è¡Œç§‘å­¦è®¡ç®—çš„åŸºæœ¬åŒ…
* N-dimensional array object 
* ä¸€ä¸ªNç»´æ•°ç»„çš„å¯¹è±¡
* Linear algebra, Fourier transform, random number capabilities 
* çº¿æ€§ä»£æ•°ï¼Œå‚…é‡Œå¶å˜æ¢ï¼Œéšæœºæ•°èƒ½åŠ›
* Building block for other packages (e.g. Scipy) 
* å…¶ä»–åŒ…çš„æ„å»ºå—(ä¾‹å¦‚Scipy)
* Open source
* å¼€æ”¾æºç 



## Lecture6&7&8&9&10 Decision-Tree-Learning

æˆ‘å…ˆåœ¨è¿™é‡Œè§£é‡Šä¸€ä¸‹æˆ‘è‡ªå­¦çš„å†³ç­–æ ‘ï¼Œæˆ‘è§‰å¾—è€å¸ˆè®²çš„å¥½ä¹±å•Š

https://blog.csdn.net/jiaoyangwm/article/details/79525237?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167162688516782388072538%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167162688516782388072538&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79525237-null-null.142

å‚ç…§ä¸Šè¿°è¿æ¥



### 1. The Decision Tree Representation å†³ç­–æ ‘çš„è¡¨ç¤ºæ–¹æ³•

![image-20230121151214582](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121151214582.png)

<font color=red size=5>é€šè¿‡ä¸æ–­çš„å†³ç­–å°†äº‹ç‰©è¿›è¡Œåˆ†ç±»ï¼Œæœ€ç»ˆå¾—åˆ°æˆ‘ä»¬éœ€è¦çš„ç»“æœ</font>

![image-20230121151403135](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121151403135.png)

### 2. The standard top-down approach to learning a tree  å­¦ä¹ æ ‘çš„æ ‡å‡†è‡ªé¡¶å‘ä¸‹æ–¹æ³•

![image-20221221164228436](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221164228436.png)

![image-20221221164250369](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221164250369.png)

![image-20221221164357921](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221164357921.png)



![image-20221221164616038](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221164616038.png)

![image-20221221164635236](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221164635236.png)



#### 3.1 Step(1): FindBestSplitå¯»æ‰¾åˆ†æ­§

å› ä¸ºå†³ç­–æ ‘æ˜¯ä¸€ä¸ªåˆ†ç±»çš„æ–¹æ³•ï¼Œæ‰€ä»¥ä¸€å¼€å§‹æˆ‘ä»¬å¯»æ‰¾äº‹åŠ¡ä¸­çš„åˆ†æ­§Split



##### 3.1.1 Candidate splits on numeric features æ•°å­—ç‰¹å¾ä¸Šå€™é€‰ç±»çš„åˆ†æ­§

* **Given a set of training instances D and a specific feature Xi**
* **ç»™ä¸€ä¸ªè®­ç»ƒå®ä¾‹Då’Œä¸€ä¸ªç‰¹å®šçš„ç‰¹å¾Xi**
  * Sort the values of Xi in D 
  * å¯¹åœ¨Dä¸­çš„Xiçš„å€¼è¿›è¡Œæ’åº
  * Evaluate split thresholds in intervals between instances of different classes
  * è¯„ä¼°åˆ†å‰²çš„é˜ˆå€¼åœ¨ä¸åŒç±»å®ä¾‹ä¹‹é—´çš„é—´éš”

![image-20221221170211161](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221170211161.png)

* could use midpoint of each considered interval as the threshold 
* æˆ‘ä»¬ä¼šä½¿ç”¨é—´éš”çš„ä¸­ç‚¹ä½œä¸ºé˜ˆå€¼threshold
* C4.5 instead picks the largest value of Xiin the entire training set that does not exceed the midpoint 
* C4.5åˆ™é€‰æ‹©æ•´ä¸ªè®­ç»ƒé›†ä¸­xiçš„æœ€å¤§å€¼ï¼Œä¸”ä¸è¶…è¿‡ä¸­ç‚¹



![image-20221221171004628](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221171004628.png)

![image-20221221171012513](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221171012513.png)

![image-20221221171152148](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221171152148.png)



#### 3.2 Step(2): DetermineCandidateSplit å†³å®šå€™é€‰åˆ†æ­§

##### 3.2.1 Find the best split

* How should we select the best feature to split on at each step? 
* æˆ‘ä»¬åº”è¯¥å¦‚ä½•åœ¨æ¯ä¸€æ­¥ä¸­é€‰æ‹©æœ€å¥½çš„ç‰¹å¾è¿›è¡Œåˆ†å‰²?
* **Key hypothesis: the simplest tree that classifies the training instances accurately will work well on previously unseen instances** 
* **å…³é”®å‡è®¾:å¯¹è®­ç»ƒå®ä¾‹è¿›è¡Œå‡†ç¡®åˆ†ç±»çš„æœ€ç®€å•çš„æ ‘åœ¨ä»¥å‰æœªè§è¿‡çš„å®ä¾‹ä¸Šä¹Ÿèƒ½å¾ˆå¥½åœ°å·¥ä½œ**

##### 3.2.2 Occamâ€˜s Razor å¥¥å¡å§†å‰ƒåˆ€

<font color='red'>å¦‚æ— å¿…è¦ï¼Œå‹¿å¢å®ä½“</font>

![image-20221221172242569](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221172242569.png)

â€¢çŸ­æ¨¡å‹(å³å°æ ‘)æ¯”é•¿æ¨¡å‹å°‘

**â€¢çŸ­æ¨¡å‹ä¸å¤ªå¯èƒ½å¶ç„¶åœ°å¾ˆå¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®**

**â€¢é•¿æ¨¡å‹æ›´æœ‰å¯èƒ½å·§åˆåœ°å¾ˆå¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®**

##### 3.2.3 Finding the best splits

 å› ä¸ºå¯ä»¥å†³å®šè®­ç»ƒæ•°æ®çš„æœ€å°å†³ç­–æ ‘çš„å¯»æ‰¾æ˜¯ä¸€ä¸ªNPé—®é¢˜ï¼Œ**æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ä¿¡æ¯ç†è®ºçš„å¯å‘å¼æ¥è´ªå©ªåœ°é€‰æ‹©åˆ†å‰²**

* Instead, weâ€™ll use an information-theoretic heuristics to greedily choose splits 

![image-20221221172341197](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221172341197.png)

#### 3.3 Recap: Expected Value(Finite Case) æœŸæœ›å€¼

* æ•°å­¦æœŸæœ›

![image-20221221172526992](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221172526992.png)

![image-20221221172533374](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221172533374.png)



### 3.Information theory background 

![image-20230121153705605](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121153705605.png)

![image-20230121153721207](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121153721207.png)

![image-20230121153728566](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121153728566.png)

![image-20230121153742506](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121153742506.png)

![image-20230121153941598](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121153941598.png)

* ç›´è§‚ä¸Šï¼Œå¦‚æœä¸€ä¸ªç‰¹å¾å…·æœ‰æ›´å¥½çš„åˆ†ç±»èƒ½åŠ›ï¼Œæˆ–è€…è¯´ï¼ŒæŒ‰ç…§è¿™ä¸€ç‰¹å¾å°†è®­ç»ƒæ•°æ®é›†åˆ†å‰²æˆå­é›†ï¼Œä½¿å¾—å„ä¸ªå­é›†åœ¨å½“å‰æ¡ä»¶ä¸‹æœ‰æœ€å¥½çš„åˆ†ç±»ï¼Œé‚£ä¹ˆå°±æ›´åº”è¯¥é€‰æ‹©è¿™ä¸ªç‰¹å¾ã€‚
* **ä¿¡æ¯å¢ç›Šå°±èƒ½å¤Ÿå¾ˆå¥½åœ°è¡¨ç¤ºè¿™ä¸€ç›´è§‚çš„å‡†åˆ™ã€‚**
* ä»€ä¹ˆæ˜¯ä¿¡æ¯å¢ç›Šå‘¢ï¼Ÿ**<font color=red>åœ¨åˆ’åˆ†æ•°æ®é›†ä¹‹å‰ä¹‹åä¿¡æ¯å‘ç”Ÿçš„å˜åŒ–æˆä¸ºä¿¡æ¯å¢ç›Š</font>**ï¼ŒçŸ¥é“å¦‚ä½•è®¡ç®—ä¿¡æ¯å¢ç›Šï¼Œæˆ‘ä»¬å°±å¯è®¡ç®—æ¯ä¸ªç‰¹å¾å€¼åˆ’åˆ†æ•°æ®é›†è·å¾—çš„ä¿¡æ¯å¢ç›Šï¼Œè·å¾—ä¿¡æ¯å¢ç›Šæœ€é«˜çš„ç‰¹å¾å°±æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚
* **ç†µå®šä¹‰ä¸ºä¿¡æ¯çš„æœŸæœ›å€¼**

![image-20230121154338689](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121154338689.png)







### 4. Entropy and information gain  ç†µå’Œä¿¡æ¯å¢ç›Š

**<font color='red'>è¿™ä¸ªåŒæ ·åœ¨ä¸Šè¿°é“¾æ¥æœ‰è§£é‡Š</font>**

#### 4.1 Entropy ç†µ

* **Entropy is a measure of uncertainty associated with a random** 
  **variable**
*  **ç†µæ˜¯ä¸éšæœºå˜é‡ç›¸å…³çš„ä¸ç¡®å®šæ€§çš„åº¦é‡**
*  **åŒæ ·ä¹Ÿæ˜¯ä¿¡æ¯å¢ç›Šçš„åº¦é‡**
* **<font color=red>Defined as the expected number of bits required to communicate the value of the variable</font>** 
* **å®šä¹‰ä¸ºä¼ é€’å˜é‡å€¼æ‰€éœ€çš„é¢„æœŸæ¯”ç‰¹æ•°**

![image-20221221195853302](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221195853302.png)

##### 4.1.2 Conditional Entropy æ¡ä»¶ç†µ

* **Conditional entropy(or equivocation) quantifies the amount of information needed to describe the outcome of a random variable given that the value of another random variable is known.**
* **æ¡ä»¶ç†µ(æˆ–æ¨¡æ£±ä¸¤å¯)åœ¨å·²çŸ¥å¦ä¸€ä¸ªéšæœºå˜é‡å€¼çš„æƒ…å†µä¸‹ï¼Œé‡åŒ–æè¿°ä¸€ä¸ªéšæœºå˜é‡ç»“æœæ‰€éœ€çš„ä¿¡æ¯é‡ã€‚**
* ![image-20221221200135389](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221200135389.png)

![image-20221221201952978](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221201952978.png)

![image-20221221202002119](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202002119.png)

![image-20221221202018997](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202018997.png)

#### 4.2 Information Gain ä¿¡æ¯ç†µ

* **ä¿¡æ¯å¢ç›Š**ï¼šä¿¡æ¯å¢ç›Šæ˜¯ç›¸å¯¹äºç‰¹å¾è€Œè¨€çš„ã€‚æ‰€ä»¥ï¼Œç‰¹å¾Aå¯¹è®­ç»ƒæ•°æ®é›†Dçš„ä¿¡æ¯å¢ç›Šg(D,A)ï¼Œå®šä¹‰ä¸ºé›†åˆDçš„ç»éªŒç†µH(D)ä¸ç‰¹å¾Aç»™å®šæ¡ä»¶ä¸‹Dçš„ç»éªŒæ¡ä»¶ç†µH(D|A)ä¹‹å·®ï¼Œå³ï¼š
* ![image-20230121180646423](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121180646423.png)
* **å› ä¸ºæˆ‘ä»¬éœ€è¦æ ¹æ®ç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼Œåˆ†ç±»å¾€å¾€è¦é€‰å–ä¸åŒçš„ç‰¹å¾ï¼Œè¯¥æ€ä¹ˆè¯„ä»·ä¸€ä¸ªç‰¹å¾é€‰çš„å¥½ä¸å¥½å‘¢ï¼Œå°±ç”¨ä¿¡æ¯å¢ç›Šã€‚åœ¨ä¸åŒçš„ç‰¹å¾ä¸‹ï¼Œå–ä¿¡æ¯å¢ç›Šçš„æœ€å¤§å€¼ä¸ºæœ€ä½³ç‰¹å¾**

![image-20230121180626616](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121180626616.png)

* Choosing splits in ID3: select the split S that most reduces the conditional entropy of Y for training set D
* ID3ä¸­é€‰æ‹©åˆ†å‰²:ä¸ºè®­ç»ƒé›†Dé€‰æ‹©ä½¿Yçš„æ¡ä»¶ç†µå‡å°‘æœ€å¤šçš„åˆ†å‰²S
* **å°†æ— åºæ•°æ®å˜å¾—æ›´åŠ æœ‰åºï¼Œä½†æ˜¯å„ç§æ–¹æ³•éƒ½æœ‰å„è‡ªçš„ä¼˜ç¼ºç‚¹ï¼Œä¿¡æ¯è®ºæ˜¯é‡åŒ–å¤„ç†ä¿¡æ¯çš„åˆ†æ”¯ç§‘å­¦ï¼Œåœ¨åˆ’åˆ†æ•°æ®é›†å‰åä¿¡æ¯å‘ç”Ÿçš„å˜åŒ–ç§°ä¸ºä¿¡æ¯å¢ç›Šï¼Œè·å¾—ä¿¡æ¯å¢ç›Šæœ€é«˜çš„ç‰¹å¾å°±æ˜¯æœ€å¥½çš„é€‰æ‹©**

![image-20221221202249262](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202249262.png)

![image-20221221202536762](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202536762.png)

![image-20221221202849584](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202849584.png)

![image-20221221202856763](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202856763.png)

![image-20221221202911110](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202911110.png)

![image-20221221202925632](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202925632.png)

![image-20221221202933934](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221202933934.png)

**<font color=red size=5>è¿™é‡Œå°±æ˜¯è®¡ç®—ä¸¤ä¸ªä¸åŒç‰¹å¾çš„ information gainæ¥åˆ¤æ–­é€‰æ‹©å“ªä¸€ä¸ªç‰¹å¾ä½œä¸ºç‰¹å¾å€¼,ç»“æœæ˜¾ç¤ºHumidityæ›´é€‚åˆåšç‰¹å¾å€¼</font>**

![image-20221221203002165](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221203002165.png)

##### 5.1 One Limitation Of Information Gain ä¿¡æ¯ç†µçš„ä¸€ç§é™åˆ¶

* **information gain is biased towards tests with many outcomes**
* **ä¿¡æ¯è·å–åå‘äºæœ‰å¤šç§ç»“æœçš„æµ‹è¯•** 
* e.g. consider a feature that uniquely identifies each training instance 
* è€ƒè™‘ä¸€ä¸ªå”¯ä¸€æ ‡è¯†æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„ç‰¹å¾
  * splitting on this feature would result in many branches, each of which is  â€œpureâ€ (has instances of only one class) 
  * åœ¨è¿™ä¸ªç‰¹æ€§ä¸Šçš„åˆ†è£‚å°†å¯¼è‡´è®¸å¤šåˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯éƒ½æ˜¯â€œçº¯â€çš„(åªæœ‰ä¸€ä¸ªç±»çš„å®ä¾‹)
  * maximal information gain! 
  * æœ€å¤§çš„ä¿¡æ¯å¢ç›Š!

##### 5.2 Gain Ratio ä¿¡æ¯å¢ç›Šæ¯”

* **ä¿¡æ¯å¢ç›Šæœ‰æ—¶ä¼šå€¾å‘äºæœ‰æ›´å¤šå–å€¼èŒƒå›´çš„é‚£ä¸ªç‰¹å¾ï¼ˆå¦‚ï¼Œæ˜¯å¦æœ‰å·¥ä½œçš„å–å€¼èŒƒå›´ä¸ºâ€œæ˜¯â€orâ€œå¦â€ï¼›ä¿¡è´·æƒ…å†µçš„å–å€¼èŒƒå›´ä¸ºâ€œä¸€èˆ¬â€ï¼Œâ€œå¥½â€ï¼Œâ€œéå¸¸å¥½â€ï¼Œæ­¤æ—¶ 3 > 2ï¼Œæœ‰å¯èƒ½ä¼šé€ æˆ â€œä¿¡è´·æƒ…å†µâ€ çš„ä¿¡æ¯å¢ç›Šå¤§äº â€œæ˜¯å¦æœ‰å·¥ä½œâ€ è¿™ä¸ªç‰¹å¾ ï¼‰**

* To address this limitation, C4.5 uses a splitting criterion called gain 
  ratio 
* ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼ŒC4.5ä½¿ç”¨äº†ä¸€ç§ç§°ä¸º**å¢ç›Šæ¯”**çš„åˆ†è£‚å‡†åˆ™
* Gain ratio normalizes the information gain by the entropy of the split 
  being considered 
* å¢ç›Šæ¯”é€šè¿‡æ‰€è€ƒè™‘çš„åˆ†è£‚çš„ç†µå°†ä¿¡æ¯å¢ç›Šå½’ä¸€åŒ–

![image-20221221203729874](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221203729874.png)

![image-20221221203901264](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221203901264.png)

### 5. Types of decision-tree splits  å†³ç­–æ ‘åˆ†å‰²çš„ç±»å‹

#### 5.1 (3) Stopping criteria of decision trees åœæ­¢æ ‡å‡†ï¼ˆå°±æ˜¯æ»¡è¶³æ¡ä»¶åœæ­¢åˆ†ç±»ï¼‰

* We should form a leaf when 
* æˆ‘ä»¬åº”è¯¥å½¢æˆä¸€ä¸ªå¶å­ç»“ç‚¹
  * all of the given subset of instances are of the same class 
  * æ‰€æœ‰ç»™å®šå®ä¾‹çš„å­é›†éƒ½æ˜¯åŒä¸€ä¸ªç±»
  * weâ€™ve exhausted all of the candidate splits 
  * æˆ‘ä»¬è€—å°½äº†æ‰€æœ‰çš„å€™é€‰äººåˆ†æ”¯
* <font color=red size=5>ç®€å•æ¥è¯´ï¼Œå°±æ˜¯ç¡®å®šå·²ç»æ— æ³•åˆ†ç±»</font>

### 6. Accuracy of Decision Tree å†³ç­–æ ‘çš„å‡†ç¡®åº¦

#### 6.1 Definition of Accuracy and Error å‡†ç¡®åº¦å’Œè¯¯å·®çš„å®šä¹‰

* **Given a set D of samples and a trained model M, the accuracy is the percentage of correctly labeled samples. That is,** 

* ç»™å®šä¸€ç»„æ ·æœ¬Då’Œä¸€ä¸ªè®­ç»ƒè¿‡çš„æ¨¡å‹Mï¼Œ**å‡†ç¡®ç‡æ˜¯æ­£ç¡®æ ‡è®°æ ·æœ¬çš„ç™¾åˆ†æ¯”ã€‚**

* ![image-20221221210538385](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221210538385.png)

  

  ![image-20221221210721931](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221210721931.png)

* lxæ˜¯æ ·æœ¬xçš„çœŸå®æ ‡ç­¾ï¼ŒM(x)æ˜¯é¢„æµ‹xçš„æ ‡ç­¾

* **Error is a dual concept of accuracy.**

* ![image-20221221210937550](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221210937550.png)



#### 6.2 How can we assess the accuracy of a tree? å¦‚ä½•è¯„ä»·ä¸€ä¸ªæ ‘çš„å‡†ç¡®æ€§

* Can we just calculate the fraction of training instances that are correctly classified? 
* æˆ‘ä»¬èƒ½è®¡ç®—å‡ºæ­£ç¡®åˆ†ç±»çš„è®­ç»ƒå®ä¾‹çš„æ¯”ä¾‹å—?
* Consider a problem domain in which instances are assigned labels at random with P(Y = t) = 0.5 
* è€ƒè™‘ä¸€ä¸ªé—®é¢˜åŸŸï¼Œå…¶ä¸­å®ä¾‹è¢«éšæœºåˆ†é…ä¸ºP(Y = t) = 0.5çš„æ ‡ç­¾
  * how accurate would a learned decision tree be on previously unseen instances? 
  * å¯¹äºä»¥å‰æœªè§è¿‡çš„å®ä¾‹ï¼Œä¹ å¾—çš„å†³ç­–æ ‘æœ‰å¤šå‡†ç¡®?
    * Can never reach 1.0. 
  * how accurate would it be on its training set? 
  * å®ƒåœ¨è®­ç»ƒé›†ä¸Šæœ‰å¤šå‡†ç¡®?
    * Can be arbitrarily close to, or reach, 1.0 if model can be very large. 
    * å¦‚æœæ¨¡å‹å¯ä»¥å¾ˆå¤§ï¼Œå¯ä»¥ä»»æ„æ¥è¿‘æˆ–è¾¾åˆ°1.0ã€‚
  
* To get an unbiased estimate of a learned modelâ€™s accuracy, we must 
  use a set of instances that are held-aside during learning 
* ä¸ºäº†å¯¹å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§è¿›è¡Œæ— åå€šçš„ä¼°è®¡ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨ä¸€ç»„åœ¨å­¦ä¹ æœŸé—´æç½®çš„å®ä¾‹    <font color='red'>å…¶å®å°±æ˜¯æµ‹è¯•é›†</font>
* ![image-20221221211957832](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221211957832.png)



### 7. Pruning and Validation Dataset å‰ªæå’ŒéªŒè¯æ•°æ®é›†

#### 7.1 Stopping criteria åœæ­¢æ ‡å‡†

* We should form a leaf when 

  * all of the given subset of instances are of the same class 

  * weâ€™ve exhausted all of the candidate splits 
  
  * æˆ‘ä»¬å·²ç»ç”¨å°½äº†æ‰€æœ‰çš„å€™é€‰äººåˆ†è£‚

 So How to prune back the tree?

#### 7.2 Pruning in C4.5 åœ¨C4.5å‰ªæ

* **Split given data into training and validation(tuning) sets** 
* **å°†ç»™å®šçš„æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯(è°ƒä¼˜)é›†**
* a validation set (a.k.a. tuning set) is a subset of the training set that is held aside 
* éªŒè¯é›†(ä¹Ÿç§°ä¸ºè°ƒä¼˜é›†)æ˜¯è®­ç»ƒé›†çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒè¢«æ”¾åœ¨ä¸€è¾¹
  * not used for primary training process (e.g. tree growing) 
  * ä¸ç”¨äºåˆçº§çš„è®­ç»ƒè¿‡ç¨‹
  * but used to select among models (e.g. trees pruned to varying degrees) 
  * ä½†ç”¨äºåœ¨æ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©(ä¾‹å¦‚ï¼Œä¿®å‰ªä¸åŒç¨‹åº¦çš„æ ‘æœ¨)

![image-20221221212811169](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221212811169.png)

* Split given data into training and validation (tuning) sets 
* å°†ç»™å®šçš„æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯(è°ƒä¼˜)é›†
* Grow a complete tree
* é•¿æˆä¸€æ£µå®Œæ•´çš„æ ‘
* do until further pruning is harmful 
  * evaluate impact on tuning-set accuracy of pruning each node
  *  è¯„ä¼°ä¿®å‰ªæ¯ä¸ªèŠ‚ç‚¹å¯¹è°ƒä¼˜é›†ç²¾åº¦çš„å½±å“
  * greedily remove the one that least reduces tuning-set accuracy
  *  è´ªå©ªåœ°åˆ é™¤æœ€å°é™ä½è°ƒä¼˜é›†ç²¾åº¦çš„ä¸€ä¸ª

**<font color='red'>å°±æ˜¯æµ‹è¯•é›†çš„ä½œç”¨</font>**



### 8. Overfitting

- **è¿‡æ‹Ÿåˆ**æ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œåˆ°äº†éªŒè¯å’Œæµ‹è¯•é˜¶æ®µå°±å¾ˆå·®ï¼Œå³æ¨¡å‹çš„**æ³›åŒ–èƒ½åŠ›å¾ˆå·®**ã€‚
- **<font color=blue size=5>arg max function(x)è¡¨ç¤ºåœ¨functionå–æœ€å¤§å€¼æ—¶ï¼Œxå¯¹åº”çš„å€¼</font>**

![image-20221221213355110](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221213355110.png)

![image-20221221213447803](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221213447803.png)

![image-20221221213459038](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221213459038.png)

![image-20221221213509082](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221213509082.png)

![image-20221221213527471](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221213527471.png)

<font color=red size=5>äººä»¬å®é™…ä¸Šå¹¶ä¸æ¸…æ¥šæ­£ç¡®çš„æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Œä½†æ˜¯è¿™é‡Œçš„æµ‹è¯•é›†æ•°æ®éƒ½å‡ºç°åœ¨è®­ç»ƒæ¨¡å‹å’Œæ­£ç¡®æ¨¡å‹çš„äº¤ç‚¹é™„è¿‘ï¼Œæ‰€ä»¥è®¤ä¸ºæ˜¯Overfits</font>

![image-20221221220534020](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221220534020.png)

* RMS: root mean square, i.e., the square root of the mean square
* å‡æ–¹æ ¹:å‡æ–¹æ ¹ï¼Œå³å‡æ–¹æ ¹çš„å¹³æ–¹æ ¹

![image-20221221220541688](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221220541688.png)

* **Generalization æ³›åŒ– æ˜¯æŒ‡è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨å‰æ‰€æœªè§çš„æ•°æ®ä¸Šçš„æ€§èƒ½å¥½åã€‚**
* ![image-20230121215840627](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121215840627.png)

![image-20221221220550522](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221220550522.png)

#### 8.1 Prevent Overfitting

* **cause: training error and expected error are different** 
* **åŸå› :è®­ç»ƒè¯¯å·®ä¸é¢„æœŸè¯¯å·®ä¸ä¸€è‡´**
  * there may be noise in the training data 
  * è®­ç»ƒæ•°æ®ä¸­å¯èƒ½æœ‰å™ªå£°
  * training data is of limited size, resulting in difference from the true distribution 
  * è®­ç»ƒæ•°æ®è§„æ¨¡æœ‰é™ï¼Œä¸çœŸå®åˆ†å¸ƒå­˜åœ¨å·®å¼‚
  * the larger the hypothesis class, the easier to find a hypothesis that fits the difference between the training data and the true distribution 
  * å‡è®¾ç±»è¶Šå¤§ï¼Œå°±è¶Šå®¹æ˜“æ‰¾åˆ°ä¸€ä¸ªç¬¦åˆè®­ç»ƒæ•°æ®ä¸çœŸå®åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„å‡è®¾
* prevent overfitting: 
  * cleaner training data help! 
  * æ¸…ç†è®­ç»ƒæ•°æ®æ˜¯æœ‰å¸®åŠ©çš„
  * more training data help! 
  * æ›´å¤šçš„è®­ç»ƒæ•°æ®å¥½çš„
  * throwing away unnecessary hypotheses helps! (Occamâ€™s Razor) 
  * æŠ›å¼ƒä¸å¿…è¦çš„å‡è®¾

#### 8.2 Overfitting in Decision Tree

![image-20221221221128824](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221221128824.png)

åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†æ˜¯åœ¨çœŸæ˜¯æƒ…å†µè¡¨ç°ä¸å¥½

<font size=5>Example1ï¼šoverfitting with noisy data </font>

![image-20221221221448988](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221221448988.png)

å…ˆè§£é‡Šä¸€ä¸‹è¿™ä¸ªé—®é¢˜ï¼š 1. æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ±‚å‡ºç›®æ ‡å€¼ ![image-20230121220125930](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121220125930.png)

* **åœ¨correct treeä¸­ï¼Œæˆ‘ä»¬åº”è¯¥åˆ¤æ–­ç‰¹å¾X1å’Œç‰¹å¾X2ï¼Œç„¶ååœæ­¢ç”Ÿæˆå­æ ‘ï¼Œæ­¤æ—¶æ­£ç¡®ç‡ä¸º100ï¼Œå¾—å‡ºç»“æœã€‚**
* **åœ¨é”™è¯¯çš„æ¨¡å‹ä¸­ï¼Œå› ä¸ºæ•°æ®é”™è¯¯ä¸”æ¨¡å‹è¿‡äºæ‹Ÿåˆè®­ç»ƒæ•°æ®å¯¼è‡´ç”Ÿæˆäº†é”™è¯¯æ¨¡å‹**
  * **å…·ä½“çš„è¡¨ç¤ºæ˜¯æœ¬è¯¥åœ¨X2å¤„åœä¸‹çš„å†³ç­–æ ‘ï¼Œç»§ç»­è¿›è¡Œç”Ÿæˆã€‚**

![image-20221221221717427](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221221717427.png)

å› ä¸ºX2å› ä¸ºæœ‰noiseå¯¼è‡´äº†å­˜åœ¨X1=True,X2=Flase, Y=Trueçš„æƒ…å†µï¼Œæ‰€ä»¥å­˜åœ¨æœ‰å¤šä½™çš„æ ‘æ

![image-20221221222512312](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221222512312.png)

æ‰€ä»¥è®­ç»ƒé›†5/6 çœŸå®æƒ…å†µ100%

![image-20221221222604162](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221222604162.png)

![image-20221221222615103](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221222615103.png)



<font size=5>Example2ï¼šoverfitting with noise-free data ä¸çŸ¥é“å…·ä½“å™ªå£°åœ¨å“ªçš„æƒ…å†µ</font>

![image-20221221224248663](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221224248663.png)

![image-20221221224254857](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221224254857.png)

* åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›å…¶ä»–çš„ä¸œè¥¿
  * å› ä¸ºä¸ç¡®å®šä½ç½®çš„æ•°æ®é”™è¯¯ï¼Œå¯¼è‡´X3è¿™ä¸ªç‰¹å¾å®Œç¾ç¬¦åˆç»“æœï¼Œä»è€Œå¾—å‡ºè®­ç»ƒé›†çš„ç»“æœå®Œç¾ç¬¦åˆæ­£ç¡®ç»“æœï¼Œè€Œå®é™…å€¼å‡ºé”™
  * **è¿™é‡Œçš„0.5å’Œ0.66ä¸Šæ–‡æœ‰ç»™**

![image-20221221224301808](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221224301808.png)

å¦‚å›¾ï¼Œæ­£å¸¸æ¥è¯´ä¸Šè¿°è®­ç»ƒæ¨¡å‹å¯¹X3åº”è¯¥æœ‰100%çš„å‡†ç¡®åº¦ï¼Œä½†æ˜¯å®é™…æƒ…å†µæ˜¯50%



* **ä¸‹é¢çš„æ˜¯å¦ä¸€ç§æƒ…å†µï¼Œæ­¤æ—¶çš„æ¨¡å‹å¦‚ä¸‹å›¾æ‰€ç¤º**

![image-20221221224354515](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221224354515.png)

å¦‚å›¾ï¼Œæ­£å¸¸æƒ…å†µè¯¥æ¨¡å‹å¯¹Yåº”è¯¥æœ‰60%çš„æ­£ç¡®ç‡ï¼Œä½†å®é™…æ˜¯66%

![image-20221221224503976](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221221224503976.png)

* **å¾—å‡ºæœ€åçš„ç»“æœ, M1è¿‡æ‹Ÿåˆ**
* å› ä¸ºè®­ç»ƒé›†æ˜¯æœ‰é™çš„æ ·æœ¬ï¼Œå¯èƒ½ä¼šæœ‰å¶ç„¶ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å¾(ç»„åˆ)

#### 8.3 Avoiding overfitting in DT learning 

* two general strategies to avoid overfitting 
  * early stopping: stop if further splitting not justified by astatistical test 
  * æ—©æœŸåœæ­¢:å¦‚æœè¿›ä¸€æ­¥çš„åˆ†è£‚æ²¡æœ‰è¢«ç»Ÿè®¡æµ‹è¯•è¯æ˜æ˜¯åˆç†çš„ï¼Œå°±åœæ­¢
  * **<font color=red size=5>è¿™ç§æ–¹æ³•å¾€å¾€é€šè¿‡é™åˆ¶æ ‘çš„é«˜åº¦æ¥æ‰§è¡Œ</font>**
    * Quinlanâ€™s original approach in ID3 
  * post-pruning: grow a large tree, then prune back some nodes
  * åä¿®å‰ª:ç§æ¤ä¸€æ£µå¤§æ ‘ï¼Œç„¶åä¿®å‰ªä¸€äº›èŠ‚ç‚¹
    * more robust to myopia of greedy tree learning 
  * **<font color=red size=5>ä½¿ç”¨ç½®ä¿¡åº¦ä¸å¤Ÿçš„å­èŠ‚ç‚¹ä½¿ç”¨å¶å­èŠ‚ç‚¹æ¥ä»£æ›¿,è¿›è¡Œæ ‡è®°ï¼Œæ ‡è®°ååœ¨åå‰ªæçš„è¿‡ç¨‹ä¸­åˆ é™¤, è¿™ä¸ªæ›´å¸¸ç”¨ï¼Œå› ä¸ºç²¾ç¡®çš„ä¼°è®¡æ ‘åº”è¯¥ä½•æ—¶åœæ­¢å¢é•¿å¾ˆéš¾</font>**

## Lecture10 K-NN

https://blog.csdn.net/u010608296/article/details/120640343?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167171093816800188548770%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167171093816800188548770&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-120640343-null-null.142

-- K-NN

![image-20221222121030302](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222121030302.png)

### 1. Nearest-Neighbor Classification æœ€é‚»è¿‘åˆ†ç±»æ³•

* learning stage å­¦ä¹ é˜¶æ®µ
  * given a training set (x(1), y(1)) ... (x(m) , y(m)), do nothing  ä»€ä¹ˆä¹Ÿä¸åš
    * (itâ€™s sometimes called a lazy learner)
    * (æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸ºæ‡’æƒ°å­¦ä¹ è€…)
* classification stage åˆ†ç±»é˜¶æ®µ
  * given: an instance x(q)to classify ç»™ä¸€ä¸ªå…·ä½“å®ä¾‹å»åˆ†ç±»
  * <font color=red size=5>ç›¸å½“äºç»™ä¸€ä¸ªé›†åˆæ¥è¾…åŠ©ä¹‹åçš„æƒ…å†µå¯»æ‰¾ç›¸ä¼¼ç‚¹</font>
  * find the training-set instance x(i)that is most similar to x(q)
  * æ‰¾åˆ°ä¸x(q)æœ€ç›¸ä¼¼çš„è®­ç»ƒé›†å®ä¾‹x(i)
  * return the class value y(i) 
  * è¿”å›ç±»å€¼y(i)

#### 1.1 Nearest Neighbor æœ€é‚»è¿‘

* **When to Consider  ä½•æ—¶è€ƒè™‘**
  * **Less than 20 attributes per instance æ¯ä¸ªå®ä¾‹å°‘äº20ä¸ªå±æ€§**
  * **Lots of training data  æœ‰å¾ˆå¤šçš„è®­ç»ƒæ•°æ®**
* **Advantages** 
  * **Training is very fast è®­ç»ƒé€Ÿåº¦å¾ˆå¿«**
  * **Learn complex target functions å­¦ä¹ å¤æ‚çš„ç›®æ ‡å‡½æ•°**
  * **Do not lose information ä¸ä¼šä¸¢å¤±ä¿¡æ¯**
* **Disadvantages**  
  * **Slow at query time æŸ¥è¯¢ç¼“æ…¢ -- å› æ­¤æˆ‘ä»¬åœ¨åæ–‡ä½¿ç”¨K-D-Treeæ¥å¢åŠ é€Ÿåº¦**
  * **Easily fooled by irrelevant attributes   å®¹æ˜“è¢«ä¸ç›¸å…³çš„å±æ€§æ„šå¼„**

![image-20221222120834805](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222120834805.png)

**Voronoiå›¾:æ¯ä¸ªå¤šé¢ä½“è¡¨ç¤ºåœ¨æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„æœ€è¿‘é‚»åŸŸçš„ç‰¹å¾ç©ºé—´åŒºåŸŸ**



![image-20221222121147885](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222121147885.png)

![image-20230121223126999](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121223126999.png)

![image-20230121223148396](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121223148396.png)



#### 1.2 The Defination of K-NN

* Classification task
  * given: an instance x(q)to classify  ç»™ä¸€ä¸ªå®ä¾‹x(q)æ¥è¿›è¡Œåˆ†ç±»
  * find the k training-set instances (x(1), y(1))... (x(k), y(k)) that are the most similar to x(q)
  * æ±‚kä¸ªè®­ç»ƒé›†å®ä¾‹(x(1)ï¼Œ y(1))â€¦(x(k) y(k))å’Œx(q)æœ€ç›¸ä¼¼
  * return the class value 
  * ![image-20221222121449548](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222121449548.png)
  * (i.e. return the class that have the most number of instances in the k training 
    instances
  * è¿”å›kä¸ªè®­ç»ƒå®ä¾‹ä¸­å®ä¾‹æ•°æœ€å¤šçš„ç±»
  
  ![image-20230121231544822](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121231544822.png)

##### 1.2.1 How can we determine similarity/distance  å¦‚ä½•åˆ¤æ–­å±æ€§ä¹‹é—´çš„è·ç¦»

* **Suppose all features are categorical  å‡è®¾æ‰€æœ‰çš„ç‰¹å¾éƒ½æ˜¯åˆ†ç±»çš„**
  * **Hamming distance (or L0norm)**: count the number of features for which two instances differ
  * **æ±‰æ˜è·ç¦»ï¼š  è®¡ç®—ä¸¤ä¸ªå®ä¾‹ä¸åŒçš„ç‰¹å¾çš„æ•°é‡**
  * å®ä¾‹æ˜¯ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œç‰¹å¾æ˜¯ç”¨äºåˆ†ç±»çš„ï¼Œä¸€ä¸ªå®ä¾‹å¯ä»¥æœ‰å¤šä¸ªç‰¹å¾
* Example: X = (Weekday, Happy?, Weather)  Y = AttendLecture? 
  * D : in the table
  * New instance: <Friday, No, Rain>
  * Distances = {2, 3, 1, 2} 
  * For 1-nn, which instances should be selected? 
  * For 2-nn, which instances should be selected? 
  * For 3-nn, which instances should be selected?

![image-20221222124416941](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222124416941.png)



![image-20221222124448472](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222124448472.png)

**Euclidean Distance æ¬§å¼è·ç¦»**

**Manhattan Distance æ›¼å“ˆé¡¿è·ç¦»**

![image-20230121232403469](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121232403469.png) 

![image-20230121232629796](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121232629796.png)

![image-20221222124654107](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222124654107.png)

![image-20221222124708205](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222124708205.png)

###### 1.2.1.1How can we determine similarity/distance 

* **è¿™ç§æ˜¯å¯¹äºä¸åŒçš„ç‰¹å¾ä½¿ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œé¢å¯¹è¿ç»­ç‰¹å¾å’Œé—´æ–­ç‰¹å¾ä½¿ç”¨ä¸åŒçš„å€¼**

![image-20230121232921248](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121232921248.png)

##### 1.2.2 Standardizing numeric features  æ ‡å‡†åŒ–æ•°å­—ç‰¹å¾

* åŸå› ï¼šå› ä¸ºä¸åŒçš„ç‰¹å¾å¾€å¾€ä¼šæœ‰ä¸åŒçš„è§„æ ¼ï¼Œè¿™äº›æ•°æ®å¾€å¾€æ˜¯å› ä¸ºç”¨äºæè¿°ä¸åŒç§ç±»çš„æ•°æ®è€Œå·®å¼‚å·¨å¤§ï¼Œæ¯”å¦‚æ¥¼å±‚å’Œè·ç¦»å¸‚ä¸­å¿ƒçš„è·ç¦»ä¸ºä¸¤ä¸ªç‰¹å¾ï¼Œæ¥¼å±‚å¯ä»¥æ˜¯1-10ï¼Œå¸‚ä¸­å¿ƒè·ç¦»å¯èƒ½æœ‰å‡ åƒç±³
* **è¿™æ ·çš„å·®åˆ«å°±ä¼šå½±å“æœ€ç»ˆçš„å·¥ä½œæ•ˆç‡. æ‰€ä»¥, æˆ‘ä»¬è¦æé«˜æ•ˆç‡, ç‰¹å¾çš„æ ‡å‡†åŒ–å°±å¯ä»¥å¸®ä¸Šå¿™.æˆ‘ä»¬åœ¨æœºå™¨å­¦ä¹ è®­ç»ƒä¹‹å‰, å…ˆå¯¹æ•°æ®é¢„å…ˆå¤„ç†ä¸€ä¸‹, å–å€¼è·¨åº¦å¤§çš„ç‰¹å¾æ•°æ®, æˆ‘ä»¬æµ“ç¼©ä¸€ä¸‹, è·¨åº¦å°çš„æ‹¬å±•ä¸€ä¸‹, ä½¿å¾—ä»–ä»¬çš„è·¨åº¦å°½é‡ç»Ÿä¸€.**

* given the training set D, determine the mean and stddev for feature xi
* ç»™å®šè®­ç»ƒé›†Dï¼Œç¡®å®šç‰¹å¾xiçš„å‡å€¼å’Œstddev

![image-20221222125135200](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222125135200.png)

* standardize each value of feature xias follows 
* **æ ‡å‡†åŒ–ç‰¹å¾çš„æ¯ä¸ªå€¼ï¼Œå¦‚ä¸‹æ‰€ç¤º**
* ![image-20221222125210953](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222125210953.png)
* do the same for test instances, using the same ğœ‡and ğœderived from the training data 
* å¯¹æµ‹è¯•å®ä¾‹åšç›¸åŒçš„äº‹æƒ…

#### 1.3 Speeding up k-NN

<Font size=5>Issues</font>

* Choosing k
  * **Increasing k reduces variance, increases bias** 
  * **å¢åŠ kå‡å°‘æ–¹å·®ï¼Œå¢åŠ åå·®**
* For high-dimensional space, problem that the nearest neighbor may not be very close at all! 
* å¯¹äºä¸€äº›é«˜ç»´ç©ºé—´ï¼Œç›¸é‚»çš„ç‚¹å¯èƒ½å¹¶ä¸æ˜¯è¿™ä¹ˆå¤š
* Memory-based technique. Must make a pass through the data for each classification. This can be prohibitive for large data sets.
* åŸºäºå†…å­˜çš„æŠ€æœ¯ã€‚å¿…é¡»å¯¹æ¯ä¸ªåˆ†ç±»çš„æ•°æ®è¿›è¡Œéå†ã€‚è¿™å¯¹äºå¤§å‹æ•°æ®é›†æ¥è¯´å¯èƒ½æ˜¯ä¸å¯å–çš„ã€‚

![image-20221222130205278](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222130205278.png)

* Given sample S = ((x1,y1),...,(xm,ym)) and a test point x, 
* ç»™å®šæ ·æœ¬S = ((x1,y1)ï¼Œâ€¦ï¼Œ(xm,ym))å’Œæµ‹è¯•ç‚¹xï¼Œ
* it is to find the nearest k neighbours of x
* å°±æ˜¯æ‰¾åˆ°xçš„kä¸ªæœ€è¿‘é‚»å±…ã€‚
* **Note: for the algorithms, dimensionality N, i.e., number of features, is crucial.** 
* **å¯¹äºç®—æ³•ï¼Œç»´æ•°Nï¼Œå³ç‰¹å¾çš„æ•°é‡ï¼Œæ˜¯è‡³å…³é‡è¦çš„ã€‚**

![image-20230121233755406](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121233755406.png)

* k-NN is a â€œlazyâ€ learning algorithm â€“ does virtually nothing at training time 
* k-NNæ˜¯ä¸€ç§â€œæ‡’æƒ°â€å­¦ä¹ ç®—æ³•â€”â€”åœ¨è®­ç»ƒæ—¶å‡ ä¹ä»€ä¹ˆéƒ½ä¸åš
* **but classification/prediction time can be costly when the training set is large** 
* **ä½†æ˜¯å½“è®­ç»ƒé›†å¾ˆå¤§æ—¶ï¼Œåˆ†ç±»/é¢„æµ‹æ—¶é—´ä¼šå¾ˆæ˜‚è´µ**

![image-20230121233856960](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121233856960.png)

* two general strategies for alleviating this weakness 
* ç¼“è§£è¿™ä¸€å¼±ç‚¹çš„ä¸¤ç§ä¸€èˆ¬ç­–ç•¥
  * **donâ€™t retain every training instance (edited nearest neighbor)** 
  * **ä¸è¦ä¿ç•™æ¯ä¸ªè®­ç»ƒå®ä¾‹(å·²ç¼–è¾‘çš„æœ€è¿‘é‚»)**
  * **pre-processing. Use a smart data structure to look up nearest neighbors (e.g. a k-d tree)** 
  * **é¢„å¤„ç†ã€‚ä½¿ç”¨æ™ºèƒ½æ•°æ®ç»“æ„æŸ¥æ‰¾æœ€è¿‘çš„é‚»å±…(ä¾‹å¦‚k-dæ ‘)**

![image-20230121234310108](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121234310108.png)

##### 1.4.1 Edited instance-based learning ç¼–è¾‘åŸºäºå®ä¾‹çš„å­¦ä¹ 

![image-20230121234339436](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230121234339436.png)

##### 1.4.2  K-D-Tree  

https://blog.csdn.net/qq_33143379/article/details/80962326?ops_request_misc=&request_id=&biz_id=102&utm_term=k-d%20tree&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-80962326.142

  k-d-treeï¼ˆå³k-dimensional treeï¼‰æ˜¯ä¸€æ£µå½¢å¦‚äºŒå‰æ ‘çš„ä¸€ç§éå¸¸é‡è¦çš„ç©ºé—´åˆ’åˆ†æ•°æ®ç»“æ„ï¼Œå°¤å…¶åœ¨å¤šç»´æ•°æ®è®¿é—®ä¸­æœ‰é‡è¦åº”ç”¨ï¼Œå®ƒèƒ½æ˜¾è‘—é™ä½è¿ç®—æ¬¡æ•°ã€æé«˜è¿ç®—æ•ˆç‡ï¼›**ä¸»è¦åº”ç”¨äºå¤šç»´ç©ºé—´å…³é”®æ•°æ®çš„æœç´¢ï¼ˆå¦‚ï¼šèŒƒå›´æœç´¢å’Œæœ€è¿‘é‚»æœç´¢ï¼‰ã€‚**

* <font color=red>æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªå¯¹K-NNè¿›è¡Œé¢„å¤„ç†çš„æ–¹æ³•</font>

* a k-d tree is similar to a decision tree except that each internal node 
* ä¸€ä¸ªk-d treeä¸å†³ç­–æ ‘éå¸¸ç›¸åƒé™¤äº†æ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹ä¸åŒ
  * **stores one instance**  
  * **å­˜å‚¨ä¸€ä¸ªå®ä¾‹**
  * **splits on the median value of the feature having the highest variance** 
  * **å¯¹å…·æœ‰æœ€é«˜æ–¹å·®çš„ç‰¹å¾çš„ä¸­å€¼è¿›è¡Œåˆ†å‰²**

![image-20221222163327385](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222163327385.png)

![image-20221222163810670](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222163810670.png)

![image-20221222163824803](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222163824803.png)

![image-20221222163833621](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222163833621.png)

* **è¿™æ ·æˆ‘ä»¬å°±åœ¨ä¸€ä¸ªå®ä¾‹ä¸­åˆ’åˆ†äº†ä¸åŒçš„åŒºåŸŸï¼Œä½¿å¾—åœ¨åç»­å¯»æ‰¾é‚»è¿‘ç‚¹æ—¶æ›´åŠ ä¾¿æ·ã€‚**

###### 1.4.2.1 Finding nearest neighbors with a k-d tree

* use branch-and-bound search 
* ä½¿ç”¨åˆ†æ”¯å’Œè¾¹ç•Œæœç´¢
* priority queue stores 
* ä¼˜å…ˆé˜Ÿåˆ—å­˜å‚¨
  * nodes considered
  * è€ƒè™‘èŠ‚ç‚¹
  * lower bound on their distance to query instance 
  * å®ƒä»¬åˆ°æŸ¥è¯¢å®ä¾‹è·ç¦»çš„ä¸‹ç•Œ
* lower bound given by distance using a single feature 
* ä¸‹ç•Œç”±ä½¿ç”¨å•ä¸€ç‰¹å¾çš„è·ç¦»ç»™å‡º
* average case: O(log2m) 
* å¹³å‡æƒ…å†µ:O(log2m)
* worst case: O(m) where m is the size of the training-set
* æœ€åæƒ…å†µ:O(m)ï¼Œå…¶ä¸­mæ˜¯è®­ç»ƒé›†çš„å¤§å°

![image-20221222164439045](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222164439045.png)

![image-20221222164705166](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222164705166.png)



* EXAMPLEï¼šè¿™ä¸ªæ˜¯è€å¸ˆä¸Šè¯¾ç»™çš„ä¾‹å­ï¼Œè®²çœŸæ¯å¤ªçœ‹æ‡‚
* ![image-20230122000651357](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000651357.png)

![image-20230122000657014](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000657014.png)

![image-20230122000708043](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000708043.png)

![image-20230122000714796](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000714796.png)

![image-20230122000723198](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000723198.png)

![image-20230122000736930](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000736930.png)

![image-20230122000820587](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000820587.png)

![image-20230122000843597](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122000843597.png)

* Example è¿™ä¸ªä¾‹å­æ›´åŠ é€šä¿—æ˜“æ‡‚

![image-20230122001051619](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122001051619.png)

![image-20230122001106664](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122001106664.png)





#### 1.5 Variants of K-NN   K-NNçš„å˜ä½“

* learning stage
* å­¦ä¹ é˜¶æ®µ 
  *  given a training set (x(1), y(1)) ... (x(m) , y(m)), do nothing 
  *  ç»™å®šä¸€ä¸ªè®­ç»ƒé›†(x(1)ï¼Œ y(1))ï¼Œâ€¦(x(m) y(m))ä»€ä¹ˆéƒ½ä¸åš
    * (itâ€™s sometimes called a lazy learner) 
* classification stage
* åˆ†ç±»é˜¶æ®µ
  * given: an instance x(q)to classify
  * ç»™å®š:ä¸€ä¸ªå®ä¾‹x(q)è¿›è¡Œåˆ†ç±»
  * find the k training-set instances (x(1), y(1))... (x(k), y(k)) that are most similar to x(q)
  * æ±‚kä¸ªè®­ç»ƒé›†å®ä¾‹(x(1)ï¼Œ y(1))â€¦(x(k) y(k))å’Œx(q)æœ€ç›¸ä¼¼
  * return the value ![image-20221222165537629](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222165537629.png)



![image-20221222165442929](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222165442929.png)

**å¾ˆæ˜æ˜¾ï¼Œç›¸æ¯”äºNearest-Neighbor Classificationï¼Œè¿™é‡Œè¿”å›çš„å€¼æ˜¯å‘¨å›´ç›¸ä¼¼å€¼çš„å¹³å‡å€¼**

* find the k training-set instances (x(1), y(1))... (x(k), y(k)) that are most similar to x(q)
* æ±‚kä¸ªè®­ç»ƒé›†å®ä¾‹(x(1)ï¼Œ y(1))â€¦(x(k) y(k))å’Œx(q)æœ€ç›¸ä¼¼

##### 1.5.1 Distance-Weighted Nearest Neighbor æœ€è¿‘ä¸´è¿‘ç‚¹çš„è·ç¦»æƒé‡

* We can have instances contribute to a prediction according to their distance from x(q) 
* é€šè¿‡æ¯ä¸ªå®ä¾‹åˆ°x(q)çš„è·ç¦»æ¥é¢„æµ‹å®ƒå¯¹ç›®æ ‡å€¼çš„è´¡çŒ®
* classification: 

![image-20221222170308996](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222170308996.png)

##### 1.5.2 Irrelevant features in instance-based learning  åŸºäºå®ä¾‹çš„å­¦ä¹ ä¸­ä¸ç›¸å…³çš„ç‰¹å¾

* hereâ€™s a case in which there is one relevant feature x1and a 1-NN rule classifies each instance correctly 
* è¿™é‡Œæœ‰ä¸€ä¸ªæ¡ˆä¾‹ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªç›¸å…³çš„ç‰¹å¾x1ï¼Œå¹¶ä¸”1-NNè§„åˆ™æ­£ç¡®åœ°åˆ†ç±»äº†æ¯ä¸ªå®ä¾‹
* consider the effect of an irrelevant feature x2on distances and nearest neighbors
* è€ƒè™‘ä¸€ä¸ªä¸ç›¸å…³çš„ç‰¹å¾x2å¯¹è·ç¦»å’Œæœ€è¿‘é‚»å±…çš„å½±å“

![image-20221222172644721](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222172644721.png)

* Can you find a point (a,b) which is red, if classified only according to feature x1, and is green, if classified according to both features? 
* å¦‚æœåªæ ¹æ®ç‰¹å¾x1åˆ†ç±»ï¼Œä½ èƒ½æ‰¾åˆ°ä¸€ä¸ªç‚¹(a,b)ï¼Œå®ƒæ˜¯çº¢è‰²çš„ï¼Œå¦‚æœæ ¹æ®ä¸¤ä¸ªç‰¹å¾åˆ†ç±»ï¼Œå®ƒæ˜¯ç»¿è‰²çš„å—?

##### 1.5.3 Locally weighted regression  å±€éƒ¨åŠ æƒå›å½’

* One way around this limitation is to weight features differently 
* ç»•è¿‡è¿™ç§é™åˆ¶çš„ä¸€ç§æ–¹æ³•æ˜¯å¯¹ç‰¹å¾è¿›è¡Œä¸åŒçš„æƒé‡
* locally weighted regression is one nearest-neighbor variant that does this
* å±€éƒ¨åŠ æƒå›å½’æ˜¯ä¸€ç§æœ€è¿‘é‚»æ–¹æ³•çš„å˜ç§

* prediction task 
  * given: an instance x(q) to make a prediction for 
  * find the k training-set instances (x(1), y(1)) ... (x(k), y(k)) that are most similar to x(q)
  * æ±‚kä¸ªè®­ç»ƒé›†å®ä¾‹(x(1)ï¼Œ y(1))â€¦(x(k) y(k))å’Œx(q)æœ€ç›¸ä¼¼
  * return the value ![image-20221222173212414](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222173212414.png)

![image-20221222173130651](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222173130651.png)

![image-20221222173314958](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222173314958.png)

#### 1.6 Discussion  Advantage and Disadvantage of instance-based learning  åŸºäºå®ä¾‹çš„å­¦ä¹ 

* simple to implement 
* æ˜“äºå®æ–½
* â€œtrainingâ€ is very efficient 
* è®­ç»ƒé«˜æ•ˆ
* adapts well to on-line learning 
* é€‚åº”åœ¨çº¿å­¦ä¹ 
* robust to noisy training data (when k > 1) 
* å¯¹å™ªå£°è®­ç»ƒæ•°æ®çš„é²æ£’æ€§(å½“k > 1)
* often works well in practice 
* åœ¨ç»ƒä¹ ä¸­å·¥ä½œè‰¯å¥½



* sensitive to range of feature values 
* å¯¹ç‰¹å¾å€¼èŒƒå›´æ•æ„Ÿ
* sensitive to irrelevant and correlated features, although ... 
* å¯¹ä¸ç›¸å…³å’Œç›¸å…³çš„ç‰¹å¾å¾ˆæ•æ„Ÿï¼Œå°½ç®¡â€¦
  * there are variants (such as locally weighted regression) that learn weights for different features
  * æœ‰ä¸€äº›å˜ä½“(å¦‚å±€éƒ¨åŠ æƒå›å½’)å¯ä»¥å­¦ä¹ ä¸åŒç‰¹å¾çš„æƒé‡
* classification/prediction can be inefficient, although ...
* åˆ†ç±»/é¢„æµ‹å¯èƒ½æ•ˆç‡å¾ˆä½ï¼Œå°½ç®¡â€¦â€¦
  * edited methods and k-d trees can help alleviate this weakness 
  * ç¼–è¾‘è¿‡çš„æ–¹æ³•å’Œk-dæ ‘å¯ä»¥å¸®åŠ©ç¼“è§£è¿™ä¸ªå¼±ç‚¹
* doesnâ€™t provide much insight into problem domain because there is no explicit model 
* å› ä¸ºæ²¡æœ‰æ˜ç¡®çš„æ¨¡å‹ï¼Œæ‰€ä»¥æ— æ³•æ·±å…¥äº†è§£é—®é¢˜é¢†åŸŸ







## Lecture12 Linear and Logistic Regression

### 1. Recap what we learned

#### 1.1 Inductive Bias å½’çº³åå·®

https://blog.csdn.net/qq_39478403/article/details/121107057?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167174638116800182166731%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167174638116800182166731&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121107057-null-null.142

**<font color='red'>å½’çº³åç½®å¯ä»¥ç†è§£ä¸ºï¼Œä»ç°å®ç”Ÿæ´»ä¸­è§‚å¯Ÿåˆ°çš„ç°è±¡ä¸­å½’çº³å‡ºä¸€å®šçš„è§„åˆ™ (heuristics)ï¼Œç„¶åå¯¹æ¨¡å‹åšä¸€å®šçš„çº¦æŸï¼Œä»è€Œå¯ä»¥èµ·åˆ° â€œæ¨¡å‹é€‰æ‹©â€ çš„ä½œç”¨ï¼Œç±»ä¼¼è´å¶æ–¯å­¦ä¹ ä¸­çš„ â€œå…ˆéªŒâ€ã€‚</font>**

![image-20221222220212870](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222220212870.png)

* **inductive bias is the set of assumptions a learner uses to be able to predict yi for a previously unseen instance xi** 
* **å½’çº³åå·®æ˜¯å­¦ä¹ è€…èƒ½å¤Ÿä½¿ç”¨çš„ä¸€ç»„å‡è®¾æ¥é¢„æµ‹æœªè§è¿‡çš„å®ä¾‹xiçš„yi**
* two components 
  * **hypothesis space bias**: determines the models that can be represented 
  * å‡è®¾ç©ºé—´åå·®:å†³å®šå¯ä»¥è¡¨ç¤ºçš„æ¨¡å‹
  * **preference bias**: specifies a preference ordering within the space of models 
  * åå¥½åå·®:æŒ‡å®šæ¨¡å‹ç©ºé—´å†…çš„åå¥½é¡ºåº
* in order to generalize (i.e. make predictions for previously unseen instances) a learning algorithm must have an inductive bias 
* ä¸ºäº†æ¨å¹¿(å³å¯¹ä»¥å‰æœªè§è¿‡çš„å®ä¾‹è¿›è¡Œé¢„æµ‹)ï¼Œå­¦ä¹ ç®—æ³•å¿…é¡»å…·æœ‰å½’çº³åå·®
* **å½’çº³åå·®å¾€å¾€ç”¨äºæ³›åŒ–**

![image-20221222215726282](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222215726282.png)

![image-20230122100531048](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122100531048.png)

![image-20221222215810913](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222215810913.png)

![image-20221222215923501](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222215923501.png)





### 2. Linear and Logistic Regression

#### 2.1 Recap: dot product in linear algebra

![image-20221222223006477](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222223006477.png)



#### 2.2 Linear Regression çº¿æ€§å›å½’

https://blog.csdn.net/suotanyu1595/article/details/120362070?ops_request_misc=&request_id=&biz_id=102&utm_term=%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-120362070.142

* å…³äºå›å½’ç®—æ³•å’Œåˆ†ç±»ç®—æ³•
* å…¶å®å›å½’ç®—æ³•æ˜¯ç›¸å¯¹åˆ†ç±»ç®—æ³•è€Œè¨€çš„ï¼Œä¸æˆ‘ä»¬æƒ³è¦é¢„æµ‹çš„ç›®æ ‡å˜é‡yçš„å€¼ç±»å‹æœ‰å…³ã€‚å¦‚æœç›®æ ‡å˜é‡yæ˜¯**åˆ†ç±»å‹å˜é‡**ï¼Œå¦‚é¢„æµ‹ç”¨æˆ·çš„æ€§åˆ«ï¼ˆç”·ã€å¥³ï¼‰ï¼Œé¢„æµ‹æœˆå­£èŠ±çš„é¢œè‰²ï¼ˆçº¢ã€ç™½ã€é»„â€¦â€¦ï¼‰ï¼Œé¢„æµ‹æ˜¯å¦æ‚£æœ‰è‚ºç™Œï¼ˆæ˜¯ã€å¦ï¼‰ï¼Œé‚£æˆ‘ä»¬å°±éœ€è¦ç”¨åˆ†ç±»ç®—æ³•å»æ‹Ÿåˆè®­ç»ƒæ•°æ®å¹¶åšå‡ºé¢„æµ‹ï¼›å¦‚æœyæ˜¯**è¿ç»­å‹å˜é‡**ï¼Œå¦‚é¢„æµ‹ç”¨æˆ·çš„æ”¶å…¥ï¼ˆ4åƒï¼Œ2ä¸‡ï¼Œ10ä¸‡â€¦â€¦ï¼‰ï¼Œé¢„æµ‹å‘˜å·¥çš„é€šå‹¤è·ç¦»ï¼ˆ500mï¼Œ1kmï¼Œ2ä¸‡é‡Œâ€¦â€¦ï¼‰ï¼Œé¢„æµ‹æ‚£è‚ºç™Œçš„æ¦‚ç‡ï¼ˆ1%ï¼Œ50%ï¼Œ99%â€¦â€¦ï¼‰ï¼Œæˆ‘ä»¬åˆ™éœ€è¦ç”¨å›å½’æ¨¡å‹ã€‚
* èªæ˜çš„ä½ ä¸€å®šä¼šå‘ç°ï¼Œæœ‰æ—¶åˆ†ç±»é—®é¢˜ä¹Ÿå¯ä»¥è½¬åŒ–ä¸ºå›å½’é—®é¢˜ï¼Œä¾‹å¦‚åˆšåˆšä¸¾ä¾‹çš„è‚ºç™Œé¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å›å½’æ¨¡å‹å…ˆé¢„æµ‹å‡ºæ‚£è‚ºç™Œçš„æ¦‚ç‡ï¼Œç„¶åå†ç»™å®šä¸€ä¸ªé˜ˆå€¼ï¼Œä¾‹å¦‚50%ï¼Œæ¦‚ç‡å€¼åœ¨50%ä»¥ä¸‹çš„äººåˆ’ä¸ºæ²¡æœ‰è‚ºç™Œï¼Œ50%ä»¥ä¸Šåˆ™è®¤ä¸ºæ‚£æœ‰è‚ºç™Œã€‚
* è¿™ç§åˆ†ç±»å‹é—®é¢˜çš„å›å½’ç®—æ³•é¢„æµ‹ï¼Œæœ€å¸¸ç”¨çš„å°±æ˜¯é€»è¾‘å›å½’ï¼Œåé¢æˆ‘ä»¬ä¼šè®²åˆ°ã€‚
  

![image-20221222223744981](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222223744981.png)

* çº¿æ€§å›å½’ä¸€èˆ¬åˆ†ä¸ºä¸€ç»´çº¿æ€§å›å½’å’Œå¤šå…ƒçº¿æ€§å›å½’
* å…·ä½“é“¾æ¥ï¼šhttps://blog.csdn.net/iqdutao/article/details/109402570?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167174844416800188526203%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167174844416800188526203&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-109402570-null-null.142

![image-20221222224052640](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222224052640.png)

* xiçš„é”™è¯¯ç‡
* æ‰€æœ‰è®­ç»ƒå®ä¾‹çš„ç©ºé—´é”™è¯¯ç‡
* æ‰€æœ‰è®­ç»ƒå®ä¾‹çš„å¹³å‡ç©ºé—´é”™è¯¯ç‡
* **<font color=red>ä¸Šè¿°å‡½æ•°ä¸ºæŸå¤±å‡½æ•°</font>**



* **å…³äºé”™è¯¯ç‡ï¼Œå³æŸå¤±å‡½æ•°ï¼Œé€šè¿‡ä¸Šå›¾çš„å…¬å¼å¯»æ‰¾æœ€å°çš„æŸå¤±å‡½æ•°**

![image-20221222224206398](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222224206398.png)

#### 2.3 Organise feature data into matrix åœ¨çŸ©é˜µä¸­ç»„ç»‡ç‰¹å¾æ•°æ®

![image-20221222225037801](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222225037801.png)

#### 2.4 Transform input matrix with weight vector é€šè¿‡æƒé‡å‘é‡è½¬æ¢è¾“å…¥çŸ©é˜µ

* ç»™æ¯ä¸€ä¸ªç‰¹å¾è®¾ç½®æƒé‡

![image-20221222225836676](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222225836676.png)

 ![image-20221222225850692](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222225850692.png)

#### 2.5 Error Representation è¯¯å·®è¡¨ç¤º

![image-20221222230020183](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222230020183.png)

<font color=red size=5>è¿™æ ·æˆ‘ä»¬å°±æŠŠè®¡ç®—**æŸå¤±å‡½æ•°ï¼Œå³é”™è¯¯ç‡çš„å…¬å¼æ¢æˆäº†å¦ä¸€ä¸ªè¡¨ç°å½¢å¼ï¼Œä¸¤ä¸ªçŸ©é˜µç›¸å‡**</font>

![image-20221222230341898](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222230341898.png)

### 3. Variant: Linear regression çº¿æ€§å›å½’çš„å˜ä½“

#### 3.1 Variant: Linear regression with bias  å˜ä½“:æœ‰åå·®çš„çº¿æ€§å›å½’

![image-20221222230630418](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222230630418.png)

![image-20221222230638587](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222230638587.png)

**è¿™é‡Œæ˜¯é€šè¿‡ä¸Šè¿°ç»è¿‡æƒé‡å‘é‡è½¬æ¢åçš„inputï¼Œå°†å…¶åŠ å…¥biasçš„å½±å“**

#### 3.2 Variant: Linear regression with lasso penalty  å˜ä½“:çº¿æ€§å›å½’ä¸å¥—ç´¢æƒ©ç½š

https://blog.csdn.net/matrix_studio/article/details/121115119?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167175057616800182754280%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167175057616800182754280&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-121115119-null-null.142

![image-20221222231052438](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222231052438.png)

LassoåŒæ ·æ˜¯çº¿æ€§å›å½’çš„ä¸€ç§å˜ä½“ã€‚è€Œæ–‡æ¡£ä¸­æŒ‡å‡ºï¼Œå®ƒæ˜¯ä¸€ç§èƒ½è®©å‚æ•°Ï‰ \omegaÏ‰ç¨€ç–çš„æ¨¡å‹ï¼ˆä½œç”¨ï¼‰ã€‚

<font color=red>ä¸æ‡‚ï¼Œä½†å¥½åƒä¸æ˜¯å¾ˆé‡è¦</font>

#### 3.3 Variant: Evaluation Metrics å˜ä½“ï¼šè¯„ä»·æ ‡å‡†

* Root mean squared error (RMSE) 
* å‡æ–¹æ ¹è¯¯å·®(RMSE)
* Mean absolute error (MAE) â€“ average L1error 
* å¹³å‡ç»å¯¹è¯¯å·®(MAE) -å¹³å‡l1è¯¯å·®
* R-square (R-squared) 
* rå¹³æ–¹(å¹³æ–¹)
* Historically all were computed on training data, and possibly adjusted after, but really should cross-validate 
* ä»å†å²ä¸Šçœ‹ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ˜¯æ ¹æ®è®­ç»ƒæ•°æ®è®¡ç®—çš„ï¼Œå¯èƒ½åœ¨è®­ç»ƒåè¿›è¡Œè°ƒæ•´ï¼Œä½†å®é™…ä¸Šåº”è¯¥è¿›è¡Œäº¤å‰éªŒè¯

### 4.Linear classification çº¿æ€§åˆ†ç±»

![image-20221222231805703](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222231805703.png)

* æ ¹æ®f(x)çš„çº¿è¿›è¡Œåˆ†ç±»

![image-20221222231922060](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222231922060.png)

* Piecewise Linear model ğ“— åˆ†æ®µçº¿æ€§æ¨¡å‹
* <font color='red'>å°±æ˜¯é’ˆå¯¹wTxçš„å€¼ï¼Œå¯¹yè¿›è¡Œåˆ†ç±»</font>

![image-20221222232201226](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222232201226.png)

* **loss = 0, i.e., no loss, when the classification is the same as its label.  loss =1, otherwise.** 
* **å¦‚æœloss=0ï¼Œåˆ™æ²¡æœ‰lossï¼Œæ‰€ä»¥åˆ†ç±»æ‹¥æœ‰ç›¸åŒçš„æ ‡ç­¾ã€‚å¦åˆ™loss=1ï¼Œæ ‡ç­¾ä¸åŒ**

![image-20221222232344487](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221222232344487.png)

ç¼ºç‚¹ï¼š1. å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ	2.éš¾ä»¥ä¼˜åŒ–



### 5. Logistic Regression é€»è¾‘å›å½’

https://blog.csdn.net/weixin_55073640/article/details/124683459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167179893616800213012717%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167179893616800213012717&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-124683459-null-null.142

é€»è¾‘å›å½’ä¹Ÿç§°ä½œlogisticå›å½’åˆ†æï¼Œæ˜¯ä¸€ç§å¹¿ä¹‰çš„çº¿æ€§å›å½’åˆ†ææ¨¡å‹ï¼Œå±äºæœºå™¨å­¦ä¹ ä¸­çš„ç›‘ç£å­¦ä¹ ã€‚å…¶æ¨å¯¼è¿‡ç¨‹ä¸è®¡ç®—æ–¹å¼ç±»ä¼¼äºå›å½’çš„è¿‡ç¨‹ï¼Œä½†å®é™…ä¸Šä¸»è¦æ˜¯ç”¨æ¥è§£å†³**äºŒåˆ†ç±»é—®é¢˜**ï¼ˆä¹Ÿå¯ä»¥è§£å†³å¤šåˆ†ç±»é—®é¢˜ï¼‰ã€‚é€šè¿‡ç»™å®šçš„nç»„æ•°æ®ï¼ˆè®­ç»ƒé›†ï¼‰æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒç»“æŸåå¯¹ç»™å®šçš„ä¸€ç»„æˆ–å¤šç»„æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰è¿›è¡Œ**åˆ†ç±»**ã€‚å…¶ä¸­æ¯ä¸€ç»„æ•°æ®éƒ½æ˜¯ç”±p ä¸ªæŒ‡æ ‡æ„æˆã€‚


![image-20221223123853382](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223123853382.png)

 å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå…¶ä¸­ç‚¹çš„ä¸ªæ•°æ˜¯æ ·æœ¬ä¸ªæ•°ï¼Œä¸¤ç§é¢œè‰²ä»£è¡¨ä¸¤ç§æŒ‡æ ‡ã€‚è¿™ä¸ªç›´çº¿å¯ä»¥çœ‹æˆç»è¿™äº›æ ·æœ¬è®­ç»ƒåå¾—å‡ºçš„åˆ’åˆ†æ ·æœ¬çš„ç›´çº¿ã€‚é‚£ä¹ˆå¯¹äºä¹‹åçš„æ ·æœ¬çš„p1ä¸p2çš„å€¼ï¼Œå°±å¯ä»¥æ ¹æ®è¿™æ¡ç›´çº¿æ¥åˆ¤æ–­å®ƒå±äºå“ªä¸€ç±»äº†ã€‚

**<font color='red'>æ„Ÿè§‰å°±æ˜¯ç”»æ¡çº¿å°†æ•°æ®é›†åˆ†æˆä¸¤ä¸ªç±»</font>**

#### 5.1.Introduction of Logistic Regression 

* ä»æœ¬è´¨ä¸Šæ¥è¯´ï¼Œé€»è¾‘å›å½’è®­ç»ƒåçš„æ¨¡å‹æ˜¯å¹³é¢çš„ä¸€æ¡ç›´çº¿ï¼ˆp=2),æˆ–æ˜¯å¹³é¢ï¼ˆp=3)ï¼Œè¶…å¹³é¢ï¼ˆp>3)ã€‚å¹¶ä¸”è¿™æ¡çº¿æˆ–å¹³é¢æŠŠç©ºé—´ä¸­çš„æ•£ç‚¹åˆ†æˆä¸¤åŠï¼Œå±äºåŒä¸€ç±»çš„æ•°æ®å¤§å¤šæ•°åˆ†å¸ƒåœ¨æ›²çº¿æˆ–å¹³é¢çš„åŒä¸€ä¾§ã€‚
* ![image-20230122111531298](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122111531298.png)

<font size=5>Why logistic regresion?</font>

![image-20230122111624408](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122111624408.png)

* It's tempting to use the linear regression output as probabilities
* æˆ‘ä»¬å¾ˆå®¹æ˜“å°†çº¿æ€§å›å½’è¾“å‡ºä½œä¸ºæ¦‚ç‡
* but it's a mistake because the output can be negative and greater than 1, whereas probability can not. 
* ä½†è¿™æ˜¯ä¸€ä¸ªé”™è¯¯ï¼Œå› ä¸ºè¾“å‡ºå¯ä»¥æ˜¯è´Ÿçš„å¹¶ä¸”å¤§äº1ï¼Œè€Œæ¦‚ç‡ä¸èƒ½ã€‚

![image-20221223124528006](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223124528006.png)

![image-20221223124539077](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223124539077.png)

**å‡è®¾ä»¥ä¸‹ä¾‹å­ï¼š**

![image-20221223132327710](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223132327710.png)

æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹çš„å…¬å¼

![image-20221223124601639](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223124601639.png)

* sigmoid prediction  Så½¢é¢„æµ‹
* Squash the output of the linear function å‹ç¼©çº¿æ€§å‡½æ•°çš„è¾“å‡º
* ![image-20221223131610312](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223131610312.png)

* New optimization objective ä¸€ç§æ–°çš„ä¼˜åŒ–å¯¹è±¡
* ![image-20221223131942394](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223131942394.png)





<font size=5>Linear classification: logistic regression </font>

![image-20221223132007412](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223132007412.png)

![image-20221223132430645](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223132430645.png)

* å…³äºä¸Šè¿°å…¬å¼çš„æ¨å¯¼è¿‡ç¨‹
* ![image-20230122112124210](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122112124210.png)

#### 5.2 æœ€å¤§ä¼¼ç„¶ä¼°è®¡

![image-20221223132836842](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223132836842.png)

![image-20221223132850334](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223132850334.png)

![image-20221223135113584](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223135113584.png)

* è¾¹ç•ŒBounded
* å¯¹ç§°Symmetric
* æ¢¯åº¦Gradient

![image-20221223135227018](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223135227018.png)



## Lecture13 Gradient Descent  æ¢¯åº¦ä¸‹é™

https://blog.csdn.net/qq_41800366/article/details/86583789?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167183047716782427420351%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167183047716782427420351&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-86583789-null-null.142

-- æ¢¯åº¦ä¸‹é™

æ¢¯åº¦ä¸‹é™çš„åŸºæœ¬è¿‡ç¨‹å°±å’Œä¸‹å±±çš„åœºæ™¯å¾ˆç±»ä¼¼ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¯å¾®åˆ†çš„å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°å°±ä»£è¡¨ç€ä¸€åº§å±±ã€‚æˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°è¿™ä¸ªå‡½æ•°çš„æœ€å°å€¼ï¼Œä¹Ÿå°±æ˜¯å±±åº•ã€‚æ ¹æ®ä¹‹å‰çš„åœºæ™¯å‡è®¾ï¼Œæœ€å¿«çš„ä¸‹å±±çš„æ–¹å¼å°±æ˜¯æ‰¾åˆ°å½“å‰ä½ç½®æœ€é™¡å³­çš„æ–¹å‘ï¼Œç„¶åæ²¿ç€æ­¤æ–¹å‘å‘ä¸‹èµ°ï¼Œ**å¯¹åº”åˆ°å‡½æ•°ä¸­ï¼Œå°±æ˜¯æ‰¾åˆ°ç»™å®šç‚¹çš„æ¢¯åº¦ ï¼Œç„¶åæœç€æ¢¯åº¦ç›¸åçš„æ–¹å‘ï¼Œå°±èƒ½è®©å‡½æ•°å€¼ä¸‹é™çš„æœ€å¿«**ï¼å› ä¸ºæ¢¯åº¦çš„æ–¹å‘å°±æ˜¯å‡½æ•°ä¹‹å˜åŒ–æœ€å¿«çš„æ–¹å‘(åœ¨åé¢ä¼šè¯¦ç»†è§£é‡Š)
æ‰€ä»¥ï¼Œæˆ‘ä»¬é‡å¤åˆ©ç”¨è¿™ä¸ªæ–¹æ³•ï¼Œåå¤æ±‚å–æ¢¯åº¦ï¼Œæœ€åå°±èƒ½åˆ°è¾¾**å±€éƒ¨çš„æœ€å°å€¼**ï¼Œè¿™å°±ç±»ä¼¼äºæˆ‘ä»¬ä¸‹å±±çš„è¿‡ç¨‹ã€‚è€Œæ±‚å–æ¢¯åº¦å°±ç¡®å®šäº†æœ€é™¡å³­çš„æ–¹å‘ï¼Œä¹Ÿå°±æ˜¯åœºæ™¯ä¸­æµ‹é‡æ–¹å‘çš„æ‰‹æ®µã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆæ¢¯åº¦çš„æ–¹å‘å°±æ˜¯æœ€é™¡å³­çš„æ–¹å‘å‘¢ï¼Ÿæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»å¾®åˆ†å¼€å§‹è®²èµ·ï¼š

### 1. Derivative å¯¼æ•°

* Suppose we have function y=f (x), x, y real numbers 
* å‡è®¾æœ‰å‡½æ•°y=f (x) x, yä¸ªå®æ•°
  * Derivative of function denoted: fâ€™(x) or as dy/dx 
  * å‡½æ•°çš„å¯¼æ•°è®°ä¸ºf ' (x)æˆ–dy/dx
    * Derivative fâ€™(x) gives the slope of f (x) at point x 
    * f ' (x)çš„å¯¼æ•°ç»™å‡ºäº†f (x)åœ¨xç‚¹çš„æ–œç‡
    * It specifies how to scale a small change in input to obtain a corresponding change in the output: 
    * å®ƒæŒ‡å®šäº†å¦‚ä½•ç¼©æ”¾è¾“å…¥ä¸­çš„å°å˜åŒ–ä»¥è·å¾—è¾“å‡ºä¸­çš„ç›¸åº”å˜åŒ–:

![image-20221223205053069](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223205053069.png)

* It tells how you make a small change in input to make a small improvement in y 
* å®ƒå‘Šè¯‰ä½ å¦‚ä½•å¯¹è¾“å…¥åšä¸€ä¸ªå°çš„æ”¹å˜æ¥ä½¿yæœ‰ä¸€ä¸ªå°çš„æé«˜

![image-20221223205330622](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223205330622.png)

#### 1.1 Calculus in optimization æœ€ä¼˜åŒ–ä¸­çš„å¾®ç§¯åˆ†

![image-20221223205616018](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223205616018.png)

* Therefore, we can reduce f(x) by moving x in small steps with opposite sign of derivative 
* å› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡ç§»åŠ¨ä¸å¯¼æ•°æ–¹å‘æƒ³æ³•çš„æå°çš„xæ¥å‡å°‘f(x)
* ä¸å°±æ˜¯ç§»åŠ¨ä¸å‡½æ•°æ–¹å‘ç›¸åçš„xæ¥å‡å°‘f(x)å—

![image-20221223210122024](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223210122024.png)

#### 1.2 Gradient Descent Illustrated  æ¢¯åº¦ä¸‹é™å›¾è§£

![image-20221223212100803](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223212100803.png)

#### 1.3 Stationary points, Local Optima  é™æ­¢ç‚¹ï¼Œ å±€éƒ¨ä¼˜åŒ–

* When  f`(x)=0 derivative provides no information about direction of move 

  å½“å¯¼æ•°f`(x)=0çš„æ—¶å€™ï¼Œä¸æä¾›ä»»ä½•ç§»åŠ¨æ–¹å‘çš„ä¿¡æ¯

* Points where  f`(x)=0 are known as stationary or critical points 

*  **f`(x)=0çš„ç‚¹ä»»åŠ¡æ˜¯é™æ­¢ç‚¹æˆ–è€…å…³é”®ç‚¹**

  * **Local minimum/maximum**: a point where f(x) lower/ higher than all its neighbors
  * å±€éƒ¨æœ€å°å€¼/æœ€å¤§å€¼:f(x)ä½äº/é«˜äºå…¶æ‰€æœ‰é‚»åŸŸçš„ç‚¹
  * **Saddle Points**: neither maxima nor minima 
  * éç‚¹:æ—¢ä¸æ˜¯æå¤§å€¼ä¹Ÿä¸æ˜¯æå°å€¼

![image-20221223212854945](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223212854945.png)

#### 1.4 Presence of multiple minima å­˜åœ¨å¤šä¸ªæå°å€¼

* Optimization algorithms may fail to find global minimum 
* ä¼˜åŒ–ç®—æ³•å¯èƒ½æ— æ³•æ‰¾åˆ°å…¨å±€æœ€å°å€¼
* Generally accept such solutions 
* ä¸€èˆ¬æ¥å—è¿™ç§è§£å†³æ–¹æ³•

![image-20221223213059133](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223213059133.png)

### 2. Gradient æ¢¯åº¦

#### 2.1. Minimizing with multiple dimensional inputs ç”¨å¤šç»´è¾“å…¥æœ€å°åŒ–

* We often minimize functions with multiple-dimensional inputs
* æˆ‘ä»¬ç»å¸¸æœ€å°åŒ–å…·æœ‰å¤šç»´è¾“å…¥çš„å‡½æ•°
* ![image-20221223213520745](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223213520745.png)
* For minimization to make sense there must still be only one (scalar) output 
* ä¸ºäº†ä½¿æœ€å°åŒ–æœ‰æ„ä¹‰ï¼Œä»ç„¶å¿…é¡»åªæœ‰ä¸€ä¸ª(æ ‡é‡)è¾“å‡º

![image-20221223213641338](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223213641338.png)

#### 2.2 Functions with multiple inputs å¤šé‡è¾“å…¥å‡½æ•°

* Partial derivatives  åå¯¼æ•°
* è§£é‡Šä¸€ä¸‹ï¼Œå› ä¸ºå¾€å¾€æœ‰å¤šä¸ªå˜é‡ï¼Œåå¯¼æ•°æ˜¯å…¶ä¸­ä¸€ä¸ªå˜é‡çš„å¯¼æ•°
* ![image-20221223214232406](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223214232406.png)
* measures how f changes as only variable xiincreases at point x 
* æµ‹é‡fåœ¨ç‚¹xå¤„åªæœ‰å˜é‡xiå¢åŠ æ—¶çš„å˜åŒ–
* **è¿™ä¸å°±æ˜¯é«˜ä¸­å­¦çš„æ±‚å¯¼å—ã€‚ã€‚ã€‚**
* Gradient generalizes notion of derivative where derivative is wrt a vector
*  **æ¢¯åº¦æ˜¯åŒ…å«æ‰€æœ‰åå¯¼æ•°çš„å‘é‡**

![image-20221223214544483](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223214544483.png)

![image-20221223214641319](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223214641319.png)

* **Gradient is vector containing all of the partial derivatives denoted** 
* **æ¢¯åº¦æ˜¯åŒ…å«æ‰€æœ‰åå¯¼æ•°çš„å‘é‡**

![image-20221223214738283](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223214738283.png)

* Element iof the gradient is the partial derivative of f wrt xi
* æ¢¯åº¦çš„å…ƒç´ iæ˜¯f wrt xçš„åå¯¼æ•°
* Critical points are where every element of the gradient is equal to zero 
* ä¸´ç•Œç‚¹æ˜¯æŒ‡æ¢¯åº¦ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½ç­‰äºé›¶

![image-20221223214923078](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223214923078.png)

![image-20221223214930317](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223214930317.png)

![image-20221223214948771](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223214948771.png)



### 3. Directional Derivative æ–¹å‘å¯¼æ•°

https://blog.csdn.net/The_lastest/article/details/77898799?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167183225716782425684593%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167183225716782425684593&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-77898799-null-null.142

- æ–¹å‘å¯¼æ•°æ˜¯åœ¨ä¸‰ç»´ç©ºé—´ä¸­ï¼Œæ›²é¢ä¸ŠæŸä¸€ç‚¹æ²¿ç€ä»»ä¸€æ–¹å‘çš„å˜åŒ–ç‡ï¼ˆå¡åº¦ï¼‰ï¼›

![image-20221223215040032](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223215040032.png)

![image-20221223220126162](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223220126162.png)

![image-20221223220352328](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223220352328.png)

* æ¢å¥è¯è¯´ï¼Œæ¢¯åº¦ç›´æ¥æŒ‡å‘ä¸Šå¡ï¼Œè´Ÿæ¢¯åº¦ç›´æ¥æŒ‡å‘ä¸‹å¡

### 4. Method of Gradient Descent æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•

* The gradient points directly uphill, and the negative gradient points directly downhill 
* æ¢¯åº¦ç›´æ¥æŒ‡å‘ä¸Šå¡ï¼Œè´Ÿæ¢¯åº¦ç›´æ¥æŒ‡å‘ä¸‹å¡
* Thus we can decrease f  by moving in the direction of the negative gradient 
* å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘è´Ÿæ¢¯åº¦æ–¹å‘ç§»åŠ¨æ¥å‡å°f
  * This is known as the method of **steepest descent or gradient descent** 
  * è¿™è¢«ç§°ä¸ºè¿™è¢«ç§°ä¸º**æœ€é™¡ä¸‹é™æ³•æˆ–æ¢¯åº¦ä¸‹é™æ³•**
* Steepest descent proposes a new point 
* æœ€é™¡çš„ä¸‹é™æå‡ºäº†ä¸€ä¸ªæ–°çš„ç‚¹
* ![image-20221223220815825](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223220815825.png)
  * ![image-20221223220838717](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223220838717.png)

#### 4.1 é€‰æ‹©å­¦ä¹ ç‡

![image-20221223220922810](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223220922810.png)

### 5. Example: Gradient Descent on Linear Regression  

![image-20221223221013131](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223221013131.png)

![image-20221223221132301](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223221132301.png)



### 6. Linear Regression: Analytical Solution



#### 6.1 Convergence of Steepest Descent  æœ€é™¡ä¸‹é™æ”¶æ•›

* Steepest descent converges when every element of the gradient is zero 
* å½“æ¢¯åº¦çš„æ¯ä¸ªå…ƒç´ éƒ½ä¸ºé›¶æ—¶ï¼Œæœ€é™¡ä¸‹é™æ”¶æ•›
* å°±æ˜¯åå¯¼æ•°ä¸º0
  * In practice, very close to zero 
  * å®é™…ä¸Šï¼Œéå¸¸æ¥è¿‘äºé›¶
* We may be able to avoid iterative algorithm and jump to the critical point by solving the following equation for x
* æˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚è§£ä¸‹é¢çš„xæ–¹ç¨‹æ¥é¿å…è¿­ä»£ç®—æ³•ï¼Œä»è€Œè·³åˆ°ä¸´ç•Œç‚¹
* ![image-20221223225014815](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223225014815.png)

![image-20221223225110824](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223225110824.png)

* Algebraic view of the minimizer
* æœ€å°åŒ–å™¨çš„ä»£æ•°è§†å›¾
* If ğ‘‹is invertible, just solve ğ‘‹ğ‘¤= ğ‘¦and get ğ‘¤= ğ‘‹âˆ’1ğ‘¦
* å¦‚æœğ‘‹iså¯é€†çš„,åªæ˜¯è§£å†³ğ‘‹ğ‘¤=ğ‘¦ and å¾—åˆ°ğ‘¤=ğ‘‹âˆ’1ğ‘¦
* But typically ğ‘‹is a tall matrix 
* ä½†é€šå¸¸ğ‘‹æ˜¯ä¸€ä¸ªé«˜å¤§çš„çŸ©é˜µ

![image-20221223225318506](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223225318506.png)



### 7. Generalization to discrete spaces  æ¨å¹¿åˆ°ç¦»æ•£ç©ºé—´

* Gradient descent is limited to continuous spaces 
* æ¢¯åº¦ä¸‹é™åªé™äºè¿ç»­ç©ºé—´
* **å’Œæ±‚å¯¼æ˜¯åœ¨è¿ç»­å‡½æ•°ä¸Šæ±‚å¯¼æ˜¯ä¸€ä¸ªé“ç†**
* Concept of repeatedly making the best small move can be generalized to discrete spaces 
* åå¤åšå‡ºæœ€ä½³å°åŠ¨ä½œçš„æ¦‚å¿µå¯ä»¥æ¨å¹¿åˆ°ç¦»æ•£ç©ºé—´
* Ascending an objective function of discrete parameters is called hill climbing
* ä¸Šå‡ä¸€ä¸ªç¦»æ•£å‚æ•°çš„ç›®æ ‡å‡½æ•°ç§°ä¸ºçˆ¬å¡

![image-20221223225734130](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221223225734130.png)

## Lecture14 Naive-Bayes

![image-20230122135646717](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122135646717.png)

![image-20230122135656343](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122135656343.png)

### 1. Recall: MAP Queries (Most Probable Explanation) æœ€å¯èƒ½çš„è§£é‡Šå’Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡

https://www.bilibili.com/video/BV1Hb4y1m7rE/?spm_id_from=333.337.search-card.all.click&vd_source=9d046a6203dde4f5f1794bf980388e63

* Finding a high probability assignment to some subset of variables 
* å¯»æ‰¾å˜é‡å­é›†çš„é«˜æ¦‚ç‡èµ‹å€¼
* Most likely assignment to all non-evidence variables W
* æœ€æœ‰å¯èƒ½èµ‹å€¼ç»™æ‰€æœ‰æ— è¯æ®å˜é‡W
* ![image-20230122140040841](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122140040841.png)

![image-20230122143212681](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122143212681.png)



### 2. NaÃ¯ve Bayes Algorithm è´å¶æ–¯ç®—æ³•

https://blog.csdn.net/weixin_45962068/article/details/118148350?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167439039116782428634527%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167439039116782428634527&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-118148350-null-null.142^

![image-20230122143429634](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122143429634.png)

#### 2.1 Reducing the number of parameters to estimate  å‡å°‘éœ€è¦ä¼°è®¡çš„å‚æ•°æ•°é‡

* æˆ‘ä»¬æœ‰ä¸€ä¸ªP(Y|X), Yæ˜¯æ˜¯å¦å¯Œæœ‰ï¼Œç‰¹å¾æœ‰ä¸¤ä¸ªæ˜¯ æ€§åˆ« genderå’Œ hours_worked

![image-20230122144202724](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122144202724.png)

![image-20230122144429430](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122144429430.png)

![image-20230122144708572](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122144708572.png)

![image-20230122145444982](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122145444982.png)



#### 2.2 NaÃ¯ve Bayes æœ´ç´ è´å¶æ–¯

![image-20230122145855160](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122145855160.png)

![image-20230122150054721](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122150054721.png)

#### 2.3 Recap: Conditional independence 

![image-20230122150259389](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122150259389.png)

* A is conditionally independent of B given C, if the probability distribution governing A is independent of the value of B, given the value of C 
* å¦‚æœAçš„æ¦‚ç‡åˆ†å¸ƒåœ¨ç»™å®šCçš„æƒ…å†µä¸‹ä¸Bçš„å€¼æ— å…³ï¼Œåˆ™Aåœ¨ç»™å®šCçš„æƒ…å†µä¸‹ä¸Bçš„å€¼æ— å…³

![image-20230122150502883](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122150502883.png)

#### 2.4 AssumpRon for NaÃ¯ve Bayes

![image-20230122151044388](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122151044388.png)

![image-20230122151547497](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122151547497.png)

![image-20230122151557253](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122151557253.png)



#### 2.5 NaÃ¯ve Bayes Algorithm

![image-20230122152415626](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122152415626.png)

![image-20230122152421788](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122152421788.png)

![image-20230122152432291](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122152432291.png)





## Lecture15   Deep Learning

### 1. Deep Learning in Different fileds

#### 1.1 ComputerVision

<font size=5>Some Exampleï¼š</font>

* Object and activity recognition  ç‰©ä½“å’Œè¡ŒåŠ¨çš„è¯†åˆ«
  * https://www.youtube.com/watch?v=qrzQ_AB1DZk
* Object detection and segmentation å¯¹è±¡çš„æ£€æµ‹å’Œåˆ†å‰²
  * https://www.youtube.com/watch?v=CxanE_W46ts
* Image captioning and Q&A  å›¾åƒå­—å¹•å’ŒQ&A
  * https://www.youtube.com/watch?v=8BFzu9m52sc

<font size=5>Why should we be impressed? </font>

* Vision is ultra challenging. è§†è§‰æ˜¯æå…·æŒ‘æˆ˜çš„

  ![image-20221114164842854](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114164842854.png)

* Visual object variations è§†è§‰å¯¹è±¡æ˜¯å¤šå˜çš„

  ![image-20221114164950691](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114164950691.png)

* Semantic object variations è¯­ä¹‰å¯¹è±¡å¤šå˜

  ![image-20221114165329787](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114165329787.png)



#### 1.2  Natural Language Processing (NLP)è‡ªç„¶è¯­è¨€å¤„ç† and Speech

* Word and sentence representations. è¯­è¨€å’Œå¥å­ä»£è¡¨

  ![image-20221114171135914](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114171135914.png)

![image-20221114171150134](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114171150134.png)

* Speech recognition and Machine translation è¯­éŸ³è¯†åˆ«ä¸æœºå™¨ç¿»è¯‘

  ![image-20221114171324709](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114171324709.png)

<font size=5>Why should we be impressed? </font>

* NLP is an extremely complex task. NLPæ˜¯ä¸€ä¸ªç‰¹åˆ«å¤æ‚çš„ä»»åŠ¡
  * synonymy åŒä¹‰è¯(â€œchairâ€, â€œstoolâ€ or â€œbeautifulâ€, â€œhandsomeâ€) 
  * ambiguity æ¨¡æ£±ä¸¤å¯(â€œI made her duckâ€, â€œCut to the chaseâ€) 
* NLP is very high dimensional. NLPéå¸¸é«˜ç»´
  * assuming 150K english words, we need to learn 150K classifiers  å‡è®¾æœ‰15Kä¸ªè‹±è¯­å•è¯ï¼Œæˆ‘ä»¬å°±éœ€è¦å­¦ä¹ 150Kä¸ªåˆ†ç±»å™¨ã€‚
* Beating NLP feels the closest to achieving true AI æœ€æ¥è¿‘æ‰“è´¥NPLçš„æ˜¯çœŸæ­£çš„AI
  * although true AI probably goes beyond NLP, Vision, Robotics, ... alone. å³ä½¿çœŸæ­£çš„AIå·²ç»è¶…è¿‡äº†ä¸Šè¿°å¯¹è±¡

#### 1.3 Robotics and AI

* Self-Driving cars
  * https://www.youtube.com/watch?v=-96BEoXJMs0
* Drones and robots 
  * https://www.youtube.com/watch?v=2hGngG64dNM
* Game AI 
  * https://www.youtube.com/watch?v=V1eYniJ0Rnk

<font size=5>Why should we be impressed? </font>

* Typically robotics are considered in controlled environments ä¼ ç»Ÿçš„æœºå™¨äººå¸¸å¸¸åœ¨å—æ§åˆ¶ç¯å¢ƒä¸­è¢«è€ƒè™‘ã€‚

  * Laboratory settings, Predictable positions, Standardized tasks (like in factory robots)  å®éªŒå®¤è®¾ç½®ï¼Œå¯é¢„æµ‹çš„ä½ç½®ï¼Œæ ‡å‡†åŒ–ä»»åŠ¡(æ¯”å¦‚å·¥å‚æœºå™¨äºº)

* What about real life situations? ä»€ä¹ˆæ˜¯çœŸæ˜¯çš„ç”Ÿæ´»æƒ…å†µï¼Ÿ

  * Environments constantly change, new tasks need to be learnt without guidance, unexpected factors must be dealt with . ç¯å¢ƒæ˜¯æŒç»­å˜åŒ–çš„ï¼Œæ–°çš„ä»»åŠ¡éœ€è¦åœ¨æ²¡æœ‰æŒ‡å¯¼çš„æƒ…å†µä¸‹è¢«å­¦ä¹ ï¼Œå¹¶è§£å†³æ— æ³•é¢„æµ‹çš„å› ç´ 

* Game AI æ¸¸æˆAI

  

#### 1.4 Music and the arts

* Imitating famous painters æ¨¡ä»¿è‘—åç”»å®¶

  ![image-20221114173025467](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114173025467.png)

<font size=5>Why should we be impressed? </font>

* Music, painting, etc. are tasks that are uniquely human. éŸ³ä¹ï¼Œç»˜ç”»ç­‰æ˜¯ç‹¬å±äºäººç±»çš„å·¥ä½œ
  * Difficult to model. éš¾ä»¥å»ºæ¨¡
  * Even more difficult to evaluate (if not impossible)  éš¾ä»¥è¯„ä¼°
* If machines can generate novel pieces even remotely resembling art, 
  they must have understood something about â€œbeautyâ€, â€œharmonyâ€, 
  etc.  å¦‚æœæœºå™¨äººå¯ä»¥ç”Ÿæˆæ–°å¥‡çš„å°è¯´ä½œå“ç”šè‡³æ¥è¿‘è‰ºæœ¯ï¼Œ é‚£ä¹ˆä»–ä»¬å°±ä¸€å®šç†è§£ç¾æˆ–å’Œè°ã€‚
* Have they really learned to generate new art, however?
  * We do not know.



### 2. A brief history of Neural Networks

  ![image-20221114212011036](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114212011036.png)



#### 2.1 Perceptrons ((æ„ŸçŸ¥å™¨)

* Rosenblatt proposed a machine for binary classifications. 

* **Rosenblatt æå‡ºäº†ä¸€ç§äºŒè¿›åˆ¶çš„åˆ†ç±»æœºå™¨**

* Main Idea:

  * One weight ğ‘¤ğ‘– per input ğ‘¥ğ‘–  æ¯ä¸€ä¸ªxéƒ½æœ‰ä¸€ä¸ªæƒé‡w
  * Multiply weights with respective inputs and add bias ğ‘¥0=+1 æƒé‡ä¸å„è‡ªçš„inputç›¸ä¹˜åœ¨åŠ ä¸Šåå·®å€¼x0
  * If result larger than threshold return 1, otherwise 0 å¦‚æœç»“æœå¤§äºé˜ˆå€¼è¿”å›1ï¼Œå¦åˆ™è¿”å›0

  ![image-20221114213216540](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114213216540.png)

##### 2.1.1 Training a perceptron è®­ç»ƒæ„ŸçŸ¥å™¨

* Rosenblattâ€™s innovation was mainly the learning algorithm for 
  perceptrons. ä»–çš„åˆ›æ–°ä¸»è¦æ˜¯æ„ŸçŸ¥å™¨çš„å­¦ä¹ ç®—æ³•

* Learning algorithm 

  * Initialize weights randomly éšæœºåˆå§‹åŒ–æƒé‡
  * Take one sample ğ‘¥ğ‘–and predict ğ‘¦ğ‘– å–ä¸€ä¸ªæ ·æœ¬xiå’Œé¢„æµ‹å€¼yi
  * For erroneous predictions update weights å¯¹äºé”™è¯¯çš„é¢„æµ‹å€¼ï¼Œæ›´æ–°æƒé‡ã€‚
    * ![image-20221114213714013](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114213714013.png)
      * Î·is learning rate;  set to value << 1
      * <font color='red'>å¾ˆæ˜æ˜¾ï¼ŒÎ·å¤§äº0å°äº1ï¼Œæƒå€¼å–å†³äºuçš„å·®å€¼çš„æ­£è´Ÿã€‚</font>
    * Repeat until no errors are made 

  ![image-20221114213928488](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114213928488.png)

##### 2.1.2 Representational power of perceptrons æ„ŸçŸ¥å™¨çš„è¡¨è¾¾æ–¹å¼

* in previous example, feature space was 2D so decision boundary was 
  a line.  åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œç‰¹å¾ç©ºé—´æ˜¯2Dçš„ï¼Œæ‰€ä»¥å†³ç­–è¾¹ç•Œæ˜¯ä¸€æ¡çº¿
* in higher dimensions, decision boundary is a hyperplane.  åœ¨æ›´é«˜çš„ç»´åº¦ä¸Šï¼Œå†³ç­–è¾¹ç•Œæ˜¯ä¸€ä¸ªè¶…å¹³é¢

![image-20221114214531281](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114214531281.png)

##### 2.1.3 From a perceptron to a neural network ä»æ„ŸçŸ¥å™¨åˆ°ç¥ç»ç½‘ç»œ

* One perceptron = one decision ä¸€ä¸ªæ„ŸçŸ¥å™¨=ä¸€ä¸ªå†³å®š

* What about multiple decisions?  æœ‰ä»€ä¹ˆéœ€è¦å¤æ•°å†³ç­–

  * Egï¼šæ•°å­—åˆ†ç±»å™¨

* Stack as many outputs as the possible outcomes into a layer. å°†å°½å¯èƒ½å¤šçš„è¾“å‡ºå åˆ°åŒä¸€å±‚ä¸­ã€‚

  * Neural network 

* Use one layer as input to the next layer. ä½¿ç”¨ä¸€ä¸ªå›¾å±‚ä½œä¸ºä¸‹ä¸€ä¸ªå›¾å±‚çš„è¾“å…¥

  * Multi-layer perceptron (MLP)  å¤šå±‚æ„ŸçŸ¥å™¨

  ![image-20221114215920220](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114215920220.png)

##### 2.1.4 Some linearly separable functions ä¸€äº›çº¿æ€§å¯åˆ†å‡½æ•°ã€

![image-20221114220128815](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114220128815.png)

<font color='red'>And å’ŒOræ˜¯çº¿æ€§å¯åˆ†å‡½æ•°</font>

##### 2.1.5 XOR is not linearly separable XORï¼ˆä¸”æˆ–éï¼‰ä½†ä¸æ˜¯çº¿æ€§å¯åˆ†

![image-20221114220414572](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114220414572.png)

ä¸éµå®ˆä¸”æˆ–éçš„å…³ç³»

##### 2.1.6 XOR & Multi-layer Perceptrons  XOR&å¤šå±‚æ„ŸçŸ¥å™¨

<font color=red>**However, the exclusive or (XOR) cannot be solved by perceptronsã€‚ ç„¶è€Œï¼Œæ’ä»–æˆ–XORä¸å¯ä»¥ä½¿ç”¨æ„ŸçŸ¥å™¨æ±‚è§£ã€‚**</font >

![image-20221114220727728](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114220727728.png)

å¦‚å›¾ï¼š

![image-20221114220918961](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114220918961.png)

æ ¹æ®ç»“æœï¼Œåªæœ‰01å’Œ10å¯ä»¥è¢«é€‰ä¸­ã€‚

##### 2.1.7 Minsky & Multi-layer perceptrons

* Interestingly, Minksyï¼ˆä¸Šé¢æåˆ°çš„äººï¼Œåœ¨å›¾ç‰‡é‡Œï¼‰ never said XOR cannot be solved by neural networks. ä½†æ˜¯ä»–å¯æ²¡æœ‰è¯´è¿‡è¯ç”¨ç¥ç»ç½‘ç»œä¸èƒ½è§£å†³XOR
  * **Only that XOR cannot be solved with 1 layer perceptronsï¼ˆåªæ˜¯å¼‚æˆ–ä¸èƒ½ç”¨ä¸€å±‚æ„ŸçŸ¥å™¨æ±‚è§£ï¼‰**
* Multi-layer perceptrons can solve XOR 
  * 9 years earlier Minsky built such a multi-layer perceptron 9å¹´å‰ï¼Œæ˜æ–¯åŸºå»ºç«‹äº†è¿™æ ·ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨

![image-20221114222250090](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114222250090.png)

<font color=red>ä¸Šå›¾ä¸­çš„ä¸‹é¢è¿™ä¸ªå›¾ç¡®å®å¯ä»¥</font>

é‚£ä¹ˆï¼Œæ–°çš„é—®é¢˜ï¼š å¦‚ä½•è®­ç»ƒå¤šå±‚æ„ŸçŸ¥æ¨¡å‹ï¼Ÿ

* **Rosenblattâ€™s algorithm not applicable, as it expects to know the desired target.** 
* **Rosenblattçš„ç®—æ³•å¹¶ä¸é€‚ç”¨ï¼Œå› ä¸ºå®ƒæœŸæœ›çŸ¥é“éœ€è¦çš„ç›®æ ‡**
  * For hidden layers we cannot know the desired target 
  * å¯¹äºéšè—å€¼ï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“æœŸå¾…çš„ç›®æ ‡ã€‚

![image-20221114222907012](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114222907012.png)

![image-20221114222916418](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114222916418.png)



#### 2.2 The â€œAI winterâ€ despite notable successes 

![image-20221114223110377](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221114223110377.png)



##### 2.2.1  The first â€œAI winterâ€ ç¬¬ä¸€æ¬¡AIå¯’å†¬

![image-20230122154231187](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122154231187.png)

![image-20230122154250653](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122154250653.png)

##### 2.2.2 The thaw of the AI Winter AIå†¬å¤©çš„è§£å†»

![image-20230122154802642](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122154802642.png)

![image-20230122154856315](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122154856315.png)

##### 2.2.3 Deep Learning

![image-20230122160154120](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122160154120.png)

##### Deep Learning Golden Era

![image-20230122160331743](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122160331743.png)

![image-20230122160411293](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122160411293.png)

![image-20230122160435440](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122160435440.png)





## Leture 16 Deep Learning: Functional View and Features
![image-20230122164650783](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122164650783.png)



### 1. Functional View of DNNS



#### 1.1 What is a nested function?

![image-20230122164802000](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122164802000.png)



#### 1.2 Functional View of DNNs DNNsåŠŸèƒ½è§†å›¾

![image-20230122164904178](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122164904178.png)

##### 1.2.1 Illustration of DNN 

![image-20230122165101868](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122165101868.png)

![image-20230122165140278](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122165140278.png)

![image-20230122165223994](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122165223994.png)



### 2. LearningRepresentations & Features

#### 2.1 Raw digital representation --Image and Video

![image-20230122165444815](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122165444815.png)

![image-20230122165450869](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122165450869.png)

#### 2.2 Learning Representations & Features å­¦ä¹ è¡¨ç°ä¸ç‰¹å¾

![image-20230122170625605](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122170625605.png)

![image-20230122170709500](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122170709500.png)

![image-20230122170734096](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122170734096.png)

#### 2.3 Good Features å¥½çš„ç‰¹å¾

![image-20230122170918195](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122170918195.png)



#### 2.4 Data Mainfold æ•°æ®æµå½¢

![image-20230122171137621](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122171137621.png)

![image-20230122171220072](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122171220072.png)

#### 2.5 The Digits Mainfolds

![image-20230122171403747](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122171403747.png)



#### 2.6 End-to-end learning of feature hierarchies  ç‰¹å¾å±‚æ¬¡ç»“æ„çš„ç«¯åˆ°ç«¯å­¦ä¹ 

![image-20230122171612916](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122171612916.png)

![image-20230122171624170](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122171624170.png)

![image-20230122171646454](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122171646454.png)

![image-20230122171700878](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122171700878.png)



## Lecture17 Adversarial Attack on ML models







## Lecture18 DL-Forward-and-Backward-Computation

### 1. Forward and Backward computation

#### 1.1 Forward Computations æ­£å‘è®¡ç®—

* Collect annotated data 

* æ”¶é›†æœªæ³¨é‡Šçš„æ•°æ®

* Define model and initialize randomly  

* å®šä¹‰åˆå§‹æ¨¡å‹å¹¶éšæœºåŒ–

* Predict based on current model 

* åœ¨ç°æœ‰æ¨¡å‹ä¸Šè¿›è¡ŒçŒœæµ‹

  * In neural network jargon **â€œforward propagationâ€** 
  * åœ¨ç¥ç»ç½‘ç»œæœ¯è¯­ä¸­è¢«ç§°ä¸º**æ­£å‘ä¼ æ’­**

* Evaluate Predictions

  ![image-20221115091433434](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115091433434.png)

#### 1.2 Backward Computations

* Collect gradient data  æ”¶é›†æ¢¯åº¦æ•°æ®

* Define model and initialize randomly å®šä¹‰æ¨¡å‹å¹¶è¿›è¡ŒéšæœºåŒ–

* Predict based on current model åœ¨ç°æœ‰æ¨¡å‹ä¸Šè¿›è¡ŒçŒœæµ‹

  * In neural network jargon **â€œbackpropagationâ€** åœ¨ç¥ç»ç½‘ç»œæœ¯è¯­ä¸­è¢«ç§°ä¸º**åå‘ä¼ æ’­**

* Evaluate Predictions

* è¯„ä¼°é¢„æµ‹

  ![image-20221115091757492](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115091757492.png)

  ![image-20221115135026884](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115135026884.png)

  ![image-20221115135036850](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115135036850.png)

  ![image-20221115135106052](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115135106052.png)

  ![image-20221115135127453](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115135127453.png)

<font color=red>å°±æ˜¯ä¸€ä¸ªæ­£æ¨ï¼Œä¸€ä¸ªåæ¨</font>

#### 1.3 Recall Training Objective

![image-20221115091910972](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115091910972.png)

#### 1.4 Forward Computation

Exampleï¼š

![image-20221115092041556](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115092041556.png)

* Training Data (a signle sample)

  * given inputs 0.05 and 0.10, we want the neural network to output 0.01 and 0.99.  è¾“å…¥å€¼ä¸º0.05å’Œ0.1 æœŸæœ›å€¼ä¸º0.01å’Œ0.99
  * â€¢net be the value before activation function å–æ¿€æ´»å‡½æ•°å‰çš„å€¼
  * â€¢outbe the value after activation function è¾“å‡ºæ¿€æ´»å‡½æ•°åçš„å€¼
  * â€¢Activation function is sigmoid function æ¿€æ´»å‡½æ•°æ˜¯sigmoidå‡½æ•°

  ![image-20221115092706145](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115092706145.png)

  ![image-20221115092720824](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115092720824.png)

  ![image-20221115092755397](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115092755397.png)

  ![image-20221115092921826](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115092921826.png)

* <font color=red>bæ˜¯åå·®å€¼</font>

  

### 2. Back-Propogation and Chain Rule

https://blog.csdn.net/qq_39215715/article/details/108527831?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166859522316782412582214%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166859522316782412582214&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-108527831-null-null.142

-- Back-Propogation ç®—æ³•

#### 2.1 Back-Propogation åå‘ä¼ æ’­

æœ¬è´¨ä¸Šæ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•æ±‚æå€¼çš„è¿‡ç¨‹ã€‚

##### 2.1.1 Recall: Minimizing with multiple dimensional inputs æœ€å°åŒ–å¤šç»´è¾“å…¥

* We often minimize functions with multiple-dimensional inputs. æˆ‘ä»¬ç»å¸¸æ€§çš„æœ€å°åŒ–å…·æœ‰å¤šç»´è¾“å…¥çš„å‡½æ•°

  ![image-20221115142037139](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115142037139.png)

* For minimization to make sense there must still be only one (scalar) output.  **ä¸ºäº†ä½¿æœ€å°åŒ–æœ‰æ„ä¹‰ï¼Œä»ç„¶å¿…é¡»åªæœ‰ä¸€ä¸ª(æ ‡é‡)è¾“å‡º**



##### 2.1.2 Functions with multiple inputs  å¤šé‡è¾“å…¥å‡½æ•°

* Partial derivatives åå¯¼æ•°

  ![image-20221115223833037](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115223833037.png)

* measures how f changes as only variable xi increases at point x æµ‹é‡fä¼šå¦‚ä½•æ”¹å˜ï¼Œä»…å½“xiåœ¨ç‚¹xä¸Šå¢åŠ æ—¶ã€‚

  ![image-20221115224539338](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115224539338.png)

* **<font color=red>Gradient generalizes notion of derivative where derivative is wrt a vector æ¢¯åº¦ä¸€èˆ¬åŒ–å¯¼æ•°çš„å¯¼æ•°,å¯¼æ•°æ˜¯wrtå‘é‡</font>**

* Gradient is vector containing all of the partial derivatives denoted æ¢¯åº¦æ˜¯åŒ…å«äº†æ‰€æœ‰åå¯¼æ•°çš„å‘é‡ã€‚

  ![image-20221115224907385](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115224907385.png)



##### 2.1.3 Optimization through Gradient Descent  é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–

* As with many model, we optimize our neural network with Gradient Descent. å’Œå¾ˆå¤šæ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œç®—æ³•ã€‚

  ![image-20221116101218029](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116101218029.png)

* The most important component in this formulation is the gradient åœ¨è¿™ä¸ªå…¬å¼ä¸­æœ€é‡è¦çš„ç»„æˆéƒ¨åˆ†å°±æ˜¯æ¢¯åº¦ã€‚

* Backpropagation to the rescue. åå‘ä¼ æ’­ä»¥è¿›é¡¹æ•‘æ´

  * The backward computations of network return the gradients. ç½‘ç»œçš„åå‘è®¡ç®—è¿”å›æ¢¯åº¦

  * How to make the backward computations  å¦‚ä½•è¿›è¡Œåå‘è¿ç®—

    ![image-20221116101758279](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116101758279.png)

##### 2.1.4 Weight Update

![image-20221116102011402](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116102011402.png)

#### 2.2 Chain Rule

* Assume a nested function, ğ‘§= ğ‘“(ğ‘¦) and y= g(x) å‡è®¾ä¸€ä¸ªé•¶å¥—å‡½æ•°

* Chain Rule for scalars ğ‘¥, ğ‘¦, ğ‘§ æ ‡é‡x,y,zçš„æ ‡é‡æ³•åˆ™

  ![image-20221115093618957](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115093618957.png)

* When ğ‘¥âˆˆRğ‘š(ä¸Šæ ‡),ğ‘¦âˆˆRğ‘›, ğ‘§âˆˆR 

  ![image-20221115093750715](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115093750715.png)

* ![image-20221115093800168](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115093800168.png)

* ![image-20221115093816818](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115093816818.png)

* ![image-20221115093857473](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115093857473.png)

* ![image-20221115093912480](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115093912480.png)

##### 2.2.1 Defination of Chain rule

![image-20221115094014836](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115094014836.png)

* dy/dx æ˜¯é›…å¯æ¯”çŸ©é˜µ

##### 2.2.2 The Jacobian

![image-20221116102540878](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116102540878.png)

##### 2.2.3 Backpropagation

![image-20221116103108768](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116103108768.png)



#### 2.3 Some Examples

![image-20221116103218640](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116103218640.png)

![image-20221116103258798](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116103258798.png)

![image-20221116103309364](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116103309364.png)

![image-20221116103324192](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116103324192.png)

#### 2.4 General Workflow ä¸€èˆ¬å·¥ä½œæµç¨‹

![image-20221116103428496](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116103428496.png)

1. Random Initialization éšæœºåˆå§‹åŒ–
2. Feed Forward æ­£å‘è¾“å…¥
3. Calculate loss function è®¡ç®—æŸå¤±å‡½æ•°
4. Calculate the derivative of error. è®¡ç®—é”™è¯¯çš„æ´¾ç”Ÿ
5. Backpropagate å‘åä¼ æ’­ç®—æ³•
6. Update the weights æ›´æ–°æƒé‡

### 3. Regularization æ­£åˆ™åŒ–

#### 3.1 Recall: what is regularization? 

* **In general: any method to prevent overfitting or help the optimization. ä¸€èˆ¬æ¥è¯´:ä»»ä½•é˜²æ­¢è¿‡æ‹Ÿåˆæˆ–æœ‰åŠ©äºä¼˜åŒ–çš„æ–¹æ³•**
* **Specifically: additional terms in the training optimization objective to prevent overfitting or help the optimization. å…·ä½“åœ°è¯´:åœ¨è®­ç»ƒä¼˜åŒ–ç›®æ ‡ä¸­å¢åŠ é˜²æ­¢è¿‡æ‹Ÿåˆæˆ–å¸®åŠ©ä¼˜åŒ–çš„æœ¯è¯­**

#### 3.2 Recall: Overfitting è¿‡æ‹Ÿåˆ

* Key: empirical loss and expected loss are different. ç»éªŒæŸå¤±å’Œé¢„æœŸæŸå¤±æ˜¯ä¸ä¸€æ ·çš„ã€‚
* The smaller the data set, the larger the difference between the two æ•°æ®é›†è¶Šå°ï¼ŒäºŒè€…çš„å·®åˆ«è¶Šå¤§
* The larger the hypothesis class, the easier to find a hypothesis that 
  fits the difference between the two å‡è®¾ç±»è¶Šå¤§ï¼Œå°±è¶Šå®¹æ˜“æ‰¾åˆ°ä¸€ä¸ªç¬¦åˆä¸¤è€…å·®å¼‚çš„å‡è®¾
  * Thus has small training error but large test error (overfitting)  å› æ­¤åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸­ï¼Œè¾ƒå°çš„è®­ç»ƒé›†é”™è¯¯å¯èƒ½æ˜¯æå¤§çš„æµ‹è¯•é›†é”™è¯¯ã€‚ä¹Ÿå°±æ˜¯è¿‡æ‹Ÿåˆ
* Larger data set helps å¾€å¾€æ›´å¤§çš„æ•°æ®é›†æ›´å¥½
* Throwing away useless hypotheses also helps (regularization) é™¤éå‡è®¾æœ‰ç”¨ï¼Œå¦åˆ™æ‰”æ‰ï¼ˆæ­£åˆ™åŒ–ï¼‰

#### 3.3 Regularization as hard constraint å¸¦æœ‰ç¡¬çº¦æŸçš„æ­£åˆ™åŒ–

![image-20221116174653281](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116174653281.png)

![image-20221116174741016](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116174741016.png)

#### 3.4 Regularization as soft constraint å¸¦æœ‰è½¯çº¦æŸçš„æ­£åˆ™åŒ–

* ç¡¬çº¦æŸçš„ä¼˜åŒ–ç­‰äºè½¯çº¦æŸçš„

![image-20221116174754169](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116174754169.png)

* Showed by Lagrangian multiplier method  æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•

  ![image-20221116175250280](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116175250280.png)

* Suppose ğœƒâˆ—is the optimal for hard-constraint optimization å‡è®¾ğœƒ*æ˜¯ç¡¬çº¦æŸä¼˜åŒ–çš„æœ€ä¼˜å€¼

  ![image-20221116175333499](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116175333499.png)

* Suppose ğœ†âˆ—is the corresponding optimal for max  å‡è®¾ğœ†*æ˜¯maxå¯¹åº”çš„æœ€ä¼˜å€¼

  ![image-20221116175456974](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116175456974.png)





## Lecture 19 DL-Convolutional-Neural-Networks

### 1. Convolutional Neural Networks(CNN) å·ç§¯ç¥ç»ç½‘ç»œ

<font color='red'>ä¸€ä¸ªè®­ç»ƒçš„ä»£ç æ¨¡æ¿, ä¸ªäººè®¤ä¸ºè›®å…³é”®çš„</font>

![image-20221115101205728](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115101205728.png)

#### 1.1 Fully-Connected å…¨å±‚è¿æ¥

![image-20221115101539694](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115101539694.png)



#### 1.2 Convolution å·ç§¯

* Strong empirical application performance. å¼ºå¤§çš„ç»éªŒåº”ç”¨æ€§èƒ½

* Convolutional networks: neural networks that use convolution in place of general matrix multiplication in at least one of their layers. <font color='red'> **å·ç§¯ç½‘ç»œ:åœ¨è‡³å°‘ä¸€å±‚ä¸­ä½¿ç”¨å·ç§¯ä»£æ›¿ä¸€èˆ¬çŸ©é˜µä¹˜æ³•çš„ç¥ç»ç½‘ç»œã€‚**</font>

  ![image-20221115101957535](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115101957535.png)

  for a specific kind of weight matrix ğ‘Š. æ±‚ä¸€ç§ç‰¹å®šçš„æƒé‡çŸ©é˜µğ‘Š

![image-20221116175811537](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116175811537.png)

<font size=5>çº¿æ€§</font>

![image-20221116184901769](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116184901769.png)

![image-20221116184912558](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116184912558.png)

![image-20221116184922199](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116184922199.png)

![image-20221116184934991](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116184934991.png)

![image-20221116184943420](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116184943420.png)

![image-20221116184953997](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116184953997.png)

![image-20221116185009984](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116185009984.png)

![image-20221116185027201](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116185027201.png)

<font size=5>äºŒç»´</font>

![image-20221116185200446](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116185200446.png)

![image-20221116185235537](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116185235537.png)

![image-20221116185244883](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116185244883.png)

##### 1.2.1 A Closer look at spatical dimensions ç‰¹æ®Šç»´åº¦çš„å·ç§¯

* <font size=5>Stride=1</font>

![image-20221116185858655](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116185858655.png)

![image-20221116185911472](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116185911472.png)

![image-20221116190018705](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190018705.png)

![image-20221116190027653](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190027653.png)

![image-20221116190036244](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190036244.png)

å¯ä»¥å‘ç°ï¼Œè¡¨æ ¼ä»ç¬¬ä¸€ä¸ªèµ°åˆ°äº†æœ€åä¸€ä¸ªï¼Œæ€»å…±èµ°äº†äº”æ¬¡

* <font size=5>Stride=2</font>

![image-20221116190410539](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190410539.png)

![image-20221116190418958](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190418958.png)

![image-20221116190428934](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190428934.png)

* <font size=5>Stride=3</font>

![image-20221116190454847](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190454847.png)

![image-20221116190511322](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116190511322.png)

##### 1.2.2 Advantage of Convolutional Layer

###### 1.2.2.1 Advantage: sparse interaction ç¨€ç–äº¤äº’

å…¨è”é€šå±‚

![image-20221116205345950](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116205345950.png)

å·ç§¯å±‚

![image-20221116205740662](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116205740662.png)

å¤šé‡å·ç§¯å±‚

![image-20221116205713385](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116205713385.png)



###### 1.2.2.2 Advantage: parameter sharing/weight tying 

![image-20221116231907849](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116231907849.png)

###### 1.2.2.3 Advantage: equivariant representations ç­‰å˜åŒ–è¡¨ç¤º

* Equivariant: transforming the input = transforming the output ç­‰å˜é‡:è½¬æ¢è¾“å…¥=è½¬æ¢è¾“å‡º
* Example: input is an image, transformation is shifting ç¤ºä¾‹:è¾“å…¥æ˜¯å›¾åƒï¼Œå˜æ¢æ˜¯å¹³ç§»
* Convolution(shift(input)) = shift(Convolution(input)) å·ç§¯(shift(è¾“å…¥))= shift(å·ç§¯(è¾“å…¥))
* Useful when care only about the existence of a pattern, rather than 
  the locationå½“åªå…³å¿ƒæ¨¡å¼çš„å­˜åœ¨ï¼Œè€Œä¸æ˜¯ä½ç½®æ—¶ï¼Œå®ƒæ˜¯æœ‰ç”¨çš„





#### 1.3 Zero-Padding 0å¡«å……

![image-20221115103717131](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221115103717131.png)

#### 1.4 RELU ï¼ˆæ•´æµçº¿æ€§å•ä½ï¼‰

* rectifieris an activation function defined as the positive part of its argument æ•´æµå‡½æ•°æ˜¯ä¸€ç§æ¿€æ´»å‡½æ•°,å®ƒè¢«å®šä¹‰ä¸ºå®ƒçš„è®ºç‚¹çš„ç§¯æéƒ¨åˆ†

  ![image-20221118173546773](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118173546773.png)

* A smooth approximation to the rectifier is the analytic function å¯¹æ•´æµå™¨çš„å¹³æ»‘é€¼è¿‘æ˜¯è§£æå‡½æ•°

  ![image-20221118174322619](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118174322619.png)

* ![image-20221118174338407](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118174338407.png)

#### 1.5 Pooling

* **Pooling layer** is frequently used in convolutional neural networks with the purpose to progressively reduce the spatial size of the representation to reduce the amount of features and the computational complexity of the network.  <font color='red'>Pooling Layer</font> ç»å¸¸ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œ,ç›®çš„æ˜¯é€æ­¥å‡å°‘è¡¨ç¤ºçš„ç©ºé—´å¤§å°,ä»¥å‡å°‘ç½‘ç»œçš„ç‰¹å¾å’Œè®¡ç®—å¤æ‚åº¦ã€‚ã€

----

##### 1.5.1 Terminology of Pooling Layer   Pooling Layerçš„æœ¯è¯­

![image-20221118174728571](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118174728571.png)

----

* Summarizing the input (i.e., output the max of the input) æ±‡æ€»è¾“å…¥(å³è¾“å‡ºè¾“å…¥çš„æœ€å¤§å€¼)

  ![image-20221118175101735](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118175101735.png)

* Induce invariance è¯±å¯¼ä¸å˜æ€§

  ![image-20221118175222350](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118175222350.png)

![image-20221118175313328](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118175313328.png)



#### 1.6 Softmax å½’ä¸€åŒ–æŒ‡æ•°å‡½æ•°

* Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.  å›æƒ³ä¸€ä¸‹ï¼Œé€»è¾‘å›å½’ä¼šäº§ç”Ÿä¸€ä¸ª0åˆ°1.0ä¹‹é—´çš„å°æ•°ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªé‚®ä»¶åˆ†ç±»å™¨çš„é€»è¾‘å›å½’çš„è¾“å‡ºæ˜¯0.8ï¼Œè¿™è¡¨æ˜ä¸€ä»½é‚®ä»¶æœ‰80%çš„æ¦‚ç‡æ˜¯åƒåœ¾é‚®ä»¶ï¼Œ20%çš„æ¦‚ç‡ä¸æ˜¯ã€‚æ˜¾ç„¶æ¦‚ç‡ä¹‹å’Œä¸º1.

* Softmax extends this idea into a multi-class world. That is, Softmax
  assigns decimal probabilities to each class in a multi-class problem. 
  Those decimal probabilities must add up to 1.0. This additional 
  constraint helps training converge more quickly than it otherwise 
  would.   

* softmaxå°†è¿™ä¸ªæƒ³æ³•æ‰©å±•åˆ°ä¸€ä¸ªå¤šç±»çš„ä¸–ç•Œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒSoftmaxä¸ºå¤šç±»é—®é¢˜ä¸­çš„æ¯ä¸ªç±»åˆ†é…åè¿›åˆ¶æ¦‚ç‡ã€‚

  è¿™äº›åè¿›åˆ¶æ¦‚ç‡åŠ èµ·æ¥ä¸€å®šæ˜¯1.0ã€‚è¿™ä¸€é™„åŠ çº¦æŸæœ‰åŠ©äºè®­ç»ƒæ›´å¿«åœ°æ”¶æ•›ã€‚

![image-20221118184105093](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184105093.png)

#### 1.7 Preprocessing data é¢„å¤„ç†æ•°æ®

![image-20221118184136800](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184136800.png)

![image-20221118184313009](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184313009.png)

* e.g. consider CIFAR-10 example with [32,32,3] images 
* Subtract the mean image (e.g. AlexNet) (mean image = [32,32,3] array)  å‡å»å¹³å‡å›¾åƒ(ä¾‹å¦‚AlexNet)(å¹³å‡å›¾åƒ=[32,32,3]æ•°ç»„)
* Subtract per-channel mean (e.g. VGGNet) (mean along each channel = 3 numbers) å‡å»æ¯ä¸ªé¢‘é“çš„å¹³å‡å€¼(ä¾‹å¦‚VGGNet)(æ¯ä¸ªé¢‘é“çš„å¹³å‡å€¼=3

### 2. Example

* ç”±Yann LeCun, Leon Bottou, Yoshua Bengioå’ŒPatrick Haffneråœ¨â€œåŸºäºåº¦çš„å­¦ä¹ åº”ç”¨äºæ–‡æ¡£è¯†åˆ«â€ä¸­æå‡º

  IEEEå­¦æŠ¥ï¼Œ1998

* Apply convolution on 2D images (MNIST) and use backpropagation åº”ç”¨å·ç§¯åœ¨äºŒç»´å›¾åƒä¸Šå¹¶ä½¿ç”¨åå‘ä¼ æ’­

*  ç»“æ„:2ä¸ªå·ç§¯å±‚(å¸¦æ± )+ 3ä¸ªå®Œå…¨è¿æ¥å±‚

  * Input size: 32x32x1
  * Convolution kernel size: 5x5
  * Pooling: 2x2  

![image-20221118184834581](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184834581.png)

![image-20221118184848479](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184848479.png)

![image-20221118184855699](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184855699.png)

![image-20221118184905083](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184905083.png)

![image-20221118184914908](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184914908.png)

![image-20221118184927944](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184927944.png)

![image-20221118184940283](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221118184940283.png)



## Lecture 20 Model Evaluation

### 1. Test sets revisited é‡æ–°è®¿é—®æµ‹è¯•é›†

* How can we get an unbiased estimate of the accuracy of a learned model?  æˆ‘ä»¬å¦‚ä½•å¾—åˆ°ä¸€ä¸ªå¯¹å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§çš„æ¯«æ— åè§çš„ä¼°è®¡

  ![image-20221116111333614](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116111333614.png)





### 2. Learn Curve

* How does the accuracy of a learning method change as a function of the training-set size? 
* å­¦ä¹ æ–¹æ³•çš„å‡†ç¡®æ€§å¦‚ä½•ä½œä¸ºè®­ç»ƒè®¾ç½®å¤§å°çš„åŠŸèƒ½å˜åŒ–?
  * this can be assessed by plotting learning curves  
  * å¯ä»¥ç”¨å­¦ä¹ æ›²çº¿è¡¨ç¤º

![image-20221116111807297](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221116111807297.png)







### 3. Multiple training/test partitions

#### 3.1 Limitations of using a single training/test partition ä½¿ç”¨å•ä¸ªè®­ç»ƒ/æµ‹è¯•åˆ†åŒºçš„é™åˆ¶

* we may not have enough data to make sufficiently large training and test sets
  * a larger test set gives us more reliable estimate of accuracy (i.e. a lower variance estimate) 
  * but... a larger training set will be more representative of how much data we actually have for learning process 
* a single training set doesnâ€™t tell us how sensitive accuracy is to a particular training sample 

![image-20230122222019165](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122222019165.png)

![image-20230122222037657](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122222037657.png)





#### 3.2 Using multiple training/test partitions 

![image-20230122222625749](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122222625749.png)

#### 3.4 Random resampling éšæœºé‡é‡‡æ ·

* We can address the second issue by repeatedly randomly partitioning the available data into training and test sets. 
* æˆ‘ä»¬å¯ä»¥é€šè¿‡åå¤éšæœºåœ°å°†å¯ç”¨æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¥è§£å†³ç¬¬äºŒä¸ªé—®é¢˜ã€‚

![image-20230122222620541](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122222620541.png)

#### 3.5 Stratified sampling åˆ†å±‚æŠ½æ ·

* When randomly selecting training or validation sets, we may want to ensure that class proportions are maintained in each selected set 
* å½“éšæœºé€‰æ‹©è®­ç»ƒé›†æˆ–éªŒè¯é›†æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ç¡®ä¿åœ¨æ‰€é€‰çš„æ¯ä¸ªé›†ä¸­ä¿æŒç±»çš„æ¯”ä¾‹

![image-20230122222936636](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122222936636.png)



#### 3.6 Cross Validation äº¤å‰éªŒè¯

![image-20230122223013820](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122223013820.png)

* å°†ä¸€ä¸ªæ•°æ®é›†åˆ†æˆåˆé€‚çš„5æ®µè½ï¼Œå–4ä¸ªä¸ºè®­ç»ƒé›†ï¼Œ1ä¸ªä¸ºæµ‹è¯•é›†ã€‚å¤šæ¬¡é‡å¤å–æ ·

![image-20230122223209858](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122223209858.png)

* 10-fold cross validation is common, but smaller values of n are often used when learning takes a lot of time 
* 10å€äº¤å‰éªŒè¯æ˜¯å¸¸è§çš„ï¼Œä½†å½“å­¦ä¹ éœ€è¦å¤§é‡æ—¶é—´æ—¶ï¼Œé€šå¸¸ä½¿ç”¨è¾ƒå°çš„nå€¼
* in leave-one-out cross validation, n = # instances 
* åœ¨çœç•¥ä¸€ä¸ªäº¤å‰éªŒè¯ä¸­ï¼Œn = #å®ä¾‹
* in stratified cross validation, stratified sampling is used when partitioning the data 
* åœ¨åˆ†å±‚äº¤å‰éªŒè¯ä¸­ï¼Œåœ¨åˆ’åˆ†æ•°æ®æ—¶é‡‡ç”¨åˆ†å±‚æŠ½æ ·
* Cross validation makes efficient use of the available data for testing 
* äº¤å‰éªŒè¯å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¯ç”¨æ•°æ®è¿›è¡Œæµ‹è¯•



### 4. Confusion matrices æ··æ·†çŸ©é˜µ

* How can we understand what types of mistakes a learned model makes? 
* æˆ‘ä»¬å¦‚ä½•ç†è§£ä¸€ä¸ªå­¦ä¹ æ¨¡å‹ä¼šçŠ¯ä»€ä¹ˆç±»å‹çš„é”™è¯¯å‘¢?

![image-20230122223425867](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122223425867.png)

![image-20230122223732930](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122223732930.png)

#### 4.1 Is accuracy an adequate measure of predictive performance? 

![image-20230122223805148](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122223805148.png)

* accuracy may not be a useful measure in cases where 
* åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼Œå‡†ç¡®æ€§å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„è¡¡é‡æ ‡å‡†
* we are most interested in a subset of high-confidence predictions 
* æˆ‘ä»¬å¯¹é«˜ç½®ä¿¡åº¦é¢„æµ‹çš„ä¸€ä¸ªå­é›†æœ€æ„Ÿå…´è¶£

![image-20230122224127604](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224127604.png)

![image-20230122224153042](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224153042.png)

![image-20230122224234539](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224234539.png)



### 5. ROC curves 

* A Receiver Operating Characteristic (ROC) curve plots the TP-rate vs. the FP-rate as a threshold on the confidence of an instance being positive is varied 
* å—è¯•è€…å·¥ä½œç‰¹å¾(ROC)æ›²çº¿æç»˜äº†TP-rateå’ŒFP-rateï¼Œä½œä¸ºå®ä¾‹ä¸ºé˜³æ€§çš„ç½®ä¿¡åº¦çš„é˜ˆå€¼æ˜¯ä¸åŒçš„

![image-20230122224427132](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224427132.png)

![image-20230122224450594](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224450594.png)

![image-20230122224459563](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224459563.png)

![image-20230122224510262](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224510262.png)

![image-20230122224521166](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122224521166.png)





### 6.PR curves

![image-20230122225128640](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122225128640.png)

![image-20230122225136263](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122225136263.png)

![image-20230122225147458](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122225147458.png)

![image-20230122225158507](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122225158507.png)

![image-20230122225210622](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122225210622.png)

* Both 
  * Allow predictive performance to be assessed at various levels of confidence
  * å…è®¸åœ¨ä¸åŒçš„ç½®ä¿¡åº¦æ°´å¹³ä¸Šè¯„ä¼°é¢„æµ‹æ€§èƒ½
  * Assume binary classification tasks
  * æ‰¿æ‹…äºŒè¿›åˆ¶åˆ†ç±»ä»»åŠ¡
  * Sometimes summarized by calculating area under the curve
  * æœ‰æ—¶é€šè¿‡è®¡ç®—æ›²çº¿ä¸‹çš„é¢ç§¯æ¥æ€»ç»“
* ROC curves
  * Insensitive to changes in class distribution (ROC does not change if the proportion of positive and negative instances in the test set are varied)
  * å¯¹ç±»åˆ†å¸ƒçš„å˜åŒ–ä¸æ•æ„Ÿ(å¦‚æœæµ‹è¯•é›†ä¸­æ­£å®ä¾‹å’Œè´Ÿå®ä¾‹çš„æ¯”ä¾‹å˜åŒ–ï¼ŒROCä¸å˜åŒ–)
  * Can identify optimal classification thresholds for tasks with differential misclassification costs
  * èƒ½å¦ä¸ºå…·æœ‰ä¸åŒé”™è¯¯åˆ†ç±»ä»£ä»·çš„ä»»åŠ¡è¯†åˆ«æœ€ä½³åˆ†ç±»é˜ˆå€¼
* PR curves
  * Show the fraction of predictions that are false positives
  * æ˜¾ç¤ºå‡é˜³æ€§é¢„æµ‹çš„æ¯”ä¾‹
  * Well suited for tasks with lots of negative instances
  * éå¸¸é€‚åˆæœ‰å¾ˆå¤šè´Ÿé¢å®ä¾‹çš„ä»»åŠ¡









## Lecture 21

### 1.Positioning of Probabilistic Inference

![image-20221122091122238](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221122091122238.png)

#### 1.1 Top 10 Real-world Bayesian Network Applications

â€¢https://data-flair.training/blogs/bayesian-network-applications/

â€‹	â€¢Gene Regulatory Network åŸºå› è°ƒæ§ç½‘ç»œ
â€‹	â€¢Medicine  åŒ»å­¦
â€‹	â€¢Biomonitoring ç”Ÿç‰©
â€‹	â€¢Document Classification  æ–‡ä»¶åˆ†ç±»
â€‹	â€¢Information Retrieval  ä¿¡æ¯æ£€ç´¢
â€‹	â€¢Semantic Search  è¯­ä¹‰æŸ¥æ‰¾
â€‹	â€¢Image Processing  å›¾åƒå¤„ç†
â€‹	â€¢Spam Filter åƒåœ¾é‚®ä»¶è¿‡æ»¤
â€‹	â€¢Turbo Code æ¶¡è½®ç 
â€‹	â€¢System Biology ç³»ç»Ÿç”Ÿç‰©å­¦

![image-20221123101915161](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221123101915161.png)

![image-20221123101938543](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221123101938543.png)

![image-20221123102046622](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221123102046622.png)

#### 1.2 Fundamental Questions

![image-20221122091949987](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221122091949987.png)

* Representation è¡¨ç¤º
  * How to capture/model uncertainties in possible worlds? å¦‚ä½•åœ¨å¯èƒ½çš„ä¸–ç•Œä¸­æ•è·ä¸ç¡®å®šæ€§ï¼Ÿ
  * How to encode our domain knowledge/assumptions/constraints? å¦‚ä½•ç¼–ç æˆ‘ä»¬èŒƒå›´å†…çš„çŸ¥è¯†



* Inference 

  ![image-20221123102654670](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221123102654670.png)

* Learning

  ![image-20221123102729154](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20221123102729154.png)





### 2.Recap: NaÃ¯ve Bayes æœ´ç´ è´å¶æ–¯

#### 2.1 Recap of Basic Prob. Concepts

![image-20230122230922425](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122230922425.png)



* Each Xirepresents outcome of tossing coin i
  * Assume coin tosses are marginally independent 
  * å‡è®¾æŠ›ç¡¬å¸æ˜¯è¾¹é™…ç‹¬ç«‹çš„
  * i.e., ![image-20230122231039173](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122231039173.png) therefore 

![image-20230122231012975](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122231012975.png)

![image-20230122231210002](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122231210002.png)

![image-20230122231329515](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122231329515.png)

![image-20230122231925438](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122231925438.png)

![image-20230122231932510](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122231932510.png)

![image-20230122235832884](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230122235832884.png)

* Conditional Parameterization is combined with Conditional Independence assumptions to produce very compact representations of high dimensional probability distributions 
* æ¡ä»¶å‚æ•°åŒ–ä¸æ¡ä»¶ç‹¬ç«‹å‡è®¾ç›¸ç»“åˆå¯ä»¥äº§ç”Ÿéå¸¸ç´§å‡‘çš„é«˜ç»´æ¦‚ç‡åˆ†å¸ƒè¡¨ç¤º

### 3.Example Bayes Networks

####  3.1 BN for General Naive Bayes Model  BNä¸ºä¸€èˆ¬æœ´ç´ è´å¶æ–¯æ¨¡å‹

è´å¶æ–¯ç¥ç»ç½‘ç»œä¸ºä¸€èˆ¬æœ´ç´ è´å¶æ–¯æ¨¡å‹

![image-20230123000437239](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230123000437239.png)

![image-20230123000452369](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230123000452369.png)

![image-20230123000754849](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230123000754849.png)

* å¦‚æœxæ˜¯æ¡ä»¶ç‹¬ç«‹çš„(å¦‚PGMæ‰€æè¿°çš„é‚£æ ·)ï¼Œè”åˆåˆ†å¸ƒå¯ä»¥åˆ†è§£æˆä¸€ä¸ªæ›´ç®€å•çš„ä¹˜ç§¯ï¼Œä¾‹å¦‚:

![image-20230123000826868](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230123000826868.png)



![image-20230123001118957](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230123001118957.png)

![image-20230123001159317](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230123001159317.png)

### 4. Example Probability Query

![image-20230124113137336](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113137336.png)

![image-20230124113144520](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113144520.png)

![image-20230124113150730](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113150730.png)

**<font color=red size=5>!!!!!!</font>**

![image-20230124113208538](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113208538.png)

![image-20230124113301689](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113301689.png)

![image-20230124113322756](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113322756.png)

### 

### 5.Example: Alarm Network 

![image-20230124113422289](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113422289.png)

![image-20230124113435226](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230124113435226.png)





## Lecture22 I-Maps: Graphs 

and Distributions

### 1. Recap: COnditional Independence

### 2. Markov Assumption and Definition of I-Maps

### 3. I-Map to Factorization 

### 4. Factorization to I-Map 

### 5. Perfect Map 



![image-20230125225320142](C:\Users\ç™½æœ¨-æ³½\AppData\Roaming\Typora\typora-user-images\image-20230125225320142.png)

## Lab 7 

-- Argparseå‘½ä»¤æ¨¡å—

https://blog.csdn.net/u011913417/article/details/109047850?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166877135416800180692349%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166877135416800180692349&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-109047850-null-null.142^v65^js_top,201^v3^control_1,213^v2^t3_esquery_v3&utm_term=ArgumentParser&spm=1018.2226.3001.4187





## Lecture24-PGM-D-separation

### 1. Recap

### 2. Independencies in Graphs

* A graph structure G encodes a set of conditional independence assumptions I(G)   å›¾ç»“æ„Gç¼–ç äº†ä¸€ç»„æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾I(G)
* Are there other independencies that we can read-off?  æˆ‘ä»¬æ˜¯å¦å¯ä»¥è¯»å–å…¶ä»–çš„ç‹¬ç«‹æ€§
  * i.e., are there other independencies that hold for every distribution that factorizes over G?  å¯¹äºæ¯ä¸€ä¸ªåˆ†è§£Gçš„åˆ†å¸ƒæ˜¯å¦å­˜åœ¨å…¶çš„ç‹¬ç«‹æ€§
* D-separation holds the key   D-separationæ˜¯å…³é”®



### 3. D-separation

#### 3.1 Dependencies and Independencies  ç‹¬ç«‹æ€§å’Œä¾èµ–æ€§

* Crucial for understanding network behaviour  ç†è§£ç½‘ç»œæ´»åŠ¨éå¸¸é‡è¦
* Independence properties are important for answering queries  ç‹¬ç«‹å±æ€§å¯¹äºå›ç­”æŸ¥è¯¢å¾ˆé‡è¦
  * Exploited to reduce computation of inference  åˆ©ç”¨æ¨æ–­æ¥å‡å°‘è®¡ç®—
  * A distribution P that factorizes over G satisfies I(G)  åˆ†è§£Gçš„åˆ†å¸ƒPæ»¡è¶³I(G)

#### 3.2 D-separation

#### 3.2.1 Direct Connection between X and Y 

#### 3.2.2 Indirect Connection between X and Y 

#### 3.2.3  I. Indirect Causal Effect: X->Z->Y

##### 3.2.3.1 Causal Chains 

#### 3.2.4  II.  Indirect Evidential Effect: Y->Z->X 

#### 3.2.5  III.  Common Cause: X<-Z->Y

##### 3.2.5.1 Common Cause 

####  3.2.6 IV.  Common Effect (V-structure) X->Z<-Y 

##### 3.2.6.1 Common Effect

