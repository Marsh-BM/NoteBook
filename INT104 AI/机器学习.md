# **机器学习**

## 机器学习概论

### 机器学习

机器学习是从数据中自动获得分析模型，并利用模型对未知的数据进行预测。

![image-20220318063152674](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220318063152674.png)

![image-20220318063340511](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220318063340511.png)

![image-20220318063350679](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220318063350679.png)

### 数据集

结构：数据集=特征值+目标值

* 对于每一行的数据我们称之为样本
* 有些数据可以没有目标值

### 机器学习的算法的分类

监督学习：

* 目标值：类别 - 分类问题

* 目标值：连续型的数据 - 回归问题
* k-临近算法，贝叶斯算法，决策树与随机森林，逻辑回归，线性回归，岭回归。。。

无监督学习：

* 目标值：无
* 聚类 k-means

![image-20220318064411527](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220318064411527.png)

### 机器学习的开发流程

1. 获取数据
2. 数据处理
3. 特征工程
4. 机器学习算法 - 模型
5. 模型评估
6. 应用

![image-20220318065740302](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220318065740302.png)





# 2.特征工程



## 2.1数据集

### 可用数据集

![image-20220318070858908](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220318070858908.png)

### 2.1.2 sklearn数据集

#### 1 sklearn.datasets数据集API介绍

sklearn.datasets

* 加载获取流行数据集
* datasets.load_()
  * 获取小规模的数据集
* datasets.fetch_(data_home=None)
  * 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录默认是~/scikit_learn_data/

#### 2 sklearn的小数据集

![image-20220430221822957](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430221822957.png)

#### 3 sklearn大数据集

![image-20220430221849446](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430221849446.png)



#### 4 sklearn数据集的使用

![image-20220430222059132](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430222059132.png)

![image-20220430222241254](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430222241254.png)



### 2.1.3 数据集的划分

训练数据：用于训练，构建模型

测试数据：在模型检验时使用，用于评估模型是否有效

测试集一般在全部数据集的20%~30%之间

训练集特征值（x_train）

测试集特征值（x_test）

训练集目标值（y_train）

测试集目标值（y_test）
![image-20220430223910459](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430223910459.png)

## 2.2 特征工程的介绍

### 2.2.1为什么需要特征工程

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。

## 2.2.2什么是特征工程

特征工程是使用专业的背景知识和技巧处理数据，使得特征工程能够在机器学习算法上发挥更好的作用的过程。

意义：会直接影响机器学习的效果

## 2.2.3特征工程的位置和数据处理的比较

![image-20220430225054595](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430225054595.png)

**特征工程包含的内容**

* **特征抽取** 

* **特征预处理**

* **特征降维**

## 2.3 特征抽取/提取

### 2.3.1 特征抽取

1. 将任意的数据（文本或者图像）转换成可用于机器学习的数据特征

   * 字典特征提取（特征离散值化）
   * 文本特征提取
   * 图像特征提取（深度学习）

2. 特征提取API

   ![image-20220430231605819](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430231605819.png)

### 2.3.2 字典的特征提取

![image-20220430231646488](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430231646488.png)

字典特征提取--类别--> one-hotbia(编码)

* sparse指的是默认返回稀疏矩阵(只表示非零值)，可以通过fit_transform(sparse=False)

**应用：**

 ![image-20220430232157729](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220430232157729.png)

```
def dict_demo():
    data = [{'city':'北京','temperature':100},{'city':'上海','temperature':60},{'city':'深圳','temperature':30}]
    # 1.实例化一个转换器类
    transfer = DictVectorizer()
    data_new = transfer.fit_transform(sparse=False)
    print("data_new:\n", data_new)
    return None
```

**应用场景：**

1) pclass, sex 数据集当中类别特征比较多
   1) 将数据集特征--》字典类型
   2) DictVectorizer转换
2) 本身拿到的数据就是字典类型



### 2.3.3对文本进行数据提取

以单词作为特征进行提取

方法一：

![image-20220501111958168](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501111958168.png)

* stop_words(停用词)以列表的形式表明不需要统计的词。

```
def count_demo():
    data = ["life is short, i like like python", "life is too long, i dislike python"]
    transfer = CountVectorizer()
    data_new = transfer.fit_transform(data)
    print("data_new:\n",data_new)
    return None
```

**关键词：在某一类别的文章中，出现的次数很多，但在其他类别的文章中出现的很少。**

方法二：

![image-20220501114203329](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501114203329.png)

![image-20220501114621564](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501114621564.png)

## 2.4 特征预处理

### 2.4.1 什么是特征预处理

**通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程。**

**1.包含内容**

数值型数据的无量纲化

* 归一化
* 标准化(用的更多)

**为什么要进行归一化处理？**

**因为特征单位或者大小相差较大，或者在某特征的方差相比于其他的特征要大出几个数量级，容易影响（支配）目标结果**，使得一些算法无法学习到其他得特征。

**为了保证每个特征是同样重要得**

### 2.4.2归一化

**1.计算方法**

![image-20220501124603685](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501124603685.png)

**2.API**

```
from sklearn.preprocessing import MinMaxScaler
```

```
def minmax_demo():
    """
    归一化
    :return:
    """
    # 1.归一化
    data = pd.read_csv("dataing.txt")
    data = data.iloc[:, :3]
    # 2.实列化一个转换器
    transfer = MinMaxScaler()
    # 3.调用fit_transform
    data_new = transfer.fit_transform(data)
```

**缺点：归一化非常容易受到异常值的影响**，**因为最大值和最小值可能是异常值。**

总结：归一化非常容易受到异常值的影响**，**因为最大值和最小值可能是异常值。所以这种方法鲁棒性较差，只适合传统精确小数据场景。

### 2.4.3 标准化

![image-20220501131204822](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501131204822.png)

![image-20220501131329269](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501131329269.png)

**3.API**

![image-20220501131545597](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501131545597.png)

## 2.5 特征降维

### 2.5.1 降维

降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相干”的主变量的过程

![image-20220501132235846](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501132235846.png)

### 2.5.2 降维的两种方法

* 特征选择
* 主成分分析（可以理解一种特征提取的方法）

### 2.5.3 什么是特征选择

**1.定义**

数据中包含冗余或者相关变量（或称特征，属性，指标等），目的是在原有特征中找出主要特征

**2.方法**

![image-20220501143624284](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501143624284.png)

**3.模块**

```
from sklearn.feature_selection
```

**4.过滤式**

**4.1低方差特征过滤**

![image-20220501144558545](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501144558545.png)

```
def variance_demo():
    """
    删除低方差特征（当方差为0意味着所有的数据都有这一特征，不具备识别性）
    :return:
    """
    # 1.标准化
    data = pd.read_csv("dataing.txt")
    data = data.iloc[:, :3]
    # 2.实列化一个转换器
    transfer = VarianceThreshold(threshold=0) # 内部的参数是指方差低于这个值则删除。
    # 3.调用fit_transform
    data_new = transfer.fit_transform(data)
    return None
```

**4.2相关系数**

4.2.1计算公式

![image-20220501144946292](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501144946292.png)

4.2.2特点

![image-20220501145012575](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501145012575.png)

特征与特征之间相关性很高的时候：

1. 选取其中一个
2. 加权求和

## 2.6 主成分分析PCA

### 2.6.1 PCA定义

定义：**高维数据转低维数据的过程，在此过程中可能会舍弃原有数据，创建新的变量**

作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。

**API**

![image-20220501150828779](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501150828779.png)

n_components : 小数表示保留百分之多少的信息

​								整数 减少到多少的特征



### 2.6.2 案列

第17课

# 3.分类算法

## 3.1 sklearn分类算法

### 3.1.1转换器

**我们将特征工程的接口称为转换器**。

![image-20220501155948686](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501155948686.png)

### 3.1.2估计器（sklearn机器学习算法的实现）

**在sklearn中，估计器(estimator)是一个重要的角色，是一类实现了算法的API**

- **sklearn 估计器是进行机器学习的面向对象**

![image-20220501161654692](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501161654692.png)

![image-20220501161622172](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501161622172.png)



![image-20220501161819132](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501161819132.png)



### 3.2 K临近算法（KNN）

#### 3.2.1 什么是K临近算法

核心思想：**根据你的邻居判断你的类别**

定义：如果一个样本在特征空间的K个最相似（即为特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于该类别。

* 如果K=1
  * 容易受到异常点的影响
  * K也不要取太大，会受到样本不均衡的影响。
* 如何确定谁是邻居？
  * 距离公式：
    * 曼哈顿距离，欧式距离等待

#### 3.2.2 K临近算法API

![image-20220501163532533](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501163532533.png)

#### 3.2.3案例1：鸢尾花种类预测

![image-20220501163707154](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501163707154.png)

1. 获取数据
2. 数据集划分
3. 特征工程
   1. 标准化
   2. 降维（也许不需要）
4. KNN预估器流程
5. 模型评估

```
from sklearn.datasets import load_iris
from  sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
def knn_iris():
    """
    使用KNN算法对鸢尾花数据集进行分类
    :return:
    """
# 1. 获取数据
    iris = load_iris()
# 2. 数据集划分
    x_train, x_test, y_train, y_test =train_test_split(iris.data, iris.target, random_state=6, test_size=0.2 )
# 3. 特征工程
#    1. 标准化
#    2. 降维（也许不需要）
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_train)
# 4. KNN预估器流程
    estimator = KNeighborsClassifier(n_neighbors=3)
    estimator.fit(x_train, y_train)
# 5. 模型评估
    # 方法一
    y_predict = estimator.predict(x_test)
    print(y_predict)
    print(y_predict==y_test)
    # 方法二
    score = estimator.score(x_test,y_test)
    print("准确率：\n",score)

    return None


if __name__ == '__main__':
    print()
```

 

## 3.3模型选择和调优

### 3.3.1 什么是交叉验证

交叉验证：将拿到的训练数据，分为训练集和验证集。以下图为例：将数据分成四份，其中一份作为验证集。然后经过四组测试，每次都更换不同的验证集。即得到四组模型的结果，取平均值作为最终的结果。

![image-20220501175131408](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501175131408.png)

可以让模型更加准确可信

### 3.3.2 超参数搜索-网格搜索（Grid Search）

![image-20220501175344689](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501175344689.png)

![image-20220501175458099](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501175458099.png)

param_grid 将想要传入的参数按照字典的顺序传入

### 3.2.4 案例：预测facebook签到的位置

23课



## 3.4朴素贝叶斯算法

### 3.4.1 朴素贝叶斯算法

* **朴素：指特征与特征之间相互独立**

一种通过计算概率来进行分类的算法。

### 3.4.2 基础概率

**1 概率（probability）定义**

概率定义为一件事情发生的可能性

### 3.4.3联合概率，条件概率与相互独立

![image-20220501210106357](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501210106357.png)

![image-20220501212936279](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501212936279.png)



### 3.4.4 贝叶斯公式

![image-20220501210820690](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501210820690.png)

**应用场景：**

文本分类--- 以单词作为特征

**4.拉普拉斯平滑系数**

![image-20220501212956375](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501212956375.png)

### 3.4.5 API

![image-20220501213227259](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501213227259.png)

### 3.4.6 案例：20类新闻分类

![image-20220501213428504](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501213428504.png)

### 3.4.7 朴素贝叶斯算法的总结

![image-20220501215046815](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220501215046815.png)



## 3.5 决策树

### 3.5.1 认识决策树

决策树的思想非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一类学习方法。

* 按照特征的先后顺序进行决断

### 3.5.2 决策树分类原理详解

按照是否满足特征条件对于数据进行分类。

### 3.5.3 信息论基础

**信息：**消除随机**不定性**的东西。

信息熵：衡量信息的单位

![image-20220502140500600](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502140500600.png)

**3.决策树的划分依据之一----信息增益**

![image-20220502141352698](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502141352698.png)

![image-20220502141507221](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502141507221.png)

### 3.5.3 决策树API

![image-20220502141636660](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502141636660.png)

```
def decision_iris():
    """
    用决策树对iris进行分类
    :return:
    """
    # 1.获取数据
    iris = load_iris()
    # 2.划分数据集
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)
    # 2.5.特征工程（不必要）
    # 3.决策树预估器
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train,y_train)
    # 4.模型评估
    # 方法一
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接对比真实值和预测值：\n", y_test == y_predict)
    # 方法二
    score = estimator.score(x_test, y_test)
    print("准确率：\n", score)

    # print("最佳参数：\n", estimator.best_params_)
    return None
```

### 3.5.4 案例：泰坦尼克号乘客生存预测

课程29

![image-20220502152141170](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502152141170.png)



### 3.5.5 决策树的可视化

![image-20220502143028279](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502143028279.png)

* out_file 表示导出的文件名后缀
* feature_names: 特征的名字

**2.网站显示结构**

![image-20220502143323099](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502143323099.png)

### 3.5.6 决策树总结

![image-20220502144621512](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502144621512.png)



## 3.6 集成学习方法之随机森林

### 3.6.1 什么是集成学习方法

集成学习通过几个模型的组合来解决单一预测问题。它的工作原理是生成多个分类器模型，各自独立地学习和做出预测。这些预测最后结合成组合预测，因此优于任何一个单分类做出的预测。

### 3.6.2 什么是随机森林

在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出是由个别树输出的类别众数决定的。（多个决策树，少数服从多数。）

### 3.6.3 随机森林的原理

原理：随机有放回的抽取

假设有一个训练集：N个样本，M个特征。

* 1.训练集随机--从N个样本中随机又放回的抽取N个
  * --- bootstrap 随机有放回的抽样。
* 2.特征随机-- 从M个特征中随机抽取m个特征
  * M>>m（降维）

![image-20220502164618834](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502164618834.png)

### 3.6.4 随机森林API

![image-20220502164825632](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502164825632.png)

### 3.6.6 总结

![image-20220502171132013](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502171132013.png)



 # 4.回归与聚类算法

## 4.1 线性回归

### 4.1.1线性回归的原理

**2 线性回归**

1) 定义和公式
   1) 线性回归（Linear regression）是利用回归方程(函数)对于一个或多个自变量(特征值)和因变量(目标值)之间进行建模的一种分析方式
      1) 特点只有一个自变量的情况称为单变量回归，多个自变量的情况叫做多元回归。
2) 通用公式：![image-20220502180559030](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502180559030.png)

![image-20220502180627996](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502180627996.png)

3. 线性回归的特征与目标的关系分析

   1. **线性回归当中有线性模型有两种，一种是线性关系，另一种是非线性关系。**在这里我们只能画一个平面去更好的理解，所以都用单个的特征或者两个特征举例子。

      1. ![image-20220502181024241](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502181024241.png)

         ![image-20220502181153598](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502181153598.png)

      2. 非线性模型![image-20220502181406879](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502181406879.png)

### 4.1.2 线性回归的损失和优化原理(理解记忆)

目标：求模型方程

过程：如果某真实关系是A，那么通过随意假定某关系B的各个参数使其不断接近A关系。**所以关键在于衡量真实关系和假定关系之间的差距。**我们将衡量这个指标的参数称为**损失函数**/**cost/成本函数/目标函数**。

**1 损失函数**

![image-20220502183208774](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502183208774.png)

##### **2 优化算法**

###### 正规方程

![image-20220502183748552](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502183748552.png)

* 直接求解方程





###### 梯度下降（Gradient Descent）

![image-20220502184846564](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502184846564.png)

* 通过不断试错求解



### 4.1.3 线性回归API

```
from sklearn.linear_model import LinearRegression
```

![image-20220502213637475](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502213637475.png)

* 偏置（fit_intercept）:增加准确率

### 4.1.4 实例： 波士顿房价预测

流程：

1. 获取数据集
2. 划分数据集
3. 特征工程：
   1. 无量纲化--标准化（推荐）
4. 预估器模型
   1. fit() --> 模型
   2. coef_intercept
5. 模型评估

### 4.1.5 模型评估

![image-20220502220631919](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502220631919.png)

**平均绝对误差（MAE）**
MAE 的值越小，说明预测模型拥有更好的精确度。The MAE is used to measure the average absolute error between the predicted value and the real value on the experimental data set. For a test set containing n microblog messages, MAE is defined as:

### 4.1.6 对比

![image-20220502223758253](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502223758253.png)

![image-20220502223826793](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502223826793.png)

### 4.1.7 梯度下降的改进

![image-20220502223948964](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502223948964.png)

### 补充

#### 广义线性模型

![image-20220502182307412](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502182307412.png)

## 4.2 过拟合和欠拟合

### 4.2.1 欠拟合和过拟合的定义

欠拟合：特征太少，没学到什么

过拟合：识别特征过多，将非必要特征要囊括进去了。指过分拟合训练数据，而可能导致测试数据时效果不好。

![image-20220502231111742](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502231111742.png)

### 4.2.2 原因和解决办法

![image-20220502231333167](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502231333167.png)

1. 正则化

   1. L2正则化/**岭回归**(更常用)：**损失函数+惩罚项w^2**![image-20220502231818769](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502231818769.png)

   

   1. L1正则化:损失函数+惩罚项w

      ![image-20220502232107489](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502232107489.png)

## 4.3 线性回归的改进---岭回归

岭回归，其实也是一种线性回归，只不过在算法建立回归方程的时候，加上了正则化限制，从而达到解决过拟合的问题。

### 4.3.1 岭回归的API

![image-20220502232517960](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502232517960.png)

* alpha：惩罚项系数（正则化力度）
* fit_intercept:偏执

![image-20220502232849506](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502232849506.png)

![image-20220502233017344](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220502233017344.png)

## 4.4 逻辑回归

### 4.4.1 逻辑回归的定义以及应用场景

**逻辑回归(Logistic Regression)是机器学习中的一种分类模型，逻辑回归是一种分类算法。**与回归有一定的联系。由于算法的高效，在实际应用中非常广泛。

应用场景：

* 广告点击率
* 是否是垃圾邮件
* 是否患病
* 金融诈骗



### 4.4.2 逻辑回归的原理

**1 输入**

![image-20220503155407597](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220503155407597.png)



### 4.4.3 损失以及其优化

![image-20220503171240831](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220503171240831.png)

![image-20220503171336699](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220503171336699.png)

![image-20220503171408575](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220503171408575.png)

![image-20220503171601640](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220503171601640.png)

**可以使用梯度下降的办法去逐步优化损失函数**

![image-20220503172126534](C:\Users\白木-泽\AppData\Roaming\Typora\typora-user-images\image-20220503172126534.png)

### 4.4.4 案例：癌症分类预测

流程分析：

1. 获取数据
2. 数据处理
   1. 处理缺失值
3. 数据集划分
4. 特征工程：
   1. 标准化处理
5. 逻辑回归预估器
6. 模型评估

## 4.6 无监督学习-K-Means算法

### 4.6.1 什么是无监督学习





# 其他

24课

**获取满足下面条件的数据**

data.query("x<2.5&x>2&y<1.5&y>1.0")



**处理时间特征，将其转换为正常的时间**

pd.to_datetime(data["time"], unit="s")



**将年月日的时间格式进行转换**

date=pd.DatetimeIndex(time_value)

* 比如：date.weekday获得日期是星期几



**统计：（分组聚合）**

data.groupby("place_id").count()







